<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java操作redis]]></title>
    <url>%2F2017%2F08%2F25%2Fjava-redis%2F</url>
    <content type="text"><![CDATA[连接操作命令 123quit：关闭连接（connection）auth：简单密码认证help cmd： 查看cmd帮助，例如：help quit 持久化 1234save：将数据同步保存到磁盘bgsave：将数据异步保存到磁盘lastsave：返回上次成功将数据保存到磁盘的Unix时戳shundown：将数据同步保存到磁盘，然后关闭服务 远程服务控制 1234info：提供服务器的信息和统计monitor：实时转储收到的请求slaveof：改变复制策略设置config：在运行时配置Redis服务器 对value操作的命令 12345678910111213exists(key)：确认一个key是否存在del(key)：删除一个keytype(key)：返回值的类型keys(pattern)：返回满足给定pattern的所有keyrandomkey：随机返回key空间的一个keyrename(oldname, newname)：重命名keydbsize：返回当前数据库中key的数目expire：设定一个key的活动时间（s）ttl：获得一个key的活动时间select(index)：按索引查询move(key, dbindex)：移动当前数据库中的key到dbindex数据库flushdb：删除当前选择数据库中的所有keyflushall：删除所有数据库中的所有key String 1234567891011121314set(key, value)：给数据库中名称为key的string赋予值valueget(key)：返回数据库中名称为key的string的valuegetset(key, value)：给名称为key的string赋予上一次的valuemget(key1, key2,…, key N)：返回库中多个string的valuesetnx(key, value)：添加string，名称为key，值为valuesetex(key, time, value)：向库中添加string，设定过期时间timemset(key N, value N)：批量设置多个string的值msetnx(key N, value N)：如果所有名称为key i的string都不存在incr(key)：名称为key的string增1操作incrby(key, integer)：名称为key的string增加integerdecr(key)：名称为key的string减1操作decrby(key, integer)：名称为key的string减少integerappend(key, value)：名称为key的string的值附加valuesubstr(key, start, end)：返回名称为key的string的value的子串 List 12345678910111213rpush(key, value)：在名称为key的list尾添加一个值为value的元素lpush(key, value)：在名称为key的list头添加一个值为value的 元素llen(key)：返回名称为key的list的长度lrange(key, start, end)：返回名称为key的list中start至end之间的元素ltrim(key, start, end)：截取名称为key的listlindex(key, index)：返回名称为key的list中index位置的元素lset(key, index, value)：给名称为key的list中index位置的元素赋值lrem(key, count, value)：删除count个key的list中值为value的元素lpop(key)：返回并删除名称为key的list中的首元素rpop(key)：返回并删除名称为key的list中的尾元素blpop(key1, key2,… key N, timeout)：lpop命令的block版本。brpop(key1, key2,… key N, timeout)：rpop的block版本。rpoplpush(srckey, dstkey)：返回并删除名称为srckey的list的尾元素，并将该元素添加到名称为dstkey的list的头部 Set 1234567891011121314sadd(key, member)：向名称为key的set中添加元素membersrem(key, member) ：删除名称为key的set中的元素memberspop(key) ：随机返回并删除名称为key的set中一个元素smove(srckey, dstkey, member) ：移到集合元素scard(key) ：返回名称为key的set的基数sismember(key, member) ：member是否是名称为key的set的元素sinter(key1, key2,…key N) ：求交集sinterstore(dstkey, (keys)) ：求交集并将交集保存到dstkey的集合sunion(key1, (keys)) ：求并集sunionstore(dstkey, (keys)) ：求并集并将并集保存到dstkey的集合sdiff(key1, (keys)) ：求差集sdiffstore(dstkey, (keys)) ：求差集并将差集保存到dstkey的集合smembers(key) ：返回名称为key的set的所有元素srandmember(key) ：随机返回名称为key的set的一个元素 Hash 1234567891011hset(key, field, value)：向名称为key的hash中添加元素fieldhget(key, field)：返回名称为key的hash中field对应的valuehmget(key, (fields))：返回名称为key的hash中field i对应的valuehmset(key, (fields))：向名称为key的hash中添加元素fieldhincrby(key, field, integer)：将名称为key的hash中field的value增加integerhexists(key, field)：名称为key的hash中是否存在键为field的域hdel(key, field)：删除名称为key的hash中键为field的域hlen(key)：返回名称为key的hash中元素个数hkeys(key)：返回名称为key的hash中所有键hvals(key)：返回名称为key的hash中所有键对应的valuehgetall(key)：返回名称为key的hash中所有的键（field）及其对应的value]]></content>
      <tags>
        <tag>JAVA</tag>
        <tag>REDIS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种基本排序算法]]></title>
    <url>%2F2017%2F08%2F25%2Fsort%2F</url>
    <content type="text"><![CDATA[冒泡排序基本原理每一轮从头开始两两比较，将较大的项放在较小项的右边，这样每轮下来保证该轮最大的数在最右边。 算法实现123456789public static void bubbleSort(int[] source) &#123; for(int i = source.length - 1; i &gt; 0; i--) &#123; for(int j = 0; j &lt; i; j ++) &#123; if(source[j] &gt; source[j+1]) &#123; swap(source, j, j + 1); &#125; &#125; &#125; &#125; 算法示例待排序113 6 29 3 排序过程1231:6 13 3 29 //13和6，交换；13和29，不交换；29和3，交换2:6 3 13 29 //6和13，不交换；13和3，交换3:3 6 13 29 //6和3，交换 冒泡排序算法还有个可以改进的地方，就是在算法中加入一个布尔变量标识该轮有没有进行数据的交换，若在某一次排序中未发现气泡位置的交换，则说明待排序的无序区中所有的项均已满足排序后的结果。 123456789101112public static void bubbleSort(int[] source) &#123; for(int i = source.length - 1; i &gt; 0; i--) &#123; boolean exchange = false; for(int j = 0; j &lt; i; j ++) &#123; if(source[j] &gt; source[j+1]) &#123; swap(source, j, j + 1); exchange = true; &#125; &#125; if(!exchange) return; &#125; &#125; 算法分析 时间复杂度：冒泡排序最好的情况是初始状态是正序的，一次扫描即可完成排序，所以最好的时间复杂度为O(N)；最坏的情况是反序的，此时最坏的时间复杂度为O(N2)。平均情况，每轮N/2次循环，N轮时间复杂度为O(N2)。 稳定性：算法是稳定的，因为当a=b时，由于只有大于才做交换，故a和b的位置没有机会交换，所以算法稳定。 空间复杂度：空间复杂度为O(1)，不需要额外空间。 选择排序基本原理选择排序改进了冒泡排序，将必要的交换次数从O(n2)减少到O(n)，但是比较次数仍保持为O(n2)。冒泡排序每比较一次就可能交换一次，但是选择排序是将一轮比较完后，把最小的放到最前的位置（或者把最大的放到最后）。 算法实现1234567891011public static void selectSort(int[] source) &#123; int min; for(int i = 0; i &lt; source.length - 1; i++) &#123; min = i; for(int j = i + 1; j &lt; source.length; j++) &#123; if(source[j] &lt;source[min]) min = j; &#125; swap(source, i, min); &#125; &#125; 算法示例待排序113 6 29 3 排序过程1231:3 6 29 13 //第一遍，3最小，13和3交换2:3 6 29 13 //第二遍，6最小，不需要交换3:3 6 13 29 //第三遍，13最小，交换 算法分析 时间复杂度：选择排序最好和最坏的情况一样运行了O(N2)时间，但是选择排序无疑更快，因为它进行的交换少得多，当N值较小时，特别是如果交换时间比比较时间大得多时，选择排序实际上是相当快的。平均复杂度也是O(N2)。 稳定性：算法是不稳定的，假设a=b，且a在b前面，而某轮循环中最小值在b后面，而次最小值需要跟a交换，这时候b就在a前面了，所以选择排序是不稳定的。 空间复杂度：空间复杂度为O(1)，不需要额外的空间。 插入排序基本原理插入排序的实现步骤为：从第一个元素开始，该元素可以认为已经被排序 -&gt; 取出下一个元素，在已经排序的元素序列中从后向前扫描 -&gt; 如果该元素小于前一个元素，则将两者调换，再与前一个元素比较–&gt; 重复第三步，直到找到已排序的元素小于或者等于新元素的位置 -&gt; 将新元素插入到该位置中 -&gt; 重复第二步 算法实现123456789public static void insertSort(int[] source) &#123; for(int i = 1; i &lt; source.length; i++) &#123; for(int j = i; (j &gt; 0) &amp;&amp; (source[j] &lt; source[j-1]); j--) &#123; swap(source, j, j-1); &#125; sortNum ++; print(true); &#125; &#125; 算法示例待排序113 6 29 3 排序过程1231:6 13 29 3 //第一遍，位置2开始，13比6大，不需要交换2:6 13 29 3 //第二遍，29分别比13和6大，不需要交换3:3 6 13 29 //第三遍，3比29小，交换；3比13小，交换；3比6小，交换 算法分析 时间复杂度：插入排序最好的情况是序列已经是升序排列了，在这种情况下，需要进行N-1次比较即可，时间复杂度为O(N)，最坏的情况是序列降序排列，这时候时间复杂度为O(N2)。因此插入排序不适合对于数据量比较大的排序应用。但是如果需要排序的数据量很小(如小于千)，那么插入排序还是一个不错的选择。插入排序平均时间复杂度为O(N2)，但是它要比冒泡排序快一倍，比选择排序还要快一点，经常被用在较复杂的排序算法的最后阶段，例如快速排序。 稳定性：算法是稳定的，假设a=b，且a在b的前面，其排序位置必然比b先确定，而后面再插入b时，必然在a的后面，所以是稳定的。 空间复杂度：空间复杂度为O(1)，不需要额外的空间。 注：冒泡最大的，选择最小的，插入到已排好序的 希尔排序基本原理希尔排序是基于插入排序的，插入排序有个弊端，假设一个很小的数据项在很靠近右端的位置上，那么所有的中间数据项都必须向右移动一位，这个步骤对每一个数据项都执行了将近N次的复制，这也是插入排序效率为O(N2)的原因。 希尔排序的中心思想是将数据进行分组，然后对每一组数据进行插入排序，在每一组数据都有序后，再对所有的分组利用插入排序进行最后一次排序。这样可以显著减少数据交换的次数，以达到加快排序速度的目的。 算法实现12345678910111213141516171819public static void shellSort(int[] source) &#123; int h = 1; int nElem = source.length; while(h &lt;= nElem / 2) &#123; h = h * 2 + 1; &#125; while(h &gt; 0) &#123; for(int i = h; i &lt; nElem; i++) &#123; //insert sort for(int j = i; j &lt; nElem; j += h) &#123; for(int k = j; (k - h &gt;= 0) &amp;&amp; source[k] &lt; source[k - h]; k -= h) &#123; swap(source, k, k - h); &#125; &#125; &#125; h = (h-1)/2; &#125; &#125; 这种思想需要依赖一个增量序列，我们称为n-增量，n表示进行排序时数据项之间的间隔，习惯上用h表示。增量序列在希尔排序中是很重要的。一般好的增量序列都有2个共同的特征： 最后一个增量必须为1，保证最后一趟是一次普通的插入排序；应该尽量避免序列中的值（尤其是相邻的值）互为倍数的情况 算法示例待排序113 6 29 3 38 排序过程，这里选取的间隔为3和11234561:3 6 29 13 38 // 间隔为3，13和3，交换2:3 6 29 13 38 // 间隔为3，6和38不交换3:3 6 29 13 38 // 间隔为1，3和6不交换4:3 6 29 13 38 // 间隔为1，29用插入排序比6，3都大，不交换5:3 6 13 29 38 // 间隔为1，13通过插入排序和29交换，后面都类普通插入排序，省略... 算法分析 时间复杂度：希尔排序时间复杂度平均为O(NlogN)，最好与最坏情况要根据具体的增量序列来判断，对于不同的增量序列有不同的复杂度。希尔排序的性能优于直接插入排序，因为在希尔排序开始时增量较大，分组较多，每组的记录数目少，故各组内直接插入较快，后来随着增量逐渐缩小，分组数逐渐减少，而各组的记录数目逐渐增多，但是由于已经局部排过序了，所以已经接近有序状态，新的一趟排序过程也较快。因此，希尔排序在效率上较直接插入排序有较大的改进。 稳定性：希尔排序是不稳定的，因为不同的间隔对应的数据是独自比较的，如果a=b，但是不在同一个间隔上，显然会出现前后颠倒的情况，所以希尔排序是不稳定的。 空间复杂度： 空间复杂度为O(1)，不需要额外的存储空间。 快速排序基本原理快速排序本质上通过一个数组划分为两个子数组，然后递归地调用自身为每一个子数组进行快速排序来实现的，即算法分为三步： 把数组或者子数组划分为左边（较小的关键字）的一组和右边（较大的关键字）的一组； 调用自身对左边的一组进行排序； 调用自身对右边的一组进行排序。 快速排序需要划分数组，这就用到了划分算法。划分算法是由两个指针开始工作，两个指针分别指向数组的两头，左边的指针leftPtr向右移动，右边的指针rightPtr向左移动。当leftPtr遇到比枢纽（待比较的数据项，比其小的在其左边，比其大的在其右边，下面均称之为“枢纽”）小的数据项时继续右移，当遇到比枢纽大的数据项时就停下来；类似的rightPtr想反。两边都停下后，leftPtr和rightPtr都指在数组的错误一方的位置的数据项，交换这两个数据项。交换后继续移动这两个指针。 算法实现123456789101112131415161718192021222324252627282930313233public static void quickSort(int[] a) &#123; recQuickSort(a,0, a.length-1); &#125; public static void recQuickSort(int[] a, int left, int right) &#123; if(right-left &lt;= 0) &#123; return; &#125; else &#123; int pivot = a[right]; //保存最右边的值，以这个值作为划分点 int partition = partitionIt(a,left, right, pivot);//将数组划分两部分，并将划分点的值放在正确位置，并返回该位置 recQuickSort(a, left, partition-1);//调用自身对左边进行排序 recQuickSort(a, partition+1, right);//调用自身对右边进行排序 &#125; &#125; public static int partitionIt(int[] a, int left, int right, int pivot) &#123; int leftPtr = left - 1; int rightPtr = right; while(true) &#123; while(a[++leftPtr] &lt; pivot)&#123;&#125; //从左到右，找比待比较大的 while(rightPtr &gt; 0 &amp;&amp; a[--rightPtr] &gt; pivot)&#123;&#125; //从右到左，找比待比较的小的 if(leftPtr &gt;= rightPtr) break; else &#123; Logs.info(a[leftPtr] + " " + a[rightPtr]); swap(a, leftPtr, rightPtr); &#125; &#125; Logs.info(a[leftPtr] + " " + a[right]); swap(a, leftPtr, right);//将划分放在正确的位置 print(true); return leftPtr;//返回划分点，用于再次小范围划分 &#125; 算法示例待比较1source = &#123;13,6,29, 3, 15&#125; 排序过程1231:13 6 3 15 29 // 左边比15大的为29，右边比15小的为3，29和3，交换；重复，直到左右位置相同，记录位置2:3 6 13 15 29 // 上一步中位置3为中间位置，13,6,3，重复上面步骤3:3 6 13 15 29 // 右边15,19 重复第一步 算法分析 时间复杂度：平均时间复杂度为O(NlogN)，最坏的情况下退化成插入排序了，为O(N2)。 稳定性：不稳定的排序方法。 空间复杂度：空间复杂度平均为O(logN)，空间复杂度主要是由于递归造成的。 归并排序基本原理 归并排序的思想是把一个数组分成两半，排序每一半。然后用merge方法将数组的两半归并成一个有序的数组。被分的每一半使用递归，再次划分排序，直到得到的子数组只含有一个数据项为止。 算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243public static void mergeSort(int[] source) &#123; int[] workSpace = new int[source.length]; recMergeSort(source,workSpace, 0, source.length-1); &#125; private static void recMergeSort(int[] source, int[] workSpace, int lowerBound, int upperBound) &#123; if(lowerBound == upperBound) &#123; return; &#125; else &#123; int mid = (lowerBound + upperBound) / 2; recMergeSort(source, workSpace, lowerBound, mid); //左边排 recMergeSort(source, workSpace, mid+1, upperBound); //右边排 merge(source, workSpace, lowerBound, mid+1, upperBound);//归并 print(true); &#125; &#125; private static void merge(int[] a, int[] workSpace, int lowPtr, int highPtr, int upperBound) &#123; int j = 0; int lowerBound = lowPtr; int mid = highPtr - 1; int n = upperBound - lowerBound + 1; while(lowPtr &lt;= mid &amp;&amp; highPtr &lt;= upperBound) &#123; if(a[lowPtr] &lt; a[highPtr]) &#123; workSpace[j++] = a[lowPtr++]; &#125; else &#123; workSpace[j++] = a[highPtr++]; &#125; &#125; while(lowPtr &lt;= mid) &#123; workSpace[j++] = a[lowPtr++]; &#125; while(highPtr &lt;= upperBound) &#123; workSpace[j++] = a[highPtr++]; &#125; for(j = 0; j &lt; n; j++) &#123; a[lowerBound + j] = workSpace[j]; &#125; &#125; 算法示例待排序113 6 29 3 15 排序过程，先分为两堆，13,6,29一个数组，3,15一个数组；13,6,29再分为，13,6和2912341:6 13 29 3 15 // 13和6排序2:6 13 29 3 15 // 6，13，29排序3:6 13 29 3 15 // 3，15排序4:3 6 13 15 29 // 两个数组合并 算法分析 时间复杂度：归并排序的运行时间最差、最好和平均都是O(NlogN) 稳定性：归并排序是稳定的，由于没有发生数据交换 空间复杂度：空间复杂度为O(N) 二叉树排序基本原理二叉树排序就是利用二叉搜索树的特点进行排序，二叉搜索树的特点是，左子节点比自己小，右子节点比自己大，那么二叉树排序的思想就是先将待排序序列逐个添加到二叉搜索树中去，再通过中序遍历二叉搜索树就可以将数据从小到大取出来。 算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Node &#123; public int value; Node leftChild; Node rightChild; public Node(int val) &#123; value = val; &#125; &#125; public class Tree2Sort &#123; private Node root; public Tree2Sort() &#123; root = null; &#125; public Node getRoot() &#123; return root; &#125; public void insertSort(int[] source) &#123; for(int i = 0; i &lt; source.length; i++) &#123; int value = source[i]; Node node = new Node(value); if(root == null) &#123; root = node; &#125; else &#123; Node current = root; Node parent; boolean insertedOK = false; while(!insertedOK) &#123; parent = current; if(value &lt; current.value) &#123; current = current.leftChild; if(current == null) &#123; parent.leftChild = node; insertedOK = true; &#125; &#125; else &#123; current = current.rightChild; if(current == null) &#123; parent.rightChild = node; insertedOK = true; &#125; &#125; &#125; &#125; &#125; &#125; public void inOrder(Node current) &#123; if(current != null) &#123; inOrder(current.leftChild); System.out.print(current.value + " "); inOrder(current.rightChild); &#125; &#125; &#125; 算法示例略 算法分析 时间复杂度：二叉树的插入时间复杂度为O(logN)，所以二叉树排序算法的时间复杂度为O(NlogN)， 稳定性：稳定 空间复杂度：空间复杂度为O(N) 堆排序基本原理堆是一个完全二叉树，节点大于或等于自己的子节点。堆排序就是利用完全二叉树的结构将待排序的数据项依次添加到堆中，建立大根堆或者小根堆，从堆中取出的数据项是从大到小或从小到大排列的。因为根节点永远是最大或最小的，而堆中永远是取根节点。 算法实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class HeapSort &#123; private static int[] array; private static int currentIndex; private static int maxSize; public static void setHeapSort(int size) &#123; maxSize = size; array = new int[maxSize]; currentIndex = 0; &#125; public static void insertSort(int[] source) &#123; for(int i = 0; i &lt; source.length; i++) &#123; array[currentIndex] = source[i]; //插入到节点尾 tickedUp(currentIndex++); //向上重新排好序，使得满足堆的条件 &#125; &#125; private static void tickedUp(int index) &#123; int parentIndex = (index - 1) / 2; //父节点的索引 int temp = array[index]; //将新加的尾节点存在temp中 while(index &gt; 0 &amp;&amp; array[parentIndex] &lt; temp) &#123; array[index] = array[parentIndex]; index = parentIndex; parentIndex = (index - 1) / 2; &#125; array[index] = temp; &#125; public static int getMax() &#123; int maxNum = array[0]; array[0] = array[--currentIndex]; trickleDown(0); return maxNum; &#125; private static void trickleDown(int index) &#123; int top = array[index]; int largeChildIndex; while(index &lt; currentIndex/2) &#123; //while node has at least one child int leftChildIndex = 2 * index + 1; int rightChildIndex = leftChildIndex + 1; //find larger child if(rightChildIndex &lt; currentIndex &amp;&amp; //rightChild exists? array[leftChildIndex] &lt; array[rightChildIndex]) &#123; largeChildIndex = rightChildIndex; &#125; else &#123; largeChildIndex = leftChildIndex; &#125; if(top &gt;= array[largeChildIndex]) &#123; break; &#125; array[index] = array[largeChildIndex]; index = largeChildIndex; &#125; array[index] = top; &#125; static private int[] source = &#123;13,6,29, 3, 15&#125;; public static void main(String[] args) &#123; setHeapSort(5); insertSort(source); for (int i = 0; i &lt; maxSize; i ++) &#123; Logs.info(getMax()); &#125; &#125;&#125; 算法示例略 算法分析 时间复杂度：堆中插入和取出的时间复杂度均为O(logN)，所以堆排序算法的时间复杂度为O(NlogN) 稳定性：稳定 空间复杂度：空间复杂度为O(N) 拓扑排序基本原理基于有向图的排序，见拓扑排序 动图演示 参考：常用数据结构和算法操作效率的对比总结]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cookie和Session原理]]></title>
    <url>%2F2017%2F08%2F25%2Fcookie-session%2F</url>
    <content type="text"><![CDATA[Cookie和Session是为了在无状态的HTTP协议之上维护会话状态，使得服务器可以知道当前是和哪个客户在打交道。本文来详细讨论Cookie和Session的实现机制，以及其中涉及的安全问题。 因为HTTP协议是无状态的，即每次用户请求到达服务器时，HTTP服务器并不知道这个用户是谁、是否登录过等。现在的服务器之所以知道我们是否已经登录，是因为服务器在登录时设置了浏览器的Cookie。Session则是借由Cookie而实现的更高层的服务器与浏览器之间的会话。 Cookie 的实现机制Cookie是由客户端保存的小型文本文件，其内容为一系列的键值对。 Cookie是由HTTP服务器设置的，保存在浏览器中， 在用户访问其他页面时，会在HTTP请求中附上该服务器之前设置的Cookie。 那么Cookie是怎样工作的呢？下面给出整个Cookie的传递流程： 浏览器向某个URL发起HTTP请求 对应的服务器收到该HTTP请求，并计算应当返回给浏览器的HTTP响应。 在响应头加入Set-Cookie字段，它的值是要设置的Cookie。 浏览器收到来自服务器的HTTP响应。 浏览器在响应头中发现Set-Cookie字段，就会将该字段的值保存在内存或者硬盘中。Set-Cookie字段的值可以是很多项Cookie，每一项都可以指定过期时间Expires。 默认的过期时间是用户关闭浏览器时。 浏览器下次给该服务器发送HTTP请求时， 会将服务器设置的Cookie附加在HTTP请求的头字段Cookie中。浏览器可以存储多个域名下的Cookie，但只发送当前请求的域名曾经指定的Cookie， 这个域名也可以在Set-Cookie字段中指定）。 服务器收到这个HTTP请求，发现请求头中有Cookie字段， 便知道之前就和这个用户打过交道了。 总之，服务器通过Set-Cookie响应头字段来指示浏览器保存Cookie， 浏览器通过Cookie请求头字段来告诉服务器之前的状态。 Cookie中包含若干个键值对，每个键值对可以设置过期时间。 Cookie 的安全隐患和防篡改发送HTTP请求的不只是浏览器，很多HTTP客户端软件（包括curl、Node.js）都可以发送任意的HTTP请求，可以设置任何头字段。 假如我们直接设置Cookie字段并发送HTTP请求， 就可以欺骗服务器岂，这种攻击非常容易，Cookie是可以被篡改的！ 为加强安全，服务器可以单独为每个Cookie项生成签名，由于用户篡改Cookie后无法生成对应的签名， 服务器便可以得知用户对Cookie进行了篡改。 例如，Set-Cookie: authed=false|6hTiBl7lVpd1P为authed项为false时生成一个加密的签名，客户端可以随意篡改authed字段，但是无法生成authed为false时的签名，服务端校验失败。 但是因为Cookie是明文传输的， 只要服务器设置过一次authed=true|xxxx我不就知道true的签名是xxxx了么， 以后就可以用这个签名来欺骗服务器了。因此Cookie中最好不要放敏感数据。 一般来讲Cookie中只会放一个Session Id，而Session存储在服务器端。 Session 的实现机制Session 是存储在服务器端的，避免了在客户端Cookie中存储敏感数据。 Session 可以存储在HTTP服务器的内存中，也可以存在内存数据库（如redis）中， 对于重量级的应用甚至可以存储在数据库中。 用户提交包含用户名和密码的表单，发送HTTP请求。 服务器验证用户发来的用户名密码。如果正确则把当前用户名（通常是用户对象）存储到redis中，并生成它在redis中的ID。 设置Cookie为sessionId=xxxxxx|checksum并发送HTTP响应， 仍然为每一项Cookie都设置签名。 用户收到HTTP响应后，便看不到任何敏感数据了。在此后的请求中发送该Cookie给服务器。 服务器收到此后的HTTP请求后，发现Cookie中有SessionID，进行放篡改验证。 如果通过了验证，根据该ID从Redis中取出对应的用户对象， 查看该对象的状态并继续执行业务逻辑。Web应用框架都会实现上述过程，在Web应用中可以直接获得当前用户。 相当于在HTTP协议之上，通过Cookie实现了持久的会话。这个会话便称为Session。 flask中cookie和session的使用12345678910111213141516main.secret_key = 'A0Zr98j/3yX R~XHH!jmN]LWX/,?RT'@main.route('/add')def login(): res = make_response(render_template("index.html")) res.set_cookie(key='username', value='letian') session['name'] = 'tom' return res@main.route('/show')def show(): print request.cookies.get('username') if 'name' in session: print session['name'] return request.cookies.get('username') 客户端add的时候，分别在cookie中设置username，在session中设置name，使用session需要设置secret_key；客户端的show请求中，我们就能看到cookie中同时包含username：letian和session字段，在session中能解析到name字段。]]></content>
      <tags>
        <tag>WEB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos下安装Grunt]]></title>
    <url>%2F2017%2F08%2F21%2Fcentos-grunt%2F</url>
    <content type="text"><![CDATA[Grunt是一个自动化的项目构建工具. 如果你需要重复的执行像压缩, 编译, 单元测试, 代码检查以及打包发布的任务。 CentOS 下安装 Node.js1、下载源码，你需要在https://nodejs.org/en/download/下载最新的Nodejs版本，本文以v0.10.24为例: 12cd /usr/local/src/wget http://nodejs.org/dist/v0.10.24/node-v0.10.24.tar.gz 2、解压源码 1tar zxvf node-v0.10.24.tar.gz 3、 编译安装 1234cd node-v0.10.24./configure --prefix=/usr/local/node/0.10.24makemake install 4、 配置NODE_HOME，进入profile编辑环境变量vim /etc/profile 设置nodejs环境变量，添加如下内容: 123#set for nodejsexport NODE_HOME=/usr/local/node/0.10.24export PATH=$NODE_HOME/bin:$PATH :wq保存并退出，编译/etc/profile 使配置生效 1source /etc/profile 验证是否安装配置成功node -v，输出 v0.10.24 表示配置成功。 安装NPM装好NodeJS后，可以在你的终端执行下面的命令安装NPM： 1curl http://npmjs.org/install.sh | sh 检查是否安装成功npm -v Grunt安装Grunt和Grunt插件都是通过npm, Node.js包管理器安装和管理的。 1npm install -g grunt-cli 这条命令将会把grunt命令植入到你的系统路径中，这样就允许你从任意目录中运行Grunt(定位到任意目录运行grunt命令)。 查看是否安装成功grunt --version 1grunt-cli v1.2.0 注意：安装grunt-cli并不等于安装了Grunt任务运行器! Grunt CLI的工作很简单：在Gruntfile所在目录调用运行已安装好的相应版本的Grunt。这就意味着可以在同一台机器上同时安装多个版本的Grunt。]]></content>
      <tags>
        <tag>JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[p2p之NAT详解]]></title>
    <url>%2F2017%2F08%2F21%2Fp2p-nat%2F</url>
    <content type="text"><![CDATA[NAT简介&emsp;&emsp;NAT，网络地址转换，就是替换IP报文头部的地址信息。NAT通常部署在一个组织的网络出口位置，通过将内部网络IP地址替换为出口的IP地址提供公网可达性和上层协议的连接能力。&emsp;&emsp;NAT的主要目的是解决IPV4缺少带来的问题，有了NAT技术，就可以在少量的公网IP和内网地址（10.0.0.0-10.255.255.255；172.16.0.0-172.31.255.255；192.168.0.0-192.168.255.255）之间建立映射关系。 NAT分类&emsp;&emsp;NAT共分为两大类：Cone NAT和Symmetric NAT。Cone NAT指的是只要源IP端口不变，无论发往的目的IP是否相同，在NAT上都映射为同一个端口，形象的看来就像锥子一样，而SymmetricNAT对于发往不同目的IP的会话在NAT上将映射为不同的端口，也就是不同的会话。 其中Cone NAT又可细分为3类，分别是Full Cone型（全锥形）、Restricted Cone型（受限锥形）和Restricted Port Cone（端口受限锥形）。限制的严格程度和对局域网内主机的保护由松到紧依次为：Full Cone、RestrictedCone、Restricted Port Cone、Symmetric NAT。“限制”指的是NAT对由外到内的数据包进行审查、过滤，看看数据包的源地址和他发送到的“洞”是否有关系，如果没有那么就将其丢弃。由内向外的限制有防火墙管理。 全锥形NATIP、端口都不受限。NAT设备会将客户端的{X, y}转换成公网地址的{A, b}并绑定对应关系，任何数据包通过地址{A,b}都将送到客户机的{X,y}上。 受限锥形NAT IP受限，端口不受限。NAT设备会将客户端的{X, y}转换成公网地址的{A, b}并绑定对应关系，只有来自于{P}这个ip地址的包才能和主机{X, y}通信。 端口受限锥型NATIP、端口都受限。NAT设备会将客户端的{X, y}转换成公网地址的{A, b}并绑定{X, y}，{A,b}和{P,q}的对应关系，NAT只会接受来自{P,q}的数据包，并将其转发到{X,y}。如果{X，y}改为向{M,n}发送数据，则{P,q}再向{A,b}发送的数据不会被接受。 对称型NAT 对称型NAT具有端口受限锥型的受限特性。但更重要的是，他对每个外部主机或端口的会话都会映射为不同的端口（洞）。NAT设备会将客户端的{X, y}转换成公网地址的{A, b}并绑定{X, y}，{A,b}和{P,q}的对应关系，NAT只会接受来自{P,q}的数据包，并将其转发到{X,y}，当请求一个新的地址{M,n}时，NAT设备会重新分配一个新的ip，端口{C,d}。 NAT弊端在理论上，具有IP地址的每个站点在协议层面有相当的获取服务和提供服务的能力，不同的IP地址之间没有差异。NAT技术的出现破坏了IP端到端通信的能力。首先，NAT使IP会话的保持时效变短。因为一个会话建立后会在NAT设备上建立一个关联表，在会话静默的这段时间，NAT网关会进行超时回收资源的操作。所以应用协议必须有报活的协议才能保持会话。其次，NAT在实现上将多个内部主机发出的连接复用到一个IP上，这就使依赖IP进行主机跟踪的机制都失效了。如网络管理中需要的基于网络流量分析的应用无法跟踪到终端用户与流量的具体行为的关系。基于用户行为的日志分析也变得困难，因为一个IP被很多用户共享，如果存在恶意的用户行为，很难定位到发起连接的那个主机。此外，NAT工作机制依赖于修改IP包头的信息，这会妨碍一些安全协议的工作。因为NAT篡改了IP地址、传输层端口号和校验和，这会导致认证协议彻底不能工作，因为认证目的就是要保证这些信息在传输过程中没有变化。总之，因为NAT隐蔽了通信的一端，把简单的事情复杂化了。 NAT穿透技术&emsp;&emsp;为了解决IP端到端应用在NAT环境下遇到的问题，网络协议的设计者们创造了很多工具来应对，其中主要有应用层网关，探针技术STUN和TURN，中间件技术，中继代理技术，特定协议的自穿越技术。这里主要介绍探针技术STUN和TURN。 &emsp;&emsp;所谓探针技术STUN，是通过在所有参与通信的实体上安装探测插件，以检测网络中是否存在NAT网关，并对不同NAT模型实施不同穿越方法的一种技术。所谓中继服务TURN，是通过转发请求实现两台具有NAT设备的内网IP建立链接。 NAT类型识别&emsp;&emsp;STUN服务器上两个公网IP分别A1,A2，同时绑定两个端口P1,P2。在主机上使用两个IP主要是测试客户机是否是对称型NAT和时候是IP受限制型NAT。主要测试流程如下： 使用端口1发送消息给A1:P1，测试能否响应，能响应则UDP协议可以通过 使用端口1发送消息给A2:P2，比较响应中的地址和上一步返回的地址是否一致，验证NAT是否对不通的目的地址进行相同的映射 使用端口1发送消息给第一部中返回的NAT上映射的外网IP和端口，测试是否有回应，验证NAT是否回环 使用端口2发送消息给A1:P1，服务器端使用A2:P1回复，验证是否为IP受限NAT 使用端口2发送消息给A1:P1，服务器端使用A1:P2回复，验证是否为端口受限NAT 同时，通过以上服务器返回的客户端IP，可以有很多方法确定是否是公网IP，如绑定这个地址或者与本地网卡中IP地址比较 NAT穿透其中一个是公网（5种）客户端A是公网，A通知服务器S希望与B建立通信；S将消息转给B；B主动连接A，NAT上留下与A通信用的映射关系；A就可以与B进行正常通信，打洞成功。这种情况下，无论B是什么类型，都能成功建立连接。 其中一个是FC（4种）客户端A是FC类型，A通知S与B建立通信；S通知B想A已经存在的洞建立连接；A收到B的连接包，打洞成功。这种情况下，因为A主要在NAT上有对应关系，则任何其他地址都可以通着这个对应关系与A建立连接 其中一个是RC（3种） RC和RC，A连接S希望连B，NAT上映射关系Na只能与S通信；B和S也有保存会话的NAT映射关系Nb，此时B去连接A上的Na，失败，但是Nb属性更改为与A的连接，此时A连接B可以成功，同时Na的属性也更改为与B的连接，打洞成功。（注：真正使用可能不会复用与服务器的报端连接，而是A和B同时连接对方，后连接的以方可以打洞成功） RC和PC，与RC和RC类似 RC和SN，B向A不通；然后A向B发包，不通，但A上留下一个可以给B使用的洞；B在使用新的映射连接A，A可以收到，并拿到B上留给A的洞的信息；A回复B，连接建立。 其中一个是PC（2种） PC和PC，同RC和RC。B连A，丢弃； A连B，通过；B回复A，连接建立 PC和SN，与RC和SN的过程类似，但是在第三步，B使用新的映射连接A，A无法收到B的消息，因为A是端口受限，所以PC和SN无法打洞成功。 其中一个是SN（1种） SN和SN，无法成功 类型 公网 FC RC PC SN 公网 Y Y Y Y Y FC Y Y Y Y Y RC Y Y Y Y Y PC Y Y Y Y N SN Y Y Y N N 参考：P2P之NAT穿透NAT详解——详细原理、P2P简介]]></content>
      <tags>
        <tag>P2P</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark或Hadoop集群无法停止]]></title>
    <url>%2F2017%2F07%2F06%2Fspark-no-stop%2F</url>
    <content type="text"><![CDATA[现象执行sbin/stop-all.sh，提示： 12345Stopping namenodes on [master]master: no namenode to stopslave1: no datanode to stopStopping secondary namenodes [master]master: no secondarynamenode to stop jps显示进程都在： 12345625280 Jps4290 Master21235 SecondaryNameNode21395 ResourceManager15687 Master21052 NameNode 原因分析以Spark为例，Spark启动停止都是通过hadoop-daemon.sh文件，其中部分代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125...# some variableslog="$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out"pid="$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid"# Set default scheduling priorityif [ "$SPARK_NICENESS" = "" ]; then export SPARK_NICENESS=0fiexecute_command() &#123; if [ -z $&#123;SPARK_NO_DAEMONIZE+set&#125; ]; then nohup -- "$@" &gt;&gt; $log 2&gt;&amp;1 &lt; /dev/null &amp; newpid="$!" echo "$newpid" &gt; "$pid" # Poll for up to 5 seconds for the java process to start for i in &#123;1..10&#125; do if [[ $(ps -p "$newpid" -o comm=) =~ "java" ]]; then break fi sleep 0.5 done sleep 2 # Check if the process has died; in that case we'll tail the log so the user can see if [[ ! $(ps -p "$newpid" -o comm=) =~ "java" ]]; then echo "failed to launch: $@" tail -2 "$log" | sed 's/^/ /' echo "full log in $log" fi else "$@" fi&#125;run_command() &#123; mode="$1" shift mkdir -p "$SPARK_PID_DIR" if [ -f "$pid" ]; then TARGET_ID="$(cat "$pid")" if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then echo "$command running as process $TARGET_ID. Stop it first." exit 1 fi fi if [ "$SPARK_MASTER" != "" ]; then echo rsync from "$SPARK_MASTER" rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "$&#123;SPARK_HOME&#125;" fi spark_rotate_log "$log" echo "starting $command, logging to $log" case "$mode" in (class) execute_command nice -n "$SPARK_NICENESS" "$&#123;SPARK_HOME&#125;"/bin/spark-class "$command" "$@" ;; (submit) execute_command nice -n "$SPARK_NICENESS" bash "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class "$command" "$@" ;; (*) echo "unknown mode: $mode" exit 1 ;; esac&#125;case $option in (submit) run_command submit "$@" ;; (start) run_command class "$@" ;; (stop) if [ -f $pid ]; then TARGET_ID="$(cat "$pid")" if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then echo "stopping $command" kill "$TARGET_ID" &amp;&amp; rm -f "$pid" else echo "no $command to stop" fi else echo "no $command to stop" fi ;; (status) if [ -f $pid ]; then TARGET_ID="$(cat "$pid")" if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then echo $command is running. exit 0 else echo $pid file is present but $command not running exit 1 fi else echo $command not running. exit 2 fi ;; (*) echo $usage exit 1 ;;esac 可以看到，启动是会生成pid文件，停止时会读取pid文件，并kill &quot;$TARGET_ID&quot; &amp;&amp; rm -f &quot;$pid&quot;，在pid文件在不定义是，默认存放目录值tmp，linux系统默认每30天清理一次/tmp目录下的文件。pid文件丢失将导致无法正确关闭相应进程。 解决方法pid文件的默认文件名格式如下： 1pid="$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid" 通过代码可以知道， 1234$SPARK_PID_DIR = /tmp$SPARK_IDENT_STRING = hdfs #username$command = org.apache.spark.deploy.master.Master # or worker$instance = 1 所以，我们只需要找到对应进程的进程号，创建文件并添加就可以正常关闭进程 根治方法既然tmp目录会被系统定时清理，那么我们重新设置对应服务的pid存放路径即可 修改hadoop-env.sh，增加： 1export HADOOP_PID_DIR=/opt/hadoop/appid/ 修改spark-env.sh，增加： 1export SPARK_PID_DIR=/opt/hadoop/appid/ 重启对应服务： 1234-rw-rw-r-- 1 hdfs hdfs 6 Jul 6 15:54 hadoop-hdfs-datanode.pid-rw-rw-r-- 1 hdfs hdfs 6 Jul 6 15:54 hadoop-hdfs-journalnode.pid-rw-rw-r-- 1 hdfs hdfs 6 Jul 6 15:44 spark-hdfs-org.apache.spark.deploy.worker.Worker-1.pid-rw-rw-r-- 1 hdfs hdfs 6 Jul 6 15:54 yarn-hdfs-nodemanager.pid 通过这个问题我们可以知道，很对linux下对应的服务都有类似的控制脚本，或者可以添加类似的控制脚本，但是在存放pid文件是我们也应该主要到这个问题。防止因为系统原因，导致服务无法正常重启等问题。]]></content>
      <tags>
        <tag>SPARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j2使用]]></title>
    <url>%2F2017%2F07%2F06%2Flog4j2%2F</url>
    <content type="text"><![CDATA[基本使用pom.xml中添加依赖 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 使用示例： 123456789public static void main(String[] args) &#123; Logger logger = LogManager.getLogger(Logs.class.getName()); logger.trace("trace level"); logger.debug("debug level"); logger.info("info level"); logger.warn("warn level"); logger.error("error level"); logger.fatal("fatal level");&#125; 这里需要注意的是应用头文件是log4j2的头文件 12import org.apache.logging.log4j.LogManager;import org.apache.logging.log4j.Logger; 而非 12import org.apache.log4j.LogManager;import org.apache.log4j.Logger; 否则会出现编译执行通过，无法打印日志的问题 123ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.20:37:11.965 [main] ERROR - error level20:37:11.965 [main] FATAL - fatal level 显示找不到配置文件，使用了默认的配置，输出了error和fatal两个级别的信息。 配置文件详解log4j2默认会在classpath目录下寻找log4j.json、log4j.jsn、log4j2.xml等名称的文件，如果都没有找到，则会按默认配置输出，也就是输出到控制台。下面我们按默认配置添加一个log4j2.xml，添加到src根目录即可 log4j 2.x版本不再支持像1.x中的.properties后缀的文件配置方式，2.x版本配置文件后缀名只能为”.xml”,”.json”或者”.jsn”。系统选择配置文件的优先级如下： classpath下的名为log4j2-test.json 或者log4j2-test.jsn的文件 classpath下的名为log4j2-test.xml的文件 classpath下名为log4j2.json 或者log4j2.jsn的文件 classpath下名为log4j2.xml的文件 我们一般默认使用log4j2.xml进行命名。如果本地要测试，可以把log4j2-test.xml放到classpath，而正式环境使用log4j2.xml，则在打包部署的时候不要打包log4j2-test.xml即可。 12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration status="OFF"， monitorinterval="30"&gt; &lt;appenders&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n"/&gt; &lt;/Console&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;root level="trace"&gt; &lt;appender-ref ref="Console"/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; 根节点Configuration有两个属性：status和monitorinterval status用来指定log4j本身的打印日志的级别 monitorinterval用于指定log4j自动重新配置的监测间隔时间，单位是s，最小是5s 有两个子节点:Appenders和Loggers AppendersAppenders负责定义日志输出的目的地，它可以是控制台（ConsoleAppender）、文件（FileAppender）、以Email的形式发送出去（SMTPAppender）等，Log4j 2官网介绍了20种Appender：http://logging.apache.org/log4j/2.x/manual/appenders.html。 下面介绍3种常用的Appender：ConsoleAppender、FileAppender、RollingFileAppender 1.ConsoleAppender ConsoleAppender将输出写到System.err或System.out。上面测试例子中的Appender均为ConsoleAppender，输出写到了System.out。如果想将输出写到System.err，设置Console标签下的target为”SYSTEM_ERR “即可 name：指定Appender的名字 target：SYSTEM_OUT 或 SYSTEM_ERR,一般只设置默认:SYSTEM_OUT PatternLayout：输出格式，不设置默认为:%m%n 2.FileAppender FileAppender将输出写到指定文件，在File标签下设置fileName即可。fileName可以是绝对路径的文件也可以是相对路径的文件 name：指定Appender的名字 fileName：指定输出日志的目的文件带全路径的文件名 PatternLayout：输出格式，不设置默认为:%m%n 3.RollingFileAppender RollingFileAppender跟FileAppender的基本用法一样。但RollingFileAppender可以设置log文件的size（单位：KB/MB/GB）上限、数量上限，当log文件超过设置的size上限，会自动被压缩。RollingFileAppender可以理解为滚动输出日志，如果log4j2记录的日志达到上限，旧的日志将被删除，腾出的空间用于记录新的日志 name：指定Appender的名字 fileName：指定输出日志的目的文件带全路径的文件名 PatternLayout：输出格式，不设置默认为：%m%n filePattern：指定新建日志文件的名称格式 Policies：指定滚动日志的策略，就是什么时候进行新建日志文件输出日志 TimeBasedTriggeringPolicy：Policies子节点，基于时间的滚动策略，interval属性用来指定多久滚动一次，默认是1 hour。modulate=true用来调整时间：比如现在是早上3am，interval是4，那么第一次滚动是在4am，接着是8am，12am…而不是7am SizeBasedTriggeringPolicy：Policies子节点，基于指定文件大小的滚动策略，size属性用来定义每个日志文件的大小 DefaultRolloverStrategy：用来指定同一个文件夹下最多有几个日志文件时开始删除最旧的，创建新的(通过max属性)。 4.Filters Filter可以过滤log事件，并控制log输出，log4j2定义了10种 Filter：http://logging.apache.org/log4j/2.x/manual/filters.html BurstFilter可以控制某一级别的log的并发情况：1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="warn" name="MyApp" packages=""&gt; &lt;Appenders&gt; &lt;RollingFile name="RollingFile" fileName="logs/app.log" filePattern="logs/app-%d&#123;MM-dd-yyyy&#125;.log.gz"&gt; &lt;BurstFilter level="INFO" rate="16" maxBurst="100"/&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size="1 KB"/&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max="10"/&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level="trace"&gt; &lt;AppenderRef ref="RollingFile"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; level：BurstFilter过滤的事件级别 rate：每秒允许的log事件的平均值 maxBurst：当BurstFilter过滤的事件超过rate值，排队的log事件上限。超过此上限的log，将被丢弃。默认情况下maxBurst = 10*rate 按以上配置，假定每个log事件的执行时间较长，输出117个log事件（INFO级别）到RollingFileAppenders，BurstFilter会过滤得到INFO级别的log事件，之后会发生：16个log事件在执行，100个等待执行，1个被丢弃。 ThresholdFilter按日志级别区分文件输出：123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;Configuration status="WARN" monitorInterval="300"&gt; &lt;properties&gt; &lt;property name="LOG_HOME"&gt;D:/logs&lt;/property&gt; &lt;/properties&gt; &lt;Appenders&gt; ... &lt;RollingRandomAccessFile name="InfoFile" fileName="$&#123;LOG_HOME&#125;/info.log" filePattern="$&#123;LOG_HOME&#125;/$$&#123;date:yyyy-MM&#125;/info-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="warn" onMatch="DENY" onMismatch="NEUTRAL" /&gt; &lt;ThresholdFilter level="trace" onMatch="ACCEPT" onMismatch="DENY" /&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="%date&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %level [%thread][%file:%line] - %msg%n" /&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size="10 MB" /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max="20" /&gt; &lt;/RollingRandomAccessFile&gt; &lt;RollingRandomAccessFile name="ErrorFile" fileName="$&#123;LOG_HOME&#125;/error.log" filePattern="$&#123;LOG_HOME&#125;/$$&#123;date:yyyy-MM&#125;/error-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="warn" onMatch="ACCEPT" onMismatch="DENY" /&gt; &lt;ThresholdFilter level="error" onMatch="ACCEPT" onMismatch="DENY" /&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="%date&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %level [%thread][%file:%line] - %msg%n" /&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size="10 MB" /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max="20" /&gt; &lt;/RollingRandomAccessFile&gt; ... &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level="trace"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;AppenderRef ref="FatalFile" /&gt; &lt;AppenderRef ref="ErrorFile" /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; DENY，日志将立即被抛弃不再经过其他过滤器 NEUTRAL，有序列表里的下个过滤器过接着处理日志 ACCEPT，日志会被立即处理，不再经过剩余过滤器 以上两个配置将info以下的日志写入info.log，warn和error写入error.log LoggersLoggers节点，常见的有两种：Root和Logger。 1.Root Root节点用来指定项目的根日志，如果没有单独指定Logger，那么就会默认使用该Root日志输出 Level：日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF. AppenderRef：Root的子节点，用来指定该日志输出到哪个Appender。 2.Logger Logger节点用来单独指定日志的形式，比如要为指定包下的class指定不同的日志级别等。 Level：日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF. name：用来指定该Logger所适用的类或者类所在的包全路径，继承自Root节点. AppenderRef：Logger的子节点，用来指定该日志输出到哪个Appender,如果没有指定，就会默认继承自Root.如果指定了，那么会在指定的这个Appender和Root的Appender中都会输出，此时我们可以设置Logger的additivity=”false”只在自定义的Appender中进行输出。 日志级别共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF。 All：最低等级的，用于打开所有日志记录 Trace：是追踪，就是程序推进以下，你就可以写个trace输出，所以trace应该会特别多，不过没关系，我们可以设置最低日志级别不让他输出 Debug：指出细粒度信息事件对调试应用程序是非常有帮助的 Info：消息在粗粒度级别上突出强调应用程序的运行过程 Warn：输出警告及warn以下级别的日志 Error：输出错误信息日志 Fatal：输出每个严重的错误事件将会导致应用程序的退出的日志 OFF：最高等级的，用于关闭所有日志记录 程序会打印高于或等于所设置级别的日志，设置的日志等级越高，打印出来的日志就越少。 自定义读取配置文件读取配置文件方式自定义： 1234567891011121314File file = new File("./conf/log4j2.xml");BufferedInputStream in = null;try &#123; in = new BufferedInputStream(new FileInputStream(file));&#125; catch (FileNotFoundException e) &#123; e.printStackTrace();&#125;ConfigurationSource source = null;try &#123; source = new ConfigurationSource(in);&#125; catch (IOException e) &#123; e.printStackTrace();&#125;Configurator.initialize(null, source); 自定义输出格式%d{HH:mm:ss.SSS} 表示输出到毫秒的时间%t 输出当前线程名称%-5level 输出日志级别，-5表示左对齐并且固定输出5个字符，如果不足在右边补0%logger 输出logger名称，因为Root Logger没有名称，所以没有输出%msg 日志文本%n 换行 其他常用的占位符有： %F 输出所在的类文件名，如Client.java%L 输出行号%M 输出所在方法名%l 输出语句所在的行数, 包括类名、方法名、文件名、行数 自定义logger初始化logger时，可以： 1Logger logger = LogManager.getLogger("mylog"); 或者 1Logger logger = LogManager.getLogger(Logs.class.getName()); Logs为应用程序中的日志类名。 这样在配置文件中，即可以使用自定义的logger，控制自定义log的输出，与root区分开。 12345678&lt;Loggers&gt; &lt;Logger name="mylog" level="trace" additivity="false"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;/Logger&gt; &lt;Root level="error"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;/Root&gt;&lt;/Loggers&gt; 自定义Appender123456789101112131415161718&lt;Configuration status="WARN" monitorInterval="300"&gt; &lt;Appenders&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n" /&gt; &lt;/Console&gt; &lt;File name="MyFile" fileName="./logs/app.log"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n" /&gt; &lt;/File&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name="mylog" level="trace" additivity="true"&gt; &lt;AppenderRef ref="MyFile" /&gt; &lt;/Logger&gt; &lt;Root level="error"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 按大小和时间生成日志文件123456789101112131415161718192021222324252627282930&lt;Configuration status="WARN" monitorInterval="300"&gt; &lt;properties&gt; &lt;property name="LOG_HOME"&gt;D:/logs&lt;/property&gt; &lt;property name="FILE_NAME"&gt;mylog&lt;/property&gt; &lt;/properties&gt; &lt;Appenders&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n" /&gt; &lt;/Console&gt; &lt;RollingRandomAccessFile name="myfile" fileName="$&#123;LOG_HOME&#125;/$&#123;FILE_NAME&#125;.log" filePattern="$&#123;LOG_HOME&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;FILE_NAME&#125;-%d&#123;yyyy-MM-dd HH-mm&#125;-%i.log"&gt; &lt;PatternLayout pattern="%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n" /&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval="1" /&gt; &lt;SizeBasedTriggeringPolicy size="10 MB" /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max="20" /&gt; &lt;/RollingRandomAccessFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name="mylog" level="trace" additivity="false"&gt; &lt;AppenderRef ref="myfile" /&gt; &lt;/Logger&gt; &lt;Root level="error"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; TimeBasedTriggeringPolicy这个配置需要和filePattern结合使用，注意filePattern中配置的文件重命名规则是${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i，最小的时间粒度是mm，即分钟，TimeBasedTriggeringPolicy指定的size是1，结合起来就是每1分钟生成一个新文件。如果改成%d{yyyy-MM-dd HH}，最小粒度为小时，则每一个小时生成一个文件。 异步写文件12345678910111213...&lt;Appenders&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; ... &lt;/Console&gt; &lt;RollingRandomAccessFile name="MyFile" ... &lt;/RollingRandomAccessFile&gt; &lt;Async name="Async"&gt; &lt;AppenderRef ref="MyFile" /&gt; &lt;/Async&gt;&lt;/Appenders&gt;... 日志输出到flume配置文件：12345678910111213&lt;Configuration status="WARN" monitorInterval="300"&gt; &lt;Appenders&gt; &lt;Flume name="eventLogger" compress="false"&gt; &lt;Agent host="127.0.0.1" port="41414" /&gt; &lt;RFC5424Layout enterpriseNumber="18060" includeMDC="true" appName="MyApp" /&gt; &lt;/Flume&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level="trace"&gt; &lt;AppenderRef ref="eventLogger" /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; flume上配置source类型为avro即可 123456...agent1.sources.source.type=avroagent1.sources.source.channels=channelagent1.sources.source.bind=0.0.0.0agent1.sources.source.port=41414... 参考：详解log4j2(下) - Async/MongoDB/Flume Appender 按日志级别区分文件输出]]></content>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark工作机制]]></title>
    <url>%2F2017%2F07%2F05%2Fspark-mechanism%2F</url>
    <content type="text"><![CDATA[参考：Spark-core-架构及工作机制综述]]></content>
      <tags>
        <tag>SPARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名邮箱服务]]></title>
    <url>%2F2017%2F07%2F05%2Fdomain-mail%2F</url>
    <content type="text"><![CDATA[MX（Mail Exchanger）记录MX是邮件交换记录，它指向一个邮件服务器，用于电子邮件系统发邮件时根据 收信人的地址后缀来定位邮件服务器。例如，当Internet上的某用户要发一封信给 user@mydomain.com 时，该用户的邮件系统通过DNS查找mydomain.com这个域名的MX记录，如果MX记录存在， 用户计算机就将邮件发送到MX记录所指定的邮件服务器上。 MX记录也叫做邮件路由记录，用户可以将该域名下的邮件服务器指向到自己的mail server上，然后即可自行操控所有的邮箱设置 。 MX优先级如果您的域有多个 MX 记录，邮件发件人将决定使用哪一个记录。MX 记录使用一个称为“首选项”的字段以确定优先级。当您创建 MX 记录时，大多数 DNS 托管提供商要求您设置首选数字。有些提供商将输入框标记为“首选项”，另一些标记为“优先级”。有些提供商要求输入数字，另一些提供商要求您选择“低”、“中”或“高”。 一级域名邮箱设置以下是在DNSPOD上，添加主域名（mail.mydomain.com）的邮箱记录，将其指定到自定义的邮件服务器上（对应A记录的ip地址） 二级域名邮箱设置已有一级邮箱域名服务（mail.mydomain.com），现添加二级域名（mail.mails.mydomian.com）的邮件服务，将其cname到阿里云的邮件服务器上。]]></content>
      <tags>
        <tag>DOMAIN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark中redis连接池的几种使用方法]]></title>
    <url>%2F2017%2F07%2F04%2Fspark-redis%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;spark是一个大数据的分布式的计算框架，他的工作机制核心也是分布式，所以很多地方的使用一定要先理解它的工作机制，很多问题和性能优化都和spark的工作机制紧密相关。&emsp;&emsp;这里主要介绍spark中redis的几种使用方法，通过使用方法理解spark的工作机制。 方法一：Master上集中处理&emsp;&emsp;Spark采用了分布式计算中的Master-Slave模型。Master作为整个集群的控制器，负责整个集群的正常运行；Worker是计算节点，接受主节点命令以及进行状态汇报。这种方式便是将Worker上的所有数据收集到Master上，然后遍历写入redis。 TestRedisPool代码： 1234567891011121314151617181920212223242526272829303132333435public class TestRedisPool &#123; private JedisPool pool = null; public TestRedisPool(String ip, int port, String passwd, int database) &#123; if (pool == null) &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(500); config.setMaxIdle(30); config.setMinIdle(5); config.setMaxWaitMillis(1000 * 10); config.setTestWhileIdle(false); config.setTestOnBorrow(false); config.setTestOnReturn(false); pool = new JedisPool(config, ip, port, 10000, passwd, database); Logs.debug("init:" + pool); &#125; &#125; public JedisPool getRedisPool() &#123; return pool; &#125; public String set(String key,String value)&#123; Jedis jedis = null; try &#123; jedis = pool.getResource(); return jedis.set(key, value); &#125; catch (Exception e) &#123; e.printStackTrace(); return "0"; &#125; finally &#123; jedis.close(); &#125; &#125;&#125; 示例代码如下，rdd对应在三个分区上： 123456789List&lt;String&gt; list = Arrays.asList("a","b","c","d", "e");JavaRDD&lt;String&gt; javaRDD = new JavaSparkContext(spark.sparkContext()).parallelize(list, 3);TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum);List&lt;String&gt; lst = javaRDD.collect();for(String s:lst) &#123; testRedisPool.set(s, getDateString2(0));&#125; &emsp;&emsp;所有数据collect到Master上，只需要建立一个redis连接池处理。虽然减少了redis连接数，但是所有数据需要collect到Master上，大数据量的处理对Master压力较大，性能较差。 方法二：Worker上遍历所有数据&emsp;&emsp;spark中所有的数据处理分为Tranformation和Action，foreach就是一个Action，通过foreach按分区遍历所有的数据进行处理，这种情况下输出只能在分区上看到，Master上无法看到。 12345678javaRDD.foreach(new VoidFunction&lt;String&gt;() &#123; @Override public void call(String s) throws Exception &#123; TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum); Logs.debug(testRedisPool.getRedisPool()); testRedisPool.set(s, getDateString2(0)); &#125;&#125;); Worker上的输出： 123452017-07-04 12:59:18 DEBUG: redis.clients.jedis.JedisPool@6bc8c6df2017-07-04 12:59:18 DEBUG: redis.clients.jedis.JedisPool@46c2ca892017-07-04 12:59:18 DEBUG: redis.clients.jedis.JedisPool@ac221bf2017-07-04 12:59:18 DEBUG: redis.clients.jedis.JedisPool@1bccc5482017-07-04 12:59:18 DEBUG: redis.clients.jedis.JedisPool@25c1ef20 &emsp;&emsp;按分区历所有元素，TestRedisPool不需要实现序列化；每一个RDD中的元素都需要创建很多的redis连接池，即便使用短连接也会对redis造成很大的压力。效率也是极其低下的。 方法三：Master上创建，Worker上遍历所有数据&emsp;&emsp;如果在Master上创建一个实例，在进行分区遍历时使用Master上创建的实例，这种方式是可以的，只需要将类实现序列即可。同时还可以通过广播变量，将实例在Worker上持久化，减少实例使用时的网络传输。 12345678TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum);javaRDD.foreach(new VoidFunction&lt;String&gt;() &#123; @Override public void call(String s) throws Exception &#123; Logs.debug(testRedisPool.getRedisPool()); testRedisPool.set(s, getDateString2(0)); &#125;&#125;); 输出 1234Exception in thread "main" org.apache.spark.SparkException: Task not serializable...Serialization stack: - object not serializable (class: redis.clients.jedis.JedisPool, value: redis.clients.jedis.JedisPool@3e4f80cb) &emsp;&emsp;报错jedispool无法序列化，即使TestRedisPool类实现了序列化，但因为其成员变量jedispool本身并不支持序列化，所以这种方式在有成员变量无法序列化时也不可用。 方法四：Worker上按分区遍历&emsp;&emsp;foreachPartitions也是一个Action操作，foreach和foreachPartitions的实现如下： 1234def foreach(f: T =&gt; Unit) &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))&#125; 1234def foreachPartition(f: Iterator[T] =&gt; Unit) &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))&#125; &emsp;&emsp;foreach是使用提供的函数调用分区迭代器的foreach处理，foreachPartition是直接传入的是分区的指针。所以，其实foreach和foreachPartition并没有什么区别，foreachPartition只是让你有机会在迭代器的循环之外做一些事情，通常是像数据库连接或者redis等。所以，如果你没有任何可以对每个分区的迭代器做一次，并在整个过程中重复使用的操作，建议还是使用foreach来提高清晰度和降低复杂度。 12345678910javaRDD.foreachPartition(new VoidFunction&lt;Iterator&lt;String&gt;&gt;() &#123; @Override public void call(Iterator&lt;String&gt; stringIterator) throws Exception &#123; TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum); while (stringIterator.hasNext()) &#123; Logs.debug(testRedisPool.getRedisPool()); testRedisPool.set(stringIterator.next(), getDateString2(0)); &#125; &#125;&#125;); Worker上的输出 123452017-07-04 13:30:29 DEBUG: redis.clients.jedis.JedisPool@215457552017-07-04 13:30:29 DEBUG: redis.clients.jedis.JedisPool@111b1ab42017-07-04 13:30:29 DEBUG: redis.clients.jedis.JedisPool@2b9b3bd52017-07-04 13:30:29 DEBUG: redis.clients.jedis.JedisPool@215457552017-07-04 13:30:29 DEBUG: redis.clients.jedis.JedisPool@111b1ab4 &emsp;&emsp;TestRedisPool不需要实现序列化，每个分区只需要创建一个redis连接池，正常情况下会创建和线程数一样多的连成池，这种情况下，redis连接池数量明显减少。 &emsp;&emsp;TestRedisPool在Master上定义时，和方法三种一样，同样因为jedispool无法序列化报错。 方法五：使用静态类型，按分区遍历&emsp;&emsp;在方法四中，我们可以做到在每个分区上建立连接池，但是每台机器一般对应多个分区，怎么进一步减少连接池的创建呢。我们知道静态类型全局只有一份，如果将redis连接池定义为静态类型，做到每个worker上只创建一个redis连接池。 1234public class TestRedisPool &#123; private static JedisPool pool = null; ...&#125; 使用示例： 12345678910TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum);javaRDD.foreachPartition(new VoidFunction&lt;Iterator&lt;String&gt;&gt;() &#123; @Override public void call(Iterator&lt;String&gt; stringIterator) throws Exception &#123; while (stringIterator.hasNext()) &#123; Logs.debug(testRedisPool.getRedisPool()); testRedisPool.set(stringIterator.next(), getDateString2(0)); &#125; &#125;&#125;); 这种在Master上创建TestRedisPool实例的方式，在worker上无法获取到，会报java.lang.NullPointerException异常。 12345678910javaRDD.foreachPartition(new VoidFunction&lt;Iterator&lt;String&gt;&gt;() &#123; @Override public void call(Iterator&lt;String&gt; stringIterator) throws Exception &#123; TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum); while (stringIterator.hasNext()) &#123; Logs.debug(testRedisPool.getRedisPool()); testRedisPool.set(stringIterator.next(), getDateString2(0)); &#125; &#125;&#125;); Worker输出 123456782017-07-04 16:38:04 DEBUG: init:redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: init:redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: init:redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: redis.clients.jedis.JedisPool@416605b22017-07-04 16:38:04 DEBUG: redis.clients.jedis.JedisPool@416605b2 &emsp;&emsp;TestRedisPool也不需要序列化。因为本实验环境中只有一个worker节点，所以这里看到始终只有一个redis连接池实例。这种情况下是在分区上分别创建实例，分区对应的就是虚拟线程的个数，所以相当于3个线程同时去获取jedispool实现，所以一共init了三次。如果做成单例模式就能解决init多次的问题。 方法六：使用单例模式，按分区遍历jedispool使用单例模式实现： 1234567891011121314151617181920212223242526272829303132333435public class TestRedisPool &#123; private static JedisPool pool = null; String ip; Integer port; String passwd; Integer database; public TestRedisPool(String ip, int port, String passwd, int database) &#123; this.ip = ip; this.port = port; this.passwd = passwd; this.database = database; &#125; public JedisPool getRedisPool() &#123; if (pool == null) &#123; synchronized (RedisUtils.class) &#123; if (pool == null) &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(500); config.setMaxIdle(30); config.setMinIdle(5); config.setMaxWaitMillis(1000 * 10); config.setTestWhileIdle(false); config.setTestOnBorrow(false); config.setTestOnReturn(false); pool = new JedisPool(config, ip, port, 10000, passwd, database); Logs.debug("init:" + pool); &#125; &#125; &#125; return pool; &#125; ...&#125; &emsp;&emsp;以上volatile保证当jedispool未初始化完成是不能被获取到，synchronized解决多线程冲突的问题。这两个关键词的使用其实也就是lazy initialize的实现。 1234567891011javaRDD.foreachPartition(new VoidFunction&lt;Iterator&lt;String&gt;&gt;() &#123; @Override public void call(Iterator&lt;String&gt; stringIterator) throws Exception &#123; TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum); while (stringIterator.hasNext()) &#123; Logs.debug("class:" + testRedisPool ); Logs.debug("pool:" + testRedisPool .getRedisPool()); testRedisPool .set(stringIterator.next(), getDateString2(0)); &#125; &#125;&#125;); 节点上输出如下： 12345678910112017-07-04 17:13:48 DEBUG: class:test.TestRedisPool@4ac996de2017-07-04 17:13:48 DEBUG: class:test.TestRedisPool@7f6973f92017-07-04 17:13:48 DEBUG: class:test.TestRedisPool@1e24e8f62017-07-04 17:13:48 DEBUG: init:redis.clients.jedis.JedisPool@68caaac2017-07-04 17:13:48 DEBUG: pool:redis.clients.jedis.JedisPool@68caaac2017-07-04 17:13:48 DEBUG: pool:redis.clients.jedis.JedisPool@68caaac2017-07-04 17:13:48 DEBUG: pool:redis.clients.jedis.JedisPool@68caaac2017-07-04 17:13:48 DEBUG: class:test.TestRedisPool@7f6973f92017-07-04 17:13:48 DEBUG: pool:redis.clients.jedis.JedisPool@68caaac2017-07-04 17:13:48 DEBUG: class:test.TestRedisPool@1e24e8f62017-07-04 17:13:48 DEBUG: pool:redis.clients.jedis.JedisPool@68caaac &emsp;&emsp;可以看到现在jedispool只init了一次，并且全局也只有一个jedispool。但是现在TestRedisPool对象还是被创建了多个，改为在Master上定义，并已广播变量的形式分发到Worker上可以解决这个问题，这种情况下TestRedisPool需要序列化。 方法七：使用单例模式，Driver上定义，分区上遍历 12345678910111213TestRedisPool testRedisPool = new TestRedisPool(redisIp, port, passwd, dbNum);final Broadcast&lt;TestRedisPool&gt; broadcastRedis = new JavaSparkContext(spark.sparkContext()).broadcast(testRedisPool);javaRDD.foreachPartition(new VoidFunction&lt;Iterator&lt;String&gt;&gt;() &#123; @Override public void call(Iterator&lt;String&gt; stringIterator) throws Exception &#123; TestRedisPool redisClient1 = broadcastRedis.getValue(); while (stringIterator.hasNext()) &#123; Logs.debug("class:" + redisClient1); Logs.debug("pool:" + redisClient1.getRedisPool()); redisClient1.set(stringIterator.next(), getDateString2(0)); &#125; &#125;&#125;); 输出如下，类实例和redispool都只创建一次，也使用同一个。 12345678910112017-07-04 17:17:32 DEBUG: class:test.TestRedisPool@620440182017-07-04 17:17:32 DEBUG: class:test.TestRedisPool@620440182017-07-04 17:17:32 DEBUG: class:test.TestRedisPool@620440182017-07-04 17:17:32 DEBUG: init:redis.clients.jedis.JedisPool@3a820c052017-07-04 17:17:32 DEBUG: pool:redis.clients.jedis.JedisPool@3a820c052017-07-04 17:17:32 DEBUG: pool:redis.clients.jedis.JedisPool@3a820c052017-07-04 17:17:32 DEBUG: pool:redis.clients.jedis.JedisPool@3a820c052017-07-04 17:17:32 DEBUG: class:test.TestRedisPool@620440182017-07-04 17:17:32 DEBUG: pool:redis.clients.jedis.JedisPool@3a820c052017-07-04 17:17:32 DEBUG: class:test.TestRedisPool@620440182017-07-04 17:17:32 DEBUG: pool:redis.clients.jedis.JedisPool@3a820c05 &emsp;&emsp;现在是TestRedisPool在Master上定义，广播到各个Worker上；同时jedispool在每台worker上也始终只会有一个实例存在。但是也会有人会疑问，为什么jedispool现在没有序列化的问题（方法三），或者定义成静态导致worker上获取不到jedispool（方法五第一种情况）的问题。这是因为，方法三中jedispool为普通类型是，和类一起序列化，因为其本身不支持序列化，所以报错；方法五中，定义成静态类型之后，静态类型不属于类，所以TestRedisPool序列化不会出错，但是因为jedispool在Master上定义和初始化，不会传输到节点上，节点上获取到的jedispool都为null，所以报错。而方法七中使用懒启动的方式，在使用的是才会初始化jedispool，所以实际是在节点上完成的初始化，所以不会有问题。 参考：Java中单例模式实现]]></content>
      <tags>
        <tag>REDIS</tag>
        <tag>SPARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark序列化]]></title>
    <url>%2F2017%2F07%2F03%2Fspark-serialization%2F</url>
    <content type="text"><![CDATA[什么是序列化&emsp;&emsp;对象的序列化（Serialization）用于将对象编码成一个字节流，以及从字节流中重新构建对象。将一个对象编码成一个字节流称为序列化该对象（Serializing）；相反的处理过程称为反序列化（Deserializing）。 序列化有三种主要的用途： 作为一种持久化格式：一个对象被序列化以后，它的编码可以被存储到磁盘上，供以后反序列化用。 作为一种通信数据格式：序列化结果可以从一个正在运行的虚拟机，通过网络被传递到另一个虚拟机上。 作为一种拷贝、克隆（clone）机制：将对象序列化到内存的缓存区中，然后通过反序列化，可以得到一个对已存对象进行深拷贝的新对象。在分布式数据处理中，主要使用上面提到的前两种功能：数据持久化和通信数据格式。 为什么要序列化&emsp;&emsp;在写Spark的应用时，尝尝会碰到序列化的问题。例如，在Driver端的程序中创建了一个对象，而在各个Executor中会用到这个对象 ，由于Driver端代码与Executor端的代码运行在不同的JVM中，甚至在不同的节点上，因此必然要有相应的序列化机制来支撑数据实例在不同的JVM或者节点之间的传输。 什么情况下需要序列化以下一段spark代码： 1234567891011public class TransKey implements Serializable &#123; private String prefix; public TransKey() &#123; prefix = "news_"; &#125; public String addPrefix(String s) &#123; return prefix + s; &#125;&#125; 12345List&lt;String&gt; list = Arrays.asList("a","b","c");JavaRDD&lt;String&gt; javaRDD = new JavaSparkContext(spark.sparkContext()).parallelize(list);TransKey transKey = new TransKey();JavaRDD&lt;String&gt; javaRDD1 = javaRDD.map(s -&gt; transKey.addPrefix(s)); &emsp;&emsp;以上代码执行是会报错org.apache.spark.SparkException: Task not serializable，因为transkey在执行过程中需要从Driver传输到Executor。为Executor没有引用到Driver的实例。因此TransKey类需要实现序列化。 实现序列化Spark可以使用Java的序列化框架。只要一个class实现了java.io.Serializable接口，那么Spark就能使用Java的ObjectOutputStream来序列化该类。 123public class TransKey implements Serializable &#123; ...&#125; 对于用户自定义类，通过以上方法实现序列化即可正常使用。Spark还支持另一种序列化框架Kryo。Kryo是一个高效的序列化框架（可以比Java的序列化快10倍以上）。 1234567891011121314public class RedisUtilKryo implements KryoSerializable &#123; private object pool = null; private Integer port; @Override public void write(Kryo kryo, Output output) &#123; kryo.writeClassAndObject(output, pool); &#125; @Override public void read(Kryo kryo, Input input) &#123; pool = kryo.readClassAndObject(input); &#125;&#125; 参考：Java 序列化的高级认识spark-stream 访问 Redis]]></content>
      <tags>
        <tag>SPARK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加disqus评论]]></title>
    <url>%2F2017%2F06%2F19%2Fhexo-disqus%2F</url>
    <content type="text"><![CDATA[注册disqusdisqus官网，按要求注册一个账户，添加你的博客地址 获取shortname在Settings-&gt;General下可以看到你的Shortname。 添加到blog打开根目录下的config.yml, 在最后面添加，disqus_shortname: your_disqus_short_name，去掉其他评论配置。 刷新页面即可看到评论系统。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis安装配置]]></title>
    <url>%2F2017%2F06%2F16%2Fredis-install%2F</url>
    <content type="text"><![CDATA[编译1234tar -zxvf redis-2.8.12.tar.gzcd redis-2.8.19 makemake PREFIX=/opt/modules/redis 配置1234mkdir /opt/modules/redis/etc/cp redis.conf /opt/modules/redis/etc/ cd /opt/modules/redis/bin/cp redis-benchmark redis-cli redis-server /usr/bin/ 内存调整 123456#此参数可用的值为0,1,2 #0表示当用户空间请求更多的内存时，内核尝试估算出可用的内存 #1表示内核允许超量使用内存直到内存用完为止 #2表示整个内存地址空间不能超过swap+(vm.overcommit_ratio)%的RAM值 echo "vm.overcommit_memory=1"&gt;&gt;/etc/sysctl.confsysctl -p 修改redis配置 1234567891011121314151617181920212223242526vim /opt/modules/redis/etc/redis.conf#配置监听地址bind 127.0.0.1 192.168.193.228 119.18.193.228#配置监听端口port 6379# 配置log文件位置logfile "/opt/redis/log.txt"# 写文件条件save 900 1save 300 10save 60 10000# 写文件名dbfilename dump.rdb#密码设置requirepass bfsportsdt##修改命名及禁止rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c28rename-command FLUSHALL ""rename-command FLUSHDB "" 启动脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/bin/bash#chkconfig: 2345 80 90# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.PATH=/usr/local/bin:/sbin:/usr/bin:/binREDISPORT=6379EXEC=/opt/modules/redis/bin/redis-serverREDIS_CLI=/opt/modules/redis/bin/redis-cli PIDFILE=/var/run/redis.pidCONF="/opt/modules/redis/etc/redis.conf" case "$1" in start) if [ -f $PIDFILE ] then echo "$PIDFILE exists, process is already running or crashed" else echo "Starting Redis server..." $EXEC $CONF fi if [ "$?"="0" ] then echo "Redis is running..." fi ;; stop) if [ ! -f $PIDFILE ] then echo "$PIDFILE does not exist, process is not running" else PID=$(cat $PIDFILE) echo "Stopping ..." $REDIS_CLI -p $REDISPORT SHUTDOWN while [ -x $&#123;PIDFILE&#125; ] do echo "Waiting for Redis to shutdown ..." sleep 1 done echo "Redis stopped" fi ;; restart|force-reload) $&#123;0&#125; stop $&#123;0&#125; start ;; *) echo "Usage: /etc/init.d/redis &#123;start|stop|restart|force-reload&#125;" &gt;&amp;2 exit 1esac redis开机自启动123456789101112131415161718# 复制脚本文件到init.d目录下cp redis /etc/init.d/# 给脚本增加运行权限chmod +x /etc/init.d/redis# 查看服务列表chkconfig --list# 添加服务chkconfig --add redis# 配置启动级别chkconfig --level 2345 redis on#启动停止service redis start #或者 /etc/init.d/redis start service redis stop #或者 /etc/init.d/redis stop 参考：NoSQL之【Redis】学习（三）：Redis持久化 Snapshot和AOF说明]]></content>
      <tags>
        <tag>REDIS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx编译安装]]></title>
    <url>%2F2017%2F06%2F16%2Fnginx-install%2F</url>
    <content type="text"><![CDATA[编译 1234567891011121314151617181920tar xzvf nginx-1.10.1.tar.gzcd nginx-1.10.1./configure \--prefix=/opt/modules/nginx \--sbin-path=/opt/modules/nginx/sbin/nginx \--conf-path=/opt/modules/nginx/conf/nginx.conf \--error-log-path=/var/log/nginx/error.log \--http-log-path=/var/log/nginx/access.log \--pid-path=/var/run/nginx/nginx.pid \--lock-path=/var/lock/nginx.lock \--user=nginx --group=nginx \--with-http_ssl_module \--with-http_stub_status_module \--with-http_gzip_static_module \--http-client-body-temp-path=/opt/modules/nginx/client_body_temp/ \--http-proxy-temp-path=/opt/modules/nginx/nginx_proxy/ \--http-fastcgi-temp-path=/opt/modules/nginx/fastcgi_temp/ \--add-module=../redis2-nginx-module-0.13 \--add-module=../nginx-http-concat-1.2.2make 安装123456make installcp objs/nginx /usr/local/bin/groupadd nginxuseradd -g nginx nginxcp init.d/nginx /etc/init.d/chmod 755 /etc/init.d/nginx 添加开机启动123/sbin/chkconfig --add nginx/sbin/chkconfig --level 35 nginx on/sbin/service nginx start]]></content>
      <tags>
        <tag>NGINX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO多路复用]]></title>
    <url>%2F2017%2F06%2F09%2Fio-multi-reuse%2F</url>
    <content type="text"></content>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume安装使用]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-flume%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka安装使用]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-kafka%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeepr安装使用]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-zookeepr%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark on yarn提交慢解决方法]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-spark-yarn%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven下载慢的解决方法]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-mvn-slow%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark程序编译运行]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-spark-mvn-sbt%2F</url>
    <content type="text"><![CDATA[sbt构建scala应用下载安装sbt是一款Spark用来对scala编写程序进行打包的工具，下载sbt-launch.jar的下载地址。安装在 /usr/local/sbt 中： 123sudo mkdir /usr/local/sbtsudo chown -R hadoop /usr/local/sbt # 此处的 hadoop 为你的用户名cd /usr/local/sbt 下载后，执行如下命令拷贝至 /usr/local/sbt 中： 1cp /opt/hdp/sbt-launch.jar ./ 接着在 /usr/local/sbt 中创建 sbt 脚本（vim ./sbt），添加如下内容： 123#!/bin/bashSBT_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar "$@" 保存后，为 ./sbt 脚本增加可执行权限: 1chmod u+x ./sbt 最后运行如下命令，检验 sbt 是否可用，首次运行会处于 “Getting org.scala-sbt sbt 0.13.11 …” 的下载状态，请耐心等待 1./sbt sbt-version 只要能得到如下图的版本信息就没问题： 12345:: retrieving :: org.scala-sbt#boot-scala confs: [default] 5 artifacts copied, 0 already retrieved (24494kB/22ms)[info] Set current project to sbt (in build file:/usr/local/sbt/)[info] 0.13.11 编写scala应用程序在终端中执行如下命令创建一个文件夹 sparkapp 作为应用程序根目录： 123cd ~ # 进入用户主文件夹mkdir ./sparkapp # 创建应用程序根目录mkdir -p ./sparkapp/src/main/scala # 创建所需的文件夹结构 在 ./sparkapp/src/main/scala 下建立一个名为 SimpleApp.scala 的文件（vim ./sparkapp/src/main/scala/SimpleApp.scala），添加代码如下（目前不需要理解代码的具体含义，只需要理解如何编译运行代码就可以）： 12345678910111213141516/* SimpleApp.scala */import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.SparkConfobject SimpleApp &#123; def main(args: Array[String]) &#123; val logFile = "file:///usr/local/spark/README.md" // Should be some file on your system val conf = new SparkConf().setAppName("Simple Application") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line =&gt; line.contains("a")).count() val numBs = logData.filter(line =&gt; line.contains("b")).count() println("Lines with a: %s, Lines with b: %s".format(numAs, numBs)) &#125;&#125; 使用 sbt 打包 Scala 程序该程序依赖 Spark API，因此我们需要通过 sbt 进行编译打包。 ./sparkapp 中新建文件 simple.sbt（vim ./sparkapp/simple.sbt），添加内容如下，声明该独立应用程序的信息以及与 Spark 的依赖关系： 1234name := "Simple Project"version := "1.0"scalaVersion := "2.11.8"libraryDependencies += "org.apache.spark" %% "spark-core" % "2.1.0" 为保证 sbt 能正常运行，先执行如下命令检查整个应用程序的文件结构： 1234567[root@localhost sparkapp]# find .../src./src/main./src/main/scala./src/main/scala/SimpleApp.scala./simple.sbt 接着，我们就可以通过如下代码将整个应用程序打包成 JAR（首次运行同样需要下载依赖包 ）： 1/usr/local/sbt/sbt package 对于刚安装好的Spark和sbt而言，第一次运行上面的打包命令时，会需要几分钟的运行时间，因为系统会自动从网络上下载各种文件。后面再次运行上面命令，就会很快，因为不再需要下载相关文件。打包成功的话，会输出如下图内容： 123[info] Packaging /opt/hdp/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar ...[info] Done packaging.[success] Total time: 697 s, completed Feb 28, 2017 6:32:05 PM 生成的 jar 包的位置为 ~/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar 通过spark-submit 运行程序1/opt/hdp/spark-2.1.0/bin/spark-submit --class "SimpleApp" /opt/hdp/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar | grep 'Lines' maven构建java应用程序下载安装mavenapache-maven-3.3.9-bin.zip的下载地址选择安装在/usr/local/maven中： 1234tar -xzvf apache-maven-3.3.9-bin.tar.gz cd /usr/localsudo mv apache-maven-3.3.9/ ./mavensudo chown -R hadoop ./maven 编写Java应用程序代码在终端执行如下命令创建一个文件夹sparkapp2作为应用程序根目录在 ./sparkapp2/src/main/java 下建立一个名为 SimpleApp.java 的文件（vim ./sparkapp2/src/main/java/SimpleApp.java），添加代码如下： 12345678910111213141516171819202122/*** SimpleApp.java ***/import org.apache.spark.api.java.*;import org.apache.spark.api.java.function.Function; public class SimpleApp &#123; public static void main(String[] args) &#123; String logFile = "file:///usr/local/spark/README.md"; // Should be some file on your system JavaSparkContext sc = new JavaSparkContext("local", "Simple App", "file:///usr/local/spark/", new String[]&#123;"target/simple-project-1.0.jar"&#125;); JavaRDD&lt;String&gt; logData = sc.textFile(logFile).cache(); long numAs = logData.filter(new Function&lt;String, Boolean&gt;() &#123; public Boolean call(String s) &#123; return s.contains("a"); &#125; &#125;).count(); long numBs = logData.filter(new Function&lt;String, Boolean&gt;() &#123; public Boolean call(String s) &#123; return s.contains("b"); &#125; &#125;).count(); System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs); &#125;&#125; maven打包应用该程序依赖Spark Java API,因此我们需要通过Maven进行编译打包。在./sparkapp2中新建文件pom.xml(vim ./sparkapp2/pom.xml),添加内容如下，声明该独立应用程序的信息以及与Spark的依赖关系： 123456789101112131415161718192021&lt;project&gt; &lt;groupId&gt;edu.berkeley&lt;/groupId&gt; &lt;artifactId&gt;simple-project&lt;/artifactId&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;name&gt;Simple Project&lt;/name&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;Akka repository&lt;/id&gt; &lt;url&gt;http://repo.akka.io/releases&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- Spark dependency --&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 接着，我们可以通过如下代码将这整个应用程序打包成Jar首次运行mvn package命令时，系统会自动从网络下载相关的依赖包，同样消耗几分钟的时间，后面再次运行mvn package命令，速度就会快很多): 1/usr/local/maven/bin/mvn package 如出现下图，说明生成Jar包成功： 123456789[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ simple-project ---[INFO] Building jar: /opt/hdp/sparkapp2/target/simple-project-1.0.jar[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.469 s[INFO] Finished at: 2017-03-03T12:46:57+08:00[INFO] Final Memory: 32M/268M[INFO] ------------------------------------------------------------------------ 通过spark-submit 运行程序最后，可以通过将生成的jar包通过spark-submit提交到Spark中运行，如下命令： 1/opt/hdp/spark-2.1.0/bin/spark-submit --class "SimpleApp" ~/sparkapp2/target/simple-project-1.0.jar 参考Spark安装和使用]]></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rdd基本操作]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-spark-rdd%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark几种运行方式]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-spark-run%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop使用zk配置HA]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-zk-ha%2F</url>
    <content type="text"><![CDATA[简介&emsp;&emsp;在hadoop1时代，只有一个NameNode。如果该NameNode数据丢失或者不能工作，那么整个集群就不能恢复了。hadoop2.2.0中HDFS的高可靠指的是可以同时启动2个NameNode。其中一个处于工作状态(active)，另一个处于随时待命状态(standby)。这样，当一个NameNode所在的服务器宕机时，可以在数据不丢失的情况下，手工或者自动切换到另一个NameNode提供服务。 &emsp;&emsp;hadoop2.0的HA机制官方介绍了有2种方式，一种是NFS（Network File System）方式，另外一种是QJM（Quorum Journal Manager）方式。active namenode和standby namenode之间通过NFS或者JN（journalnode，QJM方式）来同步数据。 &emsp;&emsp;active namenode会把最近的操作记录写到本地的一个edits文件中（edits file），并传输到NFS或者JN中。standby namenode定期的检查，从NFS或者JN把最近的edit文件读过来，然后把edits文件和fsimage/tech文件合并成一个新的fsimage/tech，合并完成之后会通知active namenode获取这个新fsimage/tech。active namenode获得这个新的fsimage/tech文件之后，替换原来旧的fsimage/tech文件。所以启动了hadoop2.0的HA机制之后，hadoop1.0中的secondarynamenode，checkpointnode，buckcupnode这些都不需要了。 NFS方式&emsp;&emsp;NFS作为active namenode和standby namenode之间数据共享的存储。active namenode会把最近的edits文件写到NFS，而standby namenode从NFS中把数据读过来。这个方式的缺点是，如果active namenode或者standby namenode有一个和NFS之间网络有问题，则会造成他们之前数据的同步出问题。 QJM（Quorum Journal Manager ）方式&emsp;&emsp;QJM的方式可以解决上述NFS容错机制不足的问题。active namenode和standby namenode之间是通过一组journalnode（数量是奇数，可以是3,5,7…,2n+1）来共享数据。active namenode把最近的edits文件写到2n+1个journalnode上，只要有n+1个写入成功就认为这次写入操作成功了，然后standby namenode就可以从journalnode上读取了。可以看到，QJM方式有容错的机制，可以容忍n个journalnode的失败。本文主要基于这种方式搭建高可用的hadoop集群。 架构hadoop2.x高可用架构图 &emsp;&emsp;在一个典型的HA集群中，每个NameNode是一台独立的服务器。在任一时刻，只有一个NameNode处于active状态，另一个处于standby状态。其中，active状态的NameNode负责所有的客户端操作，standby状态的NameNode处于从属地位，维护着数据状态，随时准备切换。 &emsp;&emsp;两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了。 &emsp;&emsp;为了确保快速切换，standby状态的NameNode有必要知道集群中所有数据块的位置。为了做到这点，所有的datanodes必须配置两个NameNode的地址，发送数据块位置信息和心跳给他们两个。 &emsp;&emsp;对于HA集群而言，确保同一时刻只有一个NameNode处于active状态是至关重要的。否则，两个NameNode的数据状态就会产生分歧，可能丢失数据，或者产生错误的结果。为了保证这点，JNs必须确保同一时刻只有一个NameNode可以向自己写数据。 硬件资源 IP 主机名 namenode datanode zk journalnode 192.168.206.238 master 是 否 是 是 192.168.206.237 slave1 否 是 是 是 192.168.206.238 slave2 是 是 是 是 为了部署HA集群，应该准备以下事情： NameNode服务器：运行NameNode的服务器应该有相同的硬件配置。 JournalNode服务器：运行的JournalNode进程非常轻量，可以部署在其他的服务器上。 注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个，如3、5、7、9个等等。当运行N个节点时，系统可以容忍至少(N-1)/2个节点失败而不影响正常运行。 在HA集群中，standby状态的NameNode可以完成checkpoint操作，因此没必要配置Secondary NameNode、CheckpointNode、BackupNode。如果真的配置了，还会报错。 集群配置现在master上配置，拷贝到另外两台 hdfs-site.xml&emsp;&emsp;HA集群需要使用nameservice ID区分一个HDFS集群。另外，HA中还要使用一个词，叫做NameNode ID。同一个集群中的不同NameNode，使用不同的NameNode ID区分。为了支持所有NameNode使用相同的配置文件，因此在配置参数中，需要把nameservice ID作为NameNode ID的前缀。 dfs.nameservices 命名空间的逻辑名称 1234&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt; dfs.ha.namenodes.[nameservice ID]命名空间中所有NameNode的唯一标示名称。可以配置多个，使用逗号分隔。该名称是可以让DataNode知道每个集群的所有NameNode。当前，每个集群最多只能配置两个NameNode。 1234&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt; dfs.namenode.rpc-address.[nameservice ID].[name node ID] 每个namenode监听的RPC地址，dfs.namenode.http-address.[nameservice ID].[name node ID] 每个namenode监听的http地址 12345678910111213141516&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;192.168.205.173:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;192.168.205.173:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;192.168.204.238:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;192.168.204.238:50070&lt;/value&gt;&lt;/property&gt; dfs.namenode.shared.edits.dir,这是NameNode读写JNs组的uri。通过这个uri，NameNodes可以读写edit log内容。URI的格式qjournal://host1:port1;host2:port2;host3:port3/journalId。这里的host1、host2、host3指的是Journal Node的地址，这里必须是奇数个，至少3个；其中journalId是集群的唯一标识符，对于多个命名空间，也使用同一个journalId。配置如下 1234&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://192.168.204.237:8485;192.168.204.238:8485/mycluster&lt;/value&gt;&lt;/property&gt; dfs.client.failover.proxy.provider.[nameservice ID], 这里配置HDFS客户端连接到Active NameNode的一个java类，dfs.ha.fencing.methods配置active namenode出错时的处理类。当active namenode出错时，一般需要关闭该进程。处理方式可以是ssh也可以是shell。 123456789101112&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt; dfs.journalnode.edits.dir, 这是JournalNode进程保持逻辑状态的路径。这是在linux服务器文件的绝对路径。 1234&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/hdp/hadoop-2.6.5/data/journal&lt;/value&gt;&lt;/property&gt; 完整配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;192.168.205.173:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;192.168.205.173:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;192.168.204.238:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;192.168.204.238:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://192.168.204.237:8485;192.168.204.238:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/hdp/hadoop-2.6.5/data/journal&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///opt/hdp/hadoop-2.6.5/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///opt/hdp/hadoop-2.6.5/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; core-site.xmlfs.defaultFS, 客户端连接HDFS时，默认的路径前缀。如果前面配置了nameservice ID的值是mycluster，那么这里可以配置为授权信息的一部分。ha.zookeeper.quorum为zookeeper集群的地址。 12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/opt/hdp/hadoop-2.6.5/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;192.168.205.173:2181,192.168.204.237:2181,192.168.204.238:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml1234567&lt;configuration&gt; &lt;!-- 指定mr框架为yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml配置rm高可用 12 服务启动启动zk，分别在三台zk上启动三台zookeeper服务器，同时查看状态，其中一台leader，两台follower： 12zkServer.sh startzkServer.sh status 启动journalnode，在master上执行： 123[root@localhost hadoop-2.6.5]# sbin/hadoop-daemons.sh start journalnode 192.168.204.238: starting journalnode, logging to /opt/hdp/hadoop-2.6.5/logs/hadoop-root-journalnode-localhost.localdomain.out192.168.204.237: starting journalnode, logging to /opt/hdp/hadoop-2.6.5/logs/hadoop-root-journalnode-BFG-OSER-1308.out 格式化ZKFC，在master上执行： 1[root@localhost hadoop-2.6.5]# bin/hdfs zkfc -formatZK 格式成功后，查看zookeeper中可以看到 12[zk: localhost:2181(CONNECTED) 1] ls /hadoop-ha[mycluster] 格式化hdfs，master上执行： 1bin/hadoop namenode -format 启动namenode，在master上执行： 1sbin/hadoop-daemon.sh start namenode 在slave2上同步namenode的数据，同时启动standby的namenod,命令如下 12bin/hdfs namenode –bootstrapStandbysbin/hadoop-daemon.sh start namenode 启动启动datanode，在master上执行： 1sbin/hadoop-daemons.sh start datanode 启动ZKFC，在master上执行如下命令，完成ZKFC的启动 1sbin/hadoop-daemons.sh start zkfc 也可以单独没台上分别执行： 1sbin/hadoop-daemon.sh start zkfc 全部启动后，分别jps查看进程： 1234567891011121314151617181920[root@master hadoop-2.6.5]# jps5984 DFSZKFailoverController6562 Jps6137 NameNode5513 ResourceManager5071 QuorumPeerMain[root@slave1 hadoop-2.6.5]# jps27605 NodeManager27126 QuorumPeerMain28025 Jps27390 JournalNode[root@slave2 hadoop-2.6.5]# jps9924 JournalNode10677 DFSZKFailoverController10246 NodeManager9718 QuorumPeerMain10794 NameNode11053 Jps 测试高可用启动后master为active，slave2为standby 此时在master上执行如下命令，或者直接杀掉namenode进程关闭master上的namenode 1sbin/hadoop-daemon.sh stop namenode 发现slave2自动切换为active。 再次启动master上的namenode，则为standby，如此这可保证namenode的高可用。 参考：HadoopHA简述Hadoop2.5.2 HA高可靠性集群搭建(Hadoop+Zookeeper)]]></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hdfs基本操作]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-hdfs%2F</url>
    <content type="text"></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop和Spark部署]]></title>
    <url>%2F2017%2F06%2F09%2Fhadoop-spark-install%2F</url>
    <content type="text"><![CDATA[SSH 免密码登录安装Openssh server 1sudo yum install openssh-server 在所有机器上都生成私钥和公钥 1ssh-keygen -t rsa #一路回车 需要让机器间都能相互访问，就把每个机子上的id_rsa.pub发给master节点，传输公钥可以用scp来传输。 1scp ~/.ssh/id_rsa.pub spark@master:~/.ssh/id_rsa.pub.slave1 在master上，将所有公钥加到用于认证的公钥文件authorized_keys中 1cat ~/.ssh/id_rsa.pub* &gt;&gt; ~/.ssh/authorized_keys 将公钥文件authorized_keys分发给每台slave 1scp ~/.ssh/authorized_keys spark@slave1:~/.ssh/ 在每台机子上验证SSH无密码通信 123ssh masterssh slave1ssh slave2 如果登陆测试不成功，则可能需要修改文件authorized_keys的权限（权限的设置非常重要，因为不安全的设置安全设置,会让你不能使用RSA功能 ） 1chmod 600 ~/.ssh/authorized_keys 安装 Java从官网下载最新版 Java 就可以，Spark官方说明 Java 只要是6以上的版本都可以，我下的是jdk-8u121-linux-x64.tar.gz在/opt/hdp目录下直接解压 1tar -xzvf jdk-8u121-linux-x64.tar.gz 修改环境变量sudo vi /etc/profile，添加下列内容，注意将home路径替换成你的： 12345export WORK_SPACE=/opt/hadoopexport HADOOP_HOME=$WORK_SPACE/hadoop-2.6.5export JAVA_HOME=$WORK_SPACE/jdk1.8.0_121export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=$CLASSPATH:.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib 然后使环境变量生效，并验证 Java 是否安装成功 12345$ source /etc/profile #生效环境变量$ java -version #如果打印出如下版本信息，则说明安装成功java version "1.8.0_121"Java(TM) SE Runtime Environment (build 1.8.0_121-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) 安装 ScalaSpark官方要求 Scala 版本为 2.11.8，注意不要下错版本，我这里下了 2.11.8，官方下载地址 同样我们在~/workspace中解压 1tar -zxvf scala-2.11.8.tgz 再次修改环境变量sudo vi /etc/profile，添加以下内容： 12export SCALA_HOME=$WORK_SPACE/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin 同样的方法使环境变量生效，并验证 scala 是否安装成功 123$ source /etc/profile #生效环境变量$ scala -version #如果打印出如下版本信息，则说明安装成功Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL 安装配置 Hadoop YARN官网下载解压同样我们在~/workspace中解压 1tar -zxvf hadoop-2.6.5.tar.gz 配置 Hadoopcd ~/hadoop/hadoop-2.6.5/etc/hadoop进入hadoop配置目录，需要配置有以下7个文件：hadoop-env.sh，yarn-env.sh，slaves，core-site.xml，hdfs-site.xml，maprd-site.xml，yarn-site.xml 1、在hadoop-env.sh中配置JAVA_HOME 12# The java implementation to use.export JAVA_HOME=/opt/hadoop/jdk1.8.0_121 2、在yarn-env.sh中配置JAVA_HOME 12# some Java parametersexport JAVA_HOME=/opt/hadoop/jdk1.8.0_121 3、在slaves中配置slave节点的ip或者host， 12slave1slave2 4、修改core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/spark/workspace/hadoop-2.6.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; fs.default.name是NameNode的URI。hdfs://主机名:端口/ hadoop.tmp.dir ：Hadoop的默认临时路径，这个最好配置，如果在新增节点或者其他情况下莫名其妙的DataNode启动不了，就删除此文件中的tmp目录即可。不过如果删除了NameNode机器的此目录，那么就需要重新执行NameNode格式化的命令。 5、修改hdfs-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/opt/hdp/hadoop-2.6.5/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/opt/hdp/hadoop-2.6.5/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; dfs.name.dir是NameNode持久存储名字空间及事务日志的本地文件系统路径。 当这个值是一个逗号分割的目录列表时，nametable数据将会被复制到所有目录中做冗余备份。 dfs.data.dir是DataNode存放块数据的本地文件系统路径，逗号分割的列表。 当这个值是逗号分割的目录列表时，数据将被存储在所有目录下，通常分布在不同设备上。 dfs.replication是数据需要备份的数量，默认是3，如果此数大于集群的机器数会出错。注意：此处的name1、name2、data1、data2目录不能预先创建，hadoop格式化时会自动创建，如果预先创建反而会有问题。 6、修改mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7、修改yarn-site.xml 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8035&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 将配置好的hadoop-2.6.0文件夹分发给所有slaves吧 1scp -r ~/workspace/hadoop-2.6.0 spark@slave1:~/workspace/ 启动 Hadoop在 master 上执行以下操作，就可以启动 hadoop 了。 1234/opt/hadoop/hadoop-2.6.5 #进入hadoop目录bin/hadoop namenode -format #格式化namenodesbin/start-dfs.sh #启动dfs sbin/start-yarn.sh #启动yarn 验证 Hadoop 是否安装成功，可以通过jps命令查看各个节点启动的进程是否正常。在 master 上应该有以下几个进程： 12345$ jps #run on master3407 SecondaryNameNode3218 NameNode3552 ResourceManager3910 Jps 在每个slave上应该有以下几个进程： 1234$ jps #run on slaves2072 NodeManager2213 Jps1962 DataNode 或者在浏览器中输入 http://192.168.205.173:8088 ，应该有 hadoop 的管理界面出来了，并能看到 slave1 和 slave2 节点。 Spark安装下载解压进入官方下载地址下载最新版 Spark。我下载的是spark-1.3.0-bin-hadoop2.4.tgz。 在~/workspace目录下解压 12tar -zxvf spark-2.1.0-bin-hadoop2.6.tgzmv spark-2.1.0-bin-hadoop2.6.tgz spark-2.1.0 #原来的文件名太长了，修改下 配置 Spark12cd /opt/hadoop/spark-2.1.0/conf #进入spark配置目录cp spark-env.sh.template spark-env.sh #从配置模板复制 在spark-env.sh末尾添加以下内容（这是我的配置，你可以自行修改）： 1234567891011export SCALA_HOME=/opt/hadoop/scala-2.11.8export JAVA_HOME=/opt/hadoop/jdk1.8.0_121export HADOOP_HOME=/opt/hadoop/hadoop-2.6.5export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_MASTER_HOST=masterexport SPARK_LOCAL_IP=192.168.206.184export MASTER=spark://$&#123;SPARK_MASTER_IP&#125;:$&#123;SPARK_MASTER_PORT&#125;export SPARK_MASTER_PORT=7077export SPARK_LOCAL_DIRS=/opt/hadoop/spark-2.1.0export SPARK_DRIVER_MEMORY=2Gexport SPARK_EXECUTOR_MEMORY=2G 注：在设置Worker进程的CPU个数和内存大小，要注意机器的实际硬件条件，如果配置的超过当前Worker节点的硬件条件，Worker进程会启动失败。 vi slaves在slaves文件下填上slave主机名： 12slave1slave2 将配置好的spark-2.1.0文件夹分发给所有slaves吧 1scp -r spark-2.1.0 root@slave1:/opt/hadoop/ 启动Spark1sbin/start-all.sh 验证 Spark 是否安装成功用jps检查，在 master 上应该有以下几个进程： 123456$ jps7949 Jps7328 SecondaryNameNode7805 Master7137 NameNode7475 ResourceManager 在 slave 上应该有以下几个进程： 12345$jps3132 DataNode3759 Worker3858 Jps3231 NodeManager 进入Spark的Web管理页面： http://192.168.204.184:8080 运行示例12345678910111213141516#本地模式两线程运行./bin/run-example SparkPi 10 --master local[2]#Spark Standalone 集群模式运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://master:7077 \ lib/spark-examples-1.3.0-hadoop2.4.0.jar \ 100#Spark on YARN 集群上 yarn-cluster 模式运行./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ # can also be `yarn-client` lib/spark-examples*.jar \ 10 --class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)--master: 集群的master URL (如 spark://23.195.26.187:7077)--deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*--conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file://path， 那么所有的节点的path都包含同样的jar.application-arguments: 传给main()方法的参数 参考：CentOS配置ssh无密码登录Spark On YARN 集群安装部署Hadoop集群配置（最全面总结）]]></content>
      <tags>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Samba简单使用]]></title>
    <url>%2F2017%2F06%2F09%2Flinux-samba%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Samba是在Linux和UNIX系统上实现SMB协议的一个免费软件，由服务器及客户端程序构成，主要用来实现主机之间的资源共享。 samba安装centos下yum即可安装samba 1yum -y install samba 配置安装完后samba配置文件在/etc/samba目录下： vim /etc/samba/smb.conf添加 1234567891011...[test]comment = file of testpath = /opt/test/public = yeswritable = yescreate mask = 0664directory mask = 0775valid users = testuserbrowseable = yessync always = yes 添加用户授权1smbpasswd -a testuser 重启生效1/sbin/service smb restart 使用挂载到另一台机器，将服务test对应的目录挂载到/opt/data目录下 1mount -t cifs -o username=testuser,password=123456 //192.168.204.237/test /opt/data 挂载失败则在global中添加访问授权： 123...hosts allow = 192.168.204. 192.168.206. 192.168.202....]]></content>
      <tags>
        <tag>SAMBA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux自定义服务自启动脚本]]></title>
    <url>%2F2017%2F06%2F09%2Flinux-initd%2F</url>
    <content type="text"></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[login shell与non-login shell的区别]]></title>
    <url>%2F2017%2F06%2F09%2Flinux-shell%2F</url>
    <content type="text"></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-bashrc-profile]]></title>
    <url>%2F2017%2F06%2F09%2Flinux-bashrc-profile%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Linux下su与su -命令的区别]]></title>
    <url>%2F2017%2F06%2F09%2Flinux-su%2F</url>
    <content type="text"><![CDATA[su命令su命令用于切换当前用户身份到其他用户身份，变更时须输入所要变更的用户帐号与密码。在不加参数的情况下，su命令默认表示切换到root用户，之后只要输入root密码就可以切换身份为root了，操作完成后，使用exit或者Ctrl+D可以退出root切换到原先的用户。 语法1su (选项) (参数) 选项1234567-c &lt;指令&gt;，--command=&lt;指令&gt;：执行完指定的指令后，即恢复原来的身份-f，--fast：适用于csh与tsch，使shell不用去读取启动文件-，-l，--login：改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,logname。此外，也会变更PATH变量-m，-p，--preserve-environment：变更身份时，不要变更环境变量-s，--shell=：指定要执行的shell--help：显示帮助--version；显示版本信息 参数用户：指定要切换身份的目标用户。 实例变更帐号为root并在执行ls指令后退出变回原使用者： 1su -c ls root 变更帐号为test并改变工作目录至test的home目录： 1su - test 或者 su -l test su和su -的区别su命令和su -命令最大的本质区别就是：已切换到root用户为例， 前者只是切换了root身份，但Shell环境仍然是普通用户的Shell；而后者连用户和Shell环境一起切换成root身份了。因此，当时用su命令切换用户执行命令的时候，切记两者的区别，以免出现环境变量错误导致的程序执行错误问题。、如果只需要切换执行一条命令，su -c是更好的选择，执行完操作后就会切换回原来的账户。 su命令的缺点普通用户切换到root用户需要提供密码，从root切换到普通用户不需要密码 su命令虽然很方便，但是缺陷也很明显，就是切换成其他用户的时候需要知道对方密码。 如果需要切换到root用户就需要root密码，root是系统权限最高的用户，如果让太多人知道root密码，必然会不安全。为了解决这个问题我们可以使用sudo命令 sudo命令sudo命令用来以其他身份来执行命令，预设的身份为root。在/etc/sudoers中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。相比于su切换身份需要用户的密码，经常性的是需要root密码用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。 语法1sudo(选项)(参数) 选项12345678910-b：在后台执行指令-h：显示帮助-H：将HOME环境变量设为新身份的HOME环境变量-k：结束密码的有效期限，也就是下次再执行sudo时便需要输入密码-l：列出目前用户可执行与无法执行的指令-p：改变询问密码的提示符号-s：执行指定的shell-u&lt;用户&gt;：以指定的用户作为新的身份。若不加上此参数，则预设以root作为新的身份-v：延长密码有效期限5分钟-V ：显示版本信息。 sudo权限配置配置sudo必须通过编辑/etc/sudoers文件，而且只有超级用户才可以修改它，还必须使用visudo编辑。之所以使用visudo有两个原因，一是它能够防止两个用户同时修改它；二是它也能进行有限的语法检查。所以，即使只有你一个超级用户，你也最好用visudo来检查一下语法。 此时我们有三种选择：键入“e”是重新编辑，键入“x”是不保存退出，键入“Q”是退出并保存。如果真选择Q，那么sudo将不会再运行，直到错误被纠正。 visudo默认的是在vi里打开配置文件，用vi来修改文件。我们可以在编译时修改这个默认项。visudo不会擅自保存带有语法错误的配置文件，它会提示你出现的问题，并询问该如何处理。此时我们有三种选择：键入“e”是重新编辑，键入“x”是不保存退出，键入“Q”是退出并保存。如果真选择Q，那么sudo将不会再运行，直到错误被纠正。 /etc/sudoers文件修改123456789101112131415Defaults !visiblepwDefaults !env_resetDefaults env_keep = &quot;COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS&quot;Defaults env_keep += &quot;MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE&quot;Defaults env_keep += &quot;LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES&quot;Defaults env_keep += &quot;LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE&quot;Defaults env_keep += &quot;LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY&quot;Defaults env_keep += &quot;HOME&quot;root ALL=(ALL) ALLUser_Alias DTDEV = dtdevDTDEV ALL=(ALL) NOPASSWD:ALL,!/bin/bash,!/bin/su root,!/usr/bin/chattr,!/usr/sbin/useradd,!/usr/sbin/userdel,!/usr/bin/passwdUser_Alias SPORTS =dtdev1,dtdev2SPORTS ALL=(ALL) NOPASSWD: ALL 第一个ALL是指网络中的主机，我们后面把它改成了主机名，它指明foobar可以在此主机上执行后面的命令。第二个括号里的ALL是指目标用户，也就是以谁的身份去执行命令。最后一个ALL当然就是指命令名了。例如，我们想让dtdev用户在linux主机上以dtdev1或dtdev2的身份执行kill命令，这样编写配置文件： 1dtdev linux=(dtdev1,dtdev2) /bin/kill 但这还有个问题，dtdev到底以dtdev1还是dtdev2的身份执行？这时我们应该想到了sudo -u了，它正是用在这种时候。 foobar可以使用sudo -u jimmy kill PID或者sudo -u rene kill PID，但这样挺麻烦，其实我们可以不必每次加-u，把dtdev1或dtdev2设为默认的目标用户即可。再在上面加一行： 1Defaults:dtdev runas_default=dtdev1 Defaults后面如果有冒号，是对后面用户的默认，如果没有，则是对所有用户的默认。就像配置文件中自带的一行： 1Defaults !env_reset 表示所有用户切换都不重置环境变量 很多时候，我们本来就登录了，每次使用sudo还要输入密码就显得烦琐了。我们可不可以不再输入密码呢？当然可以，我们这样修改配置文件： 1SPORTS ALL=(ALL) NOPASSWD: ALL 你也可以说“某些命令用户dtdev不可以运行”，如以上配置通过使用!操作符，但这不是一个好主意。因为，用!操作符来从ALL中“剔出”一些命令一般是没什么效果的，一个用户完全可以把那个命令拷贝到别的地方，换一个名字后再来运行。 日志安全sudo为安全考虑得很周到，不仅可以记录日志，还能在有必要时向系统管理员报告。但是，sudo的日志功能不是自动的，必须由管理员开启。这样来做：12touch /var/log/sudovi /etc/syslog.conf 在syslog.conf最后面加一行（必须用tab分割开）并保存：1local2.debug /var/log/sudo 重启日志守候进程1ps aux grep syslogd 把得到的syslogd进程的PID（输出的第二列是PID）填入下面：1kill –HUP PID]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MYSQL安装及主从配置]]></title>
    <url>%2F2017%2F06%2F09%2Fmysql-master-slave%2F</url>
    <content type="text"><![CDATA[yum安装mysql 安装mysql5.6的yum源 a. 直接从网上安装mysql的yum源 12[user]$ sudo rpm -ivhhttp://repo.mysql.com/mysql-community-release-el6-5.noarch.rpm b. 从本地安装mysql的yum源将mysql-community-release-el6-5.noarch.rpm上传至目标机器 123[user]$ scp mysql-community-release-el6-5.noarch.rpm user@target:~[user]$ cd[user]$ sudo yum localinstall mysql-community-release-el6-5.noarch.rpm 更新源信息 1[user]$ sudo yum makecache 列出当前yum的可用源列表 1234[user]$ sudo yum repolistmysql-connectors-community MySQL Connectors Community 9mysql-tools-community MySQL Tools Community 12mysql56-community MySQL 5.6 Community Server 78 查看mysql5.6可用安装包 123456789101112131415161718192021[user]$ sudo yum search mysql-community================================= N/S Matched: mysql-community =================================mysql-community-bench.x86_64 : MySQL benchmark suitemysql-community-client.x86_64 : MySQL database client applications and toolsmysql-community-client.i686 : MySQL database client applications and toolsmysql-community-common.i686 : MySQL database common files for server and client libsmysql-community-common.x86_64 : MySQL database common files for server and client libsmysql-community-devel.i686 : Development header files and libraries for MySQL database client applicationsmysql-community-devel.x86_64 : Development header files and libraries for MySQL database client applicationsmysql-community-embedded.i686 : MySQL embedded librarymysql-community-embedded.x86_64 : MySQL embedded librarymysql-community-embedded-devel.i686 : Development header files and libraries for MySQL as an embeddable librarymysql-community-embedded-devel.x86_64 : Development header files and libraries for MySQL as an embeddable librarymysql-community-libs.i686 : Shared libraries for MySQL database client applicationsmysql-community-libs.x86_64 : Shared libraries for MySQL database client applicationsmysql-community-libs-compat.i686 : Shared compat libraries for MySQL 5.1 database client applicationsmysql-community-libs-compat.x86_64 : Shared compat libraries for MySQL 5.1 database client applicationsmysql-community-release.noarch : MySQL repository configuration for yummysql-community-server.x86_64 : A very fast and reliable SQL database servermysql-community-test.x86_64 : Test suite for the MySQL database server 安装mysql5.6 1[user]$ sudo yum install -y mysql-community-client mysql-community-common mysql-community-devel mysql-community-libs mysql-community-libs-compat mysql-community-release mysql-community-server mysql-community-test 启动mysqld 1[user]$ sudo /sbin/service mysqld start 查看mysql版本 12345678[user]$ mysql -u rootmysql&gt; select version();+-----------+| version() |+-----------+| 5.6.20 |+-----------+1 row in set (0.00 sec) 配置MySQL主添加一个权限，并创建测试数据库testdb 12grant all on *.* to bfsportsdt@&apos;192.168.193.226&apos; identified by &apos;85iwx|qttHsrlxPeyldb&apos;;create database testdb; 配置主的my.cnf配置文件 1234log_bin=mysql-bin #启动MySQ二进制日志系统binlog-do-db=testdb #需要同步的数据库名，如果有多个数据库，可重复此参数，每个数据库一行 binlog-ignore-db=mysql #不同步mysql系统数据库 server_id = 1 #设置服务器id，为1表示主服务器 重启并查看状态 1/sbin/service mysqld restart 1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000002 | 1394 | sports,nlp | |+------------------+----------+--------------+------------------+1 row in set (0.00 sec) 配置MySQL从导入主上的数据库testdb 配置my.cnf 12345log_bin=mysql-bin #启动MySQ二进制日志系统，注意：如果原来的配置文件中已经有这一行，就不用再添加了。 replicate-do-db=testdb replicate-ignore-db=mysql read_only=1 server_id = 2 #配置文件中已经有一行server-id=1，修改其值为2，表示为从数据库 重启MySQL,并执行以下操作 123stop slave;change master to master_host=&apos;192.168.193.226&apos;,master_user=&apos;bfsportsdt&apos;,master_password=&apos;85iwx|qttHsrlxPeyldb&apos;,master_log_file=&apos;mysql-bin.000002&apos;,master_log_pos=871;start slave; 1234567891011121314151617181920212223242526272829303132333435363738394041mysql&gt; SHOW SLAVE STATUS\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.193.226 Master_User: bfsportsdt Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000002 Read_Master_Log_Pos: 6383 Relay_Log_File: mysqld-relay-bin.000002 Relay_Log_Pos: 4892 Relay_Master_Log_File: mysql-bin.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB:testdb Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 5512 Relay_Log_Space: 5919 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 1Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: 1 row in set (0.00 sec) 注意查看：Slave_IO_Running: YesSlave_SQL_Running: Yes以上这两个参数的值为Yes，即说明配置成功！]]></content>
      <tags>
        <tag>MYSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux字符集更改]]></title>
    <url>%2F2017%2F05%2F15%2Flinux-character%2F</url>
    <content type="text"><![CDATA[查看当前字符集设置： 1echo $LANG 查看当前支持的字符集： 1locale -a 修改当前字符集设置： 12vi /etc/locale.confsource /etc/locale.conf]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux多线程下载工具axel安装使用]]></title>
    <url>%2F2017%2F05%2F15%2Flinux-axel%2F</url>
    <content type="text"><![CDATA[安装123456wget http://www.ha97.com/code/axel-2.4.tar.gztar zxvf axel-2.4.tar.gzcd axel-2.4./configuremakemake install 语法1axel [options] url1 [url2] [url...] 选项123456789101112--max-speed=x , -s x 最高速度x --num-connections=x , -n x 连接数x --output=f , -o f 下载为本地文件f --search[=x] , -S [x] 搜索镜像 --header=x , -H x 添加头文件字符串x（指定 HTTP header） --user-agent=x , -U x 设置用户代理（指定 HTTP user agent） --no-proxy ， -N 不使用代理服务器 --quiet ， -q 静默模式 --verbose ，-v 更多状态信息 --alternate ， -a Alternate progress indicator --help ，-h 帮助 --version ，-V 版本信息 实例如下载lnmp安装包指定10个线程，存到/tmp/： 1axel -n 10 -o /tmp/ http://www.linuxde.net/lnmp.tar.gz 如果下载过程中下载中断可以再执行下载命令即可恢复上次的下载进度。]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx location优先级]]></title>
    <url>%2F2016%2F10%2F28%2Fnginx-location%2F</url>
    <content type="text"><![CDATA[一、精确匹配 123location = /web/crossdomain.xml &#123;...&#125; 二、前缀匹配 普通前缀匹配 1234location /web/ &#123;...&#125; 优先前缀匹配 123location ^~/web/ &#123;...&#125; 三、正则匹配 区分大小写 123location ~ /web/.*\.xml$ &#123;...&#125; 不区分大小写 123location ~* /web/.*\.xml$ &#123;...&#125; 区分大小写取反 123location !~ /web/.*\.xml$ &#123;...&#125; 不区分大小写取反 123location !~* /web/.*\.xml$ &#123;... &#125; 四、指定匹配1234error_page 404 = @fallback;location @fallback &#123;... &#125; 五、匹配优先级详解先匹配普通，再匹配正则，正则匹配不覆盖普通匹配则精确匹配，但覆盖普通匹配的最大前缀匹配结果。 匹配普通location，没有顺序之分，原则是找到严格精确匹配（=），找到严格精确匹配则匹配结束。 如果没有找到严格精确匹配，找到所有普通location中的最大前缀匹配（~或^~）。 如果找到的最大前缀匹配是优先前缀匹配（^~），则匹配结束。否则进行正则匹配。 正则匹配没有顺序之分，原则是匹配成功便终止匹配。 如果没有匹配的正则location，则使用第三步中的普通最大前缀匹配。 六、匹配示例请求url：http://example.com/web/crossdomain.xml 1、如果精确匹配命中123location = /web/crossdomain.xml &#123;...&#125; 则优先精确匹配，并结束匹配。 2、如果命中多个前缀匹配1234567location /web/ &#123;...&#125; location /web/crossdomain.xml &#123;...&#125; 则记住其中最大前缀匹配，即/web/crossdomain.xml，并继续匹配 3、如果最长前缀匹配中是优先前缀匹配1234567location /web/ &#123;... &#125; location ^~ /web/crossdomain.xml &#123;... &#125; 则命中此最长的优先前缀匹配，即^~/web/crossdomain.xml，并终止匹配 4、如果命中多个正则匹配123456789101112131415location /web/ &#123;... &#125; location /web/crossdomain.xml &#123;... &#125; location ~* /web/ &#123;... &#125; location ~* /web/crossdomain.xml &#123;... &#125; 则忽略第二，三步中的最长前缀匹配，使用第一个命中的正则匹配，即~* /web/，并结束匹配 5、否则，命中前面几步中记住的最长前缀匹配 6、如果均未匹配到，且有404指定匹配1234error_page 404 = @fallback;location @fallback &#123;...&#125;]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx代理缓存配置详解]]></title>
    <url>%2F2016%2F10%2F28%2Fnginx-proxy-cache%2F</url>
    <content type="text"><![CDATA[Nginx代理相关配置 proxy_buffers [number] [size]number默认为8；size在32为平台下位4K，在64位平台为8K，即和一个内存页大小相等；此参数用来设定从后端server读取响应时使用的buffer空间，注意buffer的设定全部是针对一个链接而言（或者说是请求）。 proxy_buffer_size [size]默认值与proxy_buffers中size参数一致；设定用于保存后端server响应的第一部分内容的buffer大小，这部分包含一个小的响应的header。通常此值可以设置的更小。 proxy_buffering [on|off]默认值为on；设定是否buffer来自后端server的响应。当buffering开启时，nginx将会缓存从后端server收到的响应，保存在proxy_buffer_size和proxy_buffers设定的buffer中；如果整个响应无法全部保存在buffer内存中，则剩余部分将会写入临时文件，临时文件由proxy_max_temp_file_size、proxy_temp_file_write_size设定。当buffering关闭时，nginx收到响应后立即将会以同步的方式发送给客户端，nginx不会尝试读取整个响应（读取整个响应后才发送给客户端），nginx能够读取（等待客户端读取）的最大数据量为proxy_buffer_size。buffering可以通过响应的header中X-Accel-Buffering来指定，可选值为yes或者no；当然nginx也可以通过proxy_ignore_headers来忽略这个header。 proxy_busy_buffers_size [size]默认值为proxy_buffer_size或者proxy_buffers的2倍。如果nginx边读取响应，同时还将buffer中的数据发送给客户端，那么这个buffer即为busy；当buffering开启时，此指令用于设定当响应尚未完全读取之前，nginx能够发送给客户端的最大buffer数据量；剩余的buffer，将会用于读取响应，如果buffer不足，将写入临时文件。处于busy状态的buffer，尽管已经发送，当仍然不能被当前请求回收重用。 proxy_max_temp_file_size [size]默认值为1024m；当buffering开启时，如果proxy_buffer_size无法全部保存响应内容，那么剩余的将会被写入临时文件，这个指令就是控制临时文件的最大尺寸。 proxy_temp_path [path] [level]设定一个Path用来保存临时文件，path用来设定临时文件的路径，其中level表示目录的层级，最大为3级，1级的目录名有一个字符表示，2级为2个，三级为三个。例如：proxy_temp_path /home/data/nginx/tmp 1 2，表示开启2级目录。 proxy_connect_timeout [time]默认值为60s；定义nginx与后端server建立链接的超时时间，此时间不能超过75秒。 proxy_hide_header [field]默认nginx不会将响应中的Date、Server、X-Pa、X-Accel-…等headers发送给客户端。proxy_hide_header指令设置不需要发送的额外的field；反之，对于可以传递给客户端的filed，通过proxy_pass_header指令指定。 proxy_http_version [1.0 | 1.1]默认值为1.0；指定与后端server通信时使用的http协议的版本，在与keepalive链接配合使用时，建议此值为1.1。 proxy_ignore_headers [field…]指定来自后端server的响应中的某些header不会被处理，如下几个fields可以被ignore：X-Accel-Redirect、X-Accel-Expires、X-Accel-Limit-Rate、X-Accel-Buffering、X-Accel-Charset、Expires、Cache-Control、Set-Cookie、Vary。不被处理就是nginx不会尝试解析这些header并应用它们，比如nginx处理来自后端server的Expires，将会影响它本地的文件cache的机制。 proxy_intercept_errors [on | off]默认值为“off”；当后端server的响应code &gt;= 300时，是否跳转到error_page指令指定的错误页面上。如果为off，将nginx不做拦截，直接将后端server的响应返回给客户端。 123proxy_intercept_errors on; error_page 404 /404.html; error_page 500 502 503 504 /50x.html proxy_next_upstream此指令很重要，用来设定当何种情况下，请求将会被转发给下一个后端server。此指令允许的参数列表为：1）error：当nginx与后端server建立连接、发送请求、读取响应header时，发生错误。2）timeout：当nginx与后端server在建立链接、发送请求（write）、读取响应header（read）时，操作超时。3）invalid_header：如果后端server返回空的或者无效的header时。4）http_500、http_502、http_503、http_504、http_403、http_404：后端server返回指定的错误code时。5）off：不将请求转发给下一个server，直接响应错误code。 proxy_pass [URL]只在location段设置有效，设置转发给后端server时请求所使用的HTTP协议和URI，协议可以为http、https；地址可以为域名、ip，其中port是可选的，默认为80。如果proxy_pass指令指定了URI，那么请求中匹配location部分的URI将会被替换。 1234location /test &#123; proxy_pass http://127.0.0.1:8080/testex; &#125; #http://exampe.org/test/a.html将会被转发给http://127.0.0.1:8080/testex/a.html 如果location中使用了uri，但是proxy_pass中没有使用，那么请求的uri将会全部添加到proxy_pass指定的路径之后。 1234location /test &#123; proxy_pass http://127.0.0.1:8080; &#125; #请求url为http://example.org/test/a.html将会被转发到http://127.0.0.1:8080/test/a.html 如果location中使用了正则表达式，那么在proxy_pass指令中不应该指定uri，否则uri的替换是无法断定的。 如果在location中使用了“rewrite”指令（break）对请求的uri进行了修改，那么proxy_pass指令中的uri将会被忽略，被rewrite之后的全量uri将会传递给server。 12345location /test &#123; rewrite /test/([^/]+) /users?test=$1 break; proxy_pass http://127.0.0.1; &#125; #http://exmaple.org/test/zhangsan，将会转发到http://127.0.0.1/users?test=zhangsan 其中server名称、端口、uri等均可以使用变量： 123proxy_pass http://$host$uri; ##等价于 proxy_pass $request; proxy_pass_header [field]上述我们已经知道，proxy_hide_header指令默认不会把几个header传递给client，那么proxy_pass_header则允许其中某个header传递给客户端。 proxy_pass_request_body [on | off]默认值为on，不建议修改此值；是否将原始的请求body转发给后端server。如果你需要nginx对body进行裁剪，然后再转发给后端server，那么此处可以设定为off。 proxy_pass_request_header [on | off]默认值为on，不建议修改此值；指定是否将请求的原始header转发给后端的server。如果在某些场景下，nginx需要忽略所有的请求header，或者对header进行改造时，可以关闭此值。 proxy_read_timeout [time]默认值为“60s”；设定nginx从后端server读取响应时的超时时间，此时间为两次read操作之间阻塞的最大时间，而非整个响应的读取耗时。 proxy_send_timeout [time]默认为“60s”；nginx将响应发送给客户端时最长的阻塞时间。同上。 proxy_set_header [field] [value]此指令的作用是，将请求发送给后端server之前，重新设置或者append指定的header的值。当然我们也可以重新设置上述两个header。 Nginx缓存开启安装nginx之后只需要两个命令即可启用nginx缓存功能：proxy_cache_path和proxy_cache。proxy_cache_path用来设置缓存相关配置，proxy_cache用来开启缓存。1234567891011121314151617 proxy_cache_path /opt/data1 levels=1:2 keys_zone=my_cache:128m inactive=7d max_size=10g use_temp_path=off; server&#123;listen 80; location ~ \.lb$ &#123; proxy_pass http://vodcdnsrc.baofengcloud.com; proxy_cache my_cache; proxy_cache_revalidate on; proxy_cache_min_uses 3; proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504; proxy_cache_lock on; proxy_cache_key $request_uri; proxy_cache_valid 400 404 500 0m; proxy_cache_valid any 1h; &#125;&#125; /opt/data1：用于缓存本地文件的磁盘目录。 levels：用于缓存文件的目录结构配置，不配置此项则所有文件存于同一个目录，大量文件存放于同一个目录会导致访问变慢。可以按照需求配置目录结构，如：1：2，是对请求uri进行MD5hash，其中最后以为做1级目录，倒数二三位做2级目录。 keys_zone：内存缓存空间的名字和大小。 inactive：指定了项目在不被访问的情况下能够在内存中保持的时间。在上面的例子中，如果一个文件在 60 分钟之内没有被请求，则缓存管理将会自动将其在内存中删除，不管该文件是否过期。该参数默认值为 10 分钟（10m）。 max_size：设置了本地缓存的上限（在上面的例子中是 10G）。这是一个可选项；如果不指定具体值，那就是允许缓存不断增长，占用所有可用的磁盘空间。当缓存达到这个上线，处理器便调用 cache manager 来移除最近最少被使用的文件，这样把缓存的空间降低至这个限制之下。 use_temp_path：指示nginx将临时文件写在同一个目录中，减少数据的拷贝。该参数在nginx 1.7版本及以上支持。 proxy_cache：控制哪部分location启用缓存，也可将proxy_cache应用到server部分，这会将缓存应用到所有的location中。 Nginx缓存调优相关参数 proxy_cache_revalidate ：指示 Nginx 在刷新来自服务器的内容时使用 GET 请求。如果客户端的请求项已经被缓存过了，但是在缓存控制头部中定义为过期，那么 Nginx 就会在 GET 请求中包含 If-Modified-Since 字段，发送至服务器端。这项配置可以节约带宽，因为对于 Nginx 已经缓存过的文件，服务器只会在该文件请求头中 Last-Modified 记录的时间内被修改时才将全部文件一起发送。 proxy_cache_min_uses：该指令设置同一链接请求达到几次即被缓存，默认值为 1 。当缓存不断被填满时，这项设置便十分有用，因为这确保了只有那些被经常访问的内容会被缓存。 proxy_cache_use_stale：该配置项指定，当无法从原始服务器获取最新的内容时， Nginx 可以分发缓存中的陈旧（stale，编者注：即过期内容）内容。这种情况一般发生在关联缓存内容的原始服务器宕机或者繁忙时。比起对客户端传达错误信息，Nginx 可发送在其内存中的陈旧的文件。 Nginx 的这种代理方式，为服务器提供额外级别的容错能力，并确保了在服务器故障或流量峰值的情况下的正常运行。如以上配置中当 Nginx 收到服务器返回的error，timeout或者其他指定的5xx错误，并且在其缓存中有请求文件的陈旧版本，则会将这些陈旧版本的文件而不是错误信息发送给客户端。 proxy_cache_lock：该参数被启用时，当多个客户端请求一个缓存中不存在的文件（或称之为一个 MISS），只有这些请求中的第一个被允许发送至服务器。其他请求在第一个请求得到满意结果之后在缓存中得到文件。如果不启用proxy_cache_lock，则所有在缓存中找不到文件的请求都会直接与服务器通信。 proxy_cache_key：设置Web缓存的Key值，Nginx根据Key值md5哈希存储缓存。 proxy_cache_valid：设置不同响应的缓存，如以上配置，不缓存400,404,500，其他响应缓存1小时。 跨多块硬盘分割缓存当有多块磁盘是，不需要建立磁盘RAID，nginx可以在多块磁盘之间分割缓存。123456789101112131415proxy_cache_path /opt/data1 levels=1:2 keys_zone=cache_1:128m inactive=7d max_size=10g use_temp_path=off;proxy_cache_path /opt/data2 levels=1:2 keys_zone=cache_2:128m inactive=7d max_size=10g use_temp_path=off;split_clients $request_uri $cache_disk &#123; 50% cache_1; * cache_2;&#125;server&#123; location ~ \.lb$ &#123; proxy_pass http://vodcdnsrc.baofengcloud.com; proxy_cache_key $request_uri; proxy_cache $cache_disk; &#125;&#125; 上面例子中配置了两个缓存区，cache_1和cache_2，分别属于不同的存储目录。split_clients配置指定文件按照$request_uri(请求URI)的hash值来决定存储在其中哪块磁盘上，同时分别指定了每块磁盘的比例。 Nginx缓存状态检测1add_header X-Cache-Status $upstream_cache_status; nginx提供了$upstream_cache_status这个变量来显示缓存的状态，我们可以在配置中添加一个http头来显示这一状态，使用add_header可以添加一个返回头。 $upstream_cache_status的可能值如下： MISS：响应在缓存中找不到，所以需要在服务器中取得。这个响应之后可能会被缓存起来。 BYPASS：响应来自原始服务器而不是缓存，因为请求匹配了一个proxy_cache_bypass（见下面我可以在缓存中打个洞吗？）。这个响应之后可能会被缓存起来。＊EXPIRED：缓存中的某一项过期了，来自原始服务器的响应包含最新的内容。 STALE：内容陈旧是因为原始服务器不能正确响应。需要配置proxy_cache_use_stale。 UPDATING：内容过期了，因为相对于之前的请求，响应的入口（entry）已经更新，并且proxy_cache_use_stale的updating已被设置。 REVALIDATED：proxy_cache_revalidate命令被启用，NGINX检测得知当前的缓存内容依然有效（If-Modified-Since或者If-None-Match）。 HIT：响应包含来自缓存的最新有效的内容。 Nginx缓存控制和优先级nginx作为代理使用的时候，会从源站得到Cache-Control，No-Cache，No-Store等响应字段，此种情况下nginx不缓存文件，同时，nginx默认缓存GET以及HEAD信息，不缓存post。 proxy_ignore_headers：可以配置nginx忽略哪些请求头，对不缓存的文件也进行缓存。 proxy_cache_methods：可以配置nginx缓存哪些请求类型， GET HEAD POST。 影响缓存的配置项主要有：nginx的inactive，源服务器的max-age和Expires以及nginx的proxy_cache_vaild。其中优先级顺序为：inactive、源服务器Expires、源服务器的max-age、proxy-cache-vaild。 Nginx代理缓存配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071worker_processes 1; #处理进程数，一般与服务器线程数相同 events &#123; use epoll; #事件模型，epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型 worker_connections 65535;&#125; http &#123; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; &quot;$upstream_cache_status&quot; &apos;; access_log /var/log/nginx/access.log main; error_log /var/log/nginx/error.log; sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。 #autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。 #tcp_nopush on; #防止网络阻塞 #tcp_nodelay on; #防止网络阻塞 keepalive_timeout 65;#长连接超时时间，单位是秒 include mime.types; default_type application/octet-stream; client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 64k; proxy_buffers 4 128k; proxy_busy_buffers_size 256k; proxy_temp_file_write_size 256k; proxy_cache_path /opt/data1 levels=1:2 keys_zone=cache_1:128m inactive=7d max_size=10g use_temp_path=off; proxy_cache_path /opt/data2 levels=1:2 keys_zone=cache_2:128m inactive=7d max_size=10g use_temp_path=off; split_clients $request_uri $cache_disk &#123; 50% cache_1; * cache_2; &#125; proxy_redirect off; #proxy_set_header Host $host; #设置回源的host信息 proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; server&#123; listen 80; server_name localhost; access_log /var/log/nginx/doaccess.log main; error_log /var/log/nginx/doerror.log error; location = /crossdomain.xml &#123; root /opt/data; &#125; location ~ \.lb$ &#123; proxy_pass http://vodcdnsrc.baofengcloud.com; proxy_ignore_headers Cache-Control; proxy_buffering on; proxy_cache_key $request_uri; proxy_cache_lock on; proxy_cache $cache_disk; proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504; proxy_cache_valid 400 404 500 0m; proxy_cache_valid any 1h; proxy_cache_min_uses 1; add_header X-Cache-Status &quot;$upstream_cache_status&quot;; &#125; &#125;&#125; 参考ngx_http_proxy_module所有参数详解NGINX缓存使用官方指南]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP开始结束及状态转换]]></title>
    <url>%2F2016%2F10%2F11%2Ftcp-open-close%2F</url>
    <content type="text"><![CDATA[TCP的状态图 图片地址 建立连接协议（三次握手） 客户端发送一个带SYN标志的TCP报文到服务器。 服务器端回应客户端的，这个报文同时带ACK标志和SYN标志，它表示对刚才客户端SYN报文的回应；同时又标志SYN给客户端，询问客户端是否准备好进行数据通讯。 客户必须再次回应服务段一个ACK报文。 连接终止协议（四次握手）为什么是四次：由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 TCP客户端发送一个FIN，用来关闭客户到服务器的数据传送。 服务器收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号。 服务器关闭客户端的连接，发送一个FIN给客户端。 客户段发回ACK报文确认，并将确认序号设置为收到序号加1。 服务端（被动方）状态转换CLOSED-&gt;LISTEN-&gt;SYN_RECV-&gt;ESTABLISHED-&gt;CLOSE_WAIT-&gt;LAST_ACK-&gt;CLOSED 客户端（主动方）状态转换CLOSED-&gt;SYN_SENT-&gt;ESTABLISHED-&gt;FIN_WAIT_1-&gt;FIN_WAIT_2-&gt;TIME_WAIT-&gt;CLOSED 状态解释CLOSED：表示初始状态。 LISTEN：表示服务器端的某个SOCKET处于监听状态，可以接受连接。 SYN_RCVD：表示接受到了SYN报文，在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本上用netstat你是很难看到这种状态的，除非你特意写了一个客户端测试程序，故意将三次TCP握手过程中最后一个ACK报文不予发送。 SYN_SENT：这个状态与SYN_RCVD遥想呼应，当客户端SOCKET执行CONNECT连接时，它首先发送SYN报文，因此也随即它会进入到了SYN_SENT状态，并等待服务端的发送三次握手中的第2个报文。SYN_SENT状态表示客户端已发送SYN报文。 ESTABLISHED：表示连接已经建立。 FIN_WAIT_1：FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。 FIN_WAIT_2：实际上FIN_WAIT_2状态下的SOCKET，表示半连接，即客户端已经关闭连接，但服务端暂时还有点数据需要传送给你，稍后再关闭连接。 TIME_WAIT：表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。 CLOSING：这种状态比较特殊，实际情况中应该是很少见，属于一种比较罕见的例外状态。正常情况下，当你发送FIN报文后，按理来说是应该先收到（或同时收到）对方的ACK报文，再收到对方的FIN报文。但是CLOSING状态表示你发送FIN报文后，并没有收到对方的ACK报文，反而却也收到了对方的FIN报文。什么情况下会出现此种情况呢？其实细想一下，也不难得出结论：那就是如果双方几乎在同时close一个SOCKET的话，那么就出现了双方同时发送FIN报文的情况，也即会出现CLOSING状态，表示双方都正在关闭SOCKET连接。 CLOSE_WAIT：表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。 LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，即可以进入到CLOSED可用状态了。]]></content>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS解析原理]]></title>
    <url>%2F2016%2F10%2F11%2Fdns-parse%2F</url>
    <content type="text"><![CDATA[什么是DNSDNS（Domain Name System，域名系统），互联网上域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，最终得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。 域名系统结构 根域名服务器：根域名服务器是最高层次的域名服务器，也是最重要的域名服务器。所有的根域名服务器都知道所有的顶级域名服务器的域名和IP地址。 顶级域名服务器： 这些域名服务器负责管理在该顶级域名服务器注册的所有二级域名。当收到DNS查询请求时，就给出相应的回答(可能是最后的结果，也可能是下一步应当找的域名服务器的IP地址)。 权限域名服务器：这是负责一个区的域名服务器。当一个权限域名服务器还不能给出最后的查询回答时，就会告诉发出查询请求的DNS客户，下一步应当找哪一个权限域名服务器。 本地域名服务器：本地域名服务器并不属于域名服务器层次结构，但它对域名系统非常重要。当一个主机发出DNS查询请求时，这个查询请求报文就发送给本地域名服务器。 域名记录类型A记录：子域名对应的目标主机地址。目标主机地址类型只能使用IP地址MX记录：将以该域名为结尾的电子邮件指向对应的邮件服务器以进行处理。NS记录：解析服务器记录。用来表明由哪台服务器对该域名进行解析。CNAME 记录：通常称别名指向。目标主机地址只能使用主机名，不能使用IP地址。一个主机地址同时存在A记录和CNAME记录，则CNAME记录不生效。 常见的DNS权威DNS：权威DNS即最终决定域名解析结果的服务器，开发者可以在权威DNS上配置、变更、删除具体域名的对应解析结果信息。递归DNS：递归DNS又称为Local DNS，它没有域名解析结果的决定权，但代理了用户向权威DNS获取域名解析结果的过程。递归DNS上有缓存模块，当目标域名存在缓存解析结果并且TTL未过期时（每个域名都有TTL时间，即有效生存时间，若域名解析结果缓存的时间超过TTL，需要重新向权威DNS获取解析结果），递归DNS会返回缓存结果，否则，递归DNS会一级一级地查询各个层级域名的权威DNS直至获取最终完整域名的解析结果。公共DNS：公共DNS是递归DNS的一种特例，它是一种全网开放的递归DNS服务，而传统的递归DNS信息一般由运营商分发给用户。Google的8.8.8.8属于公共DNS。 DNS解析过程图片地址 浏览器中输入“www.net.compsci.googleplex.edu”，发出解析请求。 本机的域名解析器( resolver程序)查询本地缓存和host文件中是否为域名的映射关系，如果有则调用这个IP地址映射，完成解析。 如果hosts与本地解析器缓存都没有相应的网址映射关系，则本地解析器会向TCP/IP参数中设置的首选DNS服务器（我们叫它本地DNS服务器）发起一个递归的查询请求。 服务器收到查询时，如果要查询的域名由本机负责解析，则返回解析结果给客户机，完成域名解析，此解析具有权威性。如果要查询的域名，不由本地DNS服务器解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。 如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置（是否递归）进行查询，如果未用开启模式，本地DNS就把请求发至13台根DNS。如果用的是递归模式，此DNS服务器就会把请求转发至上一级DNS服务器，由上一级服务器进行解析，上一级服务器如果不能解析，或找根DNS或把转请求转至上上级，以此循环。 根DNS服务器收到请求后会判断这个域名(.edu)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。 本地DNS服务器收到IP信息后，将会联系负责.edu域的这台服务器。 负责.edu域的服务器收到请求后，如果自己无法解析，它就会找一个管理.edu域的下一级DNS服务器地址(googleplex.com)给本地DNS服务器。 当本地DNS服务器收到这个地址后，就会找googleplex.com域服务器，10、11重复上面的动作，进行查询。 最后compsci.googleplex.edu返回需要解析的域名的IP地址给本地DNS服务器。 本地域名服务器缓存这个解析结果（同时也会缓存，6,8,10返回的结果）。 本地域名服务器同时将结果返回给本机域名解析器。 本机缓存解析结果。 本机解析器将结果返回给浏览器。 浏览器通过返回的IP地址发起请求。 递归查询和迭代查询 递归查询：如果主机所询问的本地域名服务器不知道被查询域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其他根域名服务器继续发出查询请求报文，而不是让该主机自己进行下一步的查询。 迭代查询：当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地域名服务器：“你下一步应当向哪一个域名服务器进行查询”。然后让本地域名服务器进行后续的查询，而不是替本地域名服务器进行后续的查询。 由此可见，客户端到本地DNS服务器，本地DNS与上级DNS服务器之间属于递归查询；DNS服务器与根DNS服务器之前属于迭代查询。 实际环境中，因为采用递归模式会导致DNS服务器流量很大，所以现在大多数的DNS都是迭代模式。这也是CDN厂商实现智能DNS的前提。 智能DNS的基本实现原理智能DNS主要实现的功能是能够判断用户所在的具体运营商和省份，在根据域名的覆盖配置策略，返回应该为用户提供服务的IP地址。例如，当上海电信的用户解析域名时，返回上海电信的ip地址；北京电信的用户访问时，访问北京电信的ip地址。 DNS解析常见问题 域名缓存正常情况下，域名会在各级dns缓存，当缓存超过一个ttl后，解析请求会递归到权威dns上。但是部分运营商有时会恶意缓存dns的解析结果，主要原因：保证用户访问流量在本网内消化：国内的各互联网接入运营商的带宽资源、网间结算费用、IDC机房分布、网内ICP资源分布等存在较大差异。为了保证网内用户的访问质量，同时减少跨网结算，运营商在网内搭建了内容缓存服务器，通过把域名强行指向内容缓存服务器的IP地址，就实现了把本地本网流量完全留在了本地的目的。推送广告：有部分LocalDNS会把部分域名解析结果的所指向的内容缓存，并替换成第三方广告联盟的广告。域名缓存的结果是主要会导致域名更换解析地址后用户不能及时更新导致访问失败。有些缓存服务器只缓存80端口的服务，导致对其他端口的访问失败。另外，当缓存服务器故障时也会影响到用户的正常访问。 解析不精准解析转发：运营商自身不进行域名递归解析，而是把域名解析请求转发到其它运营商的递归DNS上的行为。通常小运营商会将解析请求转发到其他运营商的递归dns上。这样导致权威dns服务器无法获取客户端的运营商信息。出口NAT：运营商的LocalDNS虽然是按照标准的DNS协议进行递归，但是因为在网络上存在多出口且配置了目标路由NAT，结果导致LocalDNS最终进行递归解析的时候的出口IP就有概率不为本网的IP地址。 解析延迟高DNS首次查询或缓存过期后的查询，需要递归遍历多个DNS服务器以获取最终的解析结果，这增加了网络请求的前置延时时间。特别是在移动互联网场景下，移动网络质量参差不齐，弱网环境的RTT时间可能高达数百毫秒，对于一次普通的业务请求而言，上述延时是非常沉重的负担。另一方面，弱网环境下的解析超时、解析失败等现象屡见不鲜。 HttpDNS原理客户端以HTTP的形式通过IP请求域名，并带上一些参数。服务器根据户端地理位置、运营商信息以及给定的参数进行精准调度，给出最优的ip或者IP列表。 HttpDNS主要解决的问题 LocalDNS劫持：由于HttpDNS是通过ip直接请求http获取服务器A记录地址，不存在向本地运营商询问domain解析过程，所以从根本避免了劫持问题。 （对于http内容tcp/ip层劫持，可以使用验证因子或者数据加密等方式来保证传输数据的可信度） 平均访问延迟下降：由于是ip直接访问省掉了一次domain解析过程，（即使系统有缓存速度也会稍快一些‘毫秒级’）通过智能算法排序后找到最快节点进行访问。 用户连接失败率下降：通过算法降低以往失败率过高的服务器排序，通过时间近期访问过的数据提高服务器排序，通过历史访问成功记录提高服务器排序。如果ip(a)访问错误，在下一次返回ip(b)或者ip(c) 排序后的记录。（LocalDNS很可能在一个ttl时间内（或多个ttl）都是返回记录。 参考：DNS101全局精确流量调度新思路-HttpDNS服务详解移动互联网时代，如何优化你的网络 —— 域名解析篇企业级移动应用如何优化域名解析和业务访问]]></content>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译支持h265的ffmpeg]]></title>
    <url>%2F2016%2F09%2F20%2Fffmpeg-h265%2F</url>
    <content type="text"><![CDATA[准备cmake要升级要2.8.8 yasm要升级到1.2.0 安装x26412345678git clone git://git.videolan.org/x264.gitcd x264./configure --enable-static --disable-opencl --disable-avs--disable-cli --disable-ffms --disable-gpac --disable-lavf--disable-swscalemakesudo make installsudo ldconfig 安装x26512345678hg clone https://bitbucket.org/multicoreware/x265hg checkout 0.8cd x265/build/linux./make-Makefiles.bash# 这里将 LOG_CU_STATISTICS 设置为ON，然后，按下“c”，实现configure，按下“q”退出makesudo make installsudo ldconfig 编译ffmpeg12./configure --enable-gpl --enable-libx264 --enable-libx265 --disable-stripping --disable-optimizations --extra-cflags=-g --enable-libfaac --enable-nonfree --enable-libspeex --extra-cflags=-I/usr/local/include --extra-ldflags=-L/usr/local/libmake 出错： 查看config.log 123456789In file included from /tmp/ffconf.gFGcukGK.c:1:/usr/local/include/x264.h:40:4: warning: #warning You must include stdint.h or inttypes.h before x264.hcheck_pkg_config x265 x265.h x265_encoder_encodepkg-config --exists --print-errors x265Package x265 was not found in the pkg-config search path.Perhaps you should add the directory containing `x265.pc&apos;to the PKG_CONFIG_PATH environment variableNo package &apos;x265&apos; foundERROR: x265 not found 提示找不到x265的链接库文件x265.pc, 将编译目录下的x265.pc拷贝到/usr/local/lib下： 1cp x265.pc /usr/local/lib 然后修改/etc/profile中的环境变量PKG_CONFIG_PATH: 1export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/ 查看是否已包含h265 1pkg-config --list-all | greo x265 重新编译ffmpeg，还是出错： 1234#ls /usr/local/lib | grep libx264libx264.alibx264.solibx264.so.146 可能原先我做过编译，在/usr/local目录下面安装了x264，可能有冲突，把x264在/usr/local目录下的一些相关文件卸载掉，重新编译x264，再编译ffmpeg 运行ffmpeg，已包含x265 接下来转码一个h.265视频到h.264看看效果。ffmpeg -i h265.mkv -c:v libx264 -preset medium -c:a aac -strict experimental -f mp4 -b:a 128k outputh264.mp4 参考：Linux编译FFmpeg，支持x264和x265(HEVC)]]></content>
      <tags>
        <tag>FFMPEG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS上hls无法播放问题]]></title>
    <url>%2F2016%2F08%2F01%2Fhls-ios-profile%2F</url>
    <content type="text"><![CDATA[前段时间在一个直播项目中遇到一个直播流在pc，android均可以正常播放，但是在ios上无法播放的问题；检查过后发现并不是数据生成的异常导致的，最后怀疑可能是数据流本身的编解码参数有错误。通过ffmeg拉流解析发现，video的profile为h264 high 4:2:2，而ios设备目前并不支持profile高于high级别高于4.1的视频的解码。 File format for the file segmenter can be a QuickTime movie, MPEG-4 video, or MP3 audio, using the specified encoding. Stream format for the stream segmenter must be MPEG elementary audio and video streams, wrapped in an MPEG-2 transport stream, and using the following encoding. The Audio Technologies and Video Technologies list supported compression formats. Encode video using H.264 compression H.264 Baseline 3.0: All devices H.264 Baseline 3.1: iPhone 3G and later, and iPod touch 2nd generation and later. H.264 Main profile 3.1: iPad (all versions), Apple TV 2 and later, and iPhone 4 and later. H.264 Main Profile 4.0: Apple TV 3 and later, iPad 2 and later, and iPhone 4S and later H.264 High Profile 4.0: Apple TV 3 and later, iPad 2 and later, and iPhone 4S and later. H.264 High Profile 4.1: iPad 2 and later and iPhone 4S and later. A frame rate of 10 fps is recommended for video streams under 200 kbps. For video streams under 300 kbps, a frame rate of 12 to 15 fps is recommended. For all other streams, a frame rate of 29.97 is recommended. Encode audio as either of the following: HE-AAC or AAC-LC, stereo MP3 (MPEG-1 Audio Layer 3), stereo 以上是iOS设备上支持的音视频编解码格式，以及相关参数的推荐设置。 参考：HTTP Live Streaming Overviewh264 profile &amp; level]]></content>
      <tags>
        <tag>HLS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客颜色设置]]></title>
    <url>%2F2016%2F07%2F18%2Fhexo-tips%2F</url>
    <content type="text"><![CDATA[hexo博客的颜色由css控制，具体文件是由位于目录hexoblog/public/css下的main.css来定义，通过这篇博客Hexo博客小知识：更改背景颜色介绍的方法修改以下两处可以将博客背景色更换成自己喜欢的颜色 12345678910111213/*头部的背景颜色设置*、2134 .header &#123;2135 background: #d1fab8;2136 &#125;/*主体部分颜色设置*/ 187 position: relative; 188 font-family: 'Lato', "PingFang SC", "Microsoft YaHei", sans-serif; 189 font-size: 14px; 190 line-height: 2; 191 color: #555; 192 background: #e0fccf; 193 &#125; 但是以上修改在重新生成blog时会失效，原因是main.css也是在博客发布时生成的，每次发布都会被重新生成的文件替换掉，所以只有修改源文件中的定义才能生效，源文件目录themes/next/source/css 修改文件themes/next/source/css/_variables/base.styl以下两处，增加header-bgex 和body-bgex两个颜色定义，一个是头部和脚部的颜色值定义，一个是主题部分颜色值定义。同时主题颜色的更改在48行修改body-bg-colo的值即可。 12345678910111211 $header-bgex = #d1fab812 $body-bgex = #e0fccf13 $whitesmoke = #f5f5f514 $gainsboro = #eee15 $gray-lighter = #ddd16 $grey-light = #ccc17 $grey = #bbb18 $grey-dark = #999...47 // Background color for &lt;body&gt;48 //$body-bg-color = white49 $body-bg-color = $body-bgex 修改themes/next/source/css/_schemes/Mist/sidebar/_header.styl文件中头部的背景色为base.styl定义的背景色 13 .header &#123; background: $header-bgex; &#125; 修改themes/next/source/css/_schemes/Mist/index.styl中footer相关定义，可以修改footer颜色值 1234567872 // Footer73 // --------------------------------------------------74 .footer &#123;75 margin-top: 80px;76 padding: 10px 0;77 background: $header-bgex;78 color: $grey-dim;79 &#125; 其他部分的颜色样式修改同理。]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep使用详解]]></title>
    <url>%2F2016%2F07%2F13%2Flinux-grep%2F</url>
    <content type="text"><![CDATA[grep简介grep （global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。Unix的grep家族包 括grep、egrep和fgrep。egrep和fgrep的命令只跟grep有很小不同。egrep是grep的扩展，支持更多的re元字符， fgrep就是fixed grep或fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。linux使用GNU版本的grep。它功能 更强，可以通过-G、-E、-F命令行选项来使用egrep和fgrep的功能。 grep的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到屏幕，不影响原文件内容。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 grep正则表达式元字符集（基本集） 元字符 基本功能 示例 ^ 锚定行的开始 ‘^grep’匹配所有以grep开头的行 $ 锚定行的结束 ‘grep$’匹配所有以grep结尾的行 . 匹配一个非换行符的字符 ‘gr.p’匹配gr后接一个任意字符，然后是p * 匹配零个或多个先前字符 ‘grep’匹配所有一个或多个空格后紧跟grep的行。 .一起用代表任意字符 [] 匹配一个指定范围内的字符 ‘[Gg]rep’匹配Grep和grep [^] 匹配一个不在指定范围内的字符 ‘[^A-FH-Z]rep’匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行 (..) 标记匹配字符 ‘(love)‘，love被标记为1 \&lt; 锚定单词的开始 ‘\&lt;grep’匹配包含以grep开头的单词的行 > 锚定单词的结束 ‘grep>‘匹配包含以grep结尾的单词的行 x{m} 重复字符x，m次 ‘0{5}‘匹配包含5个o的行 x{m,} 重复字符x,至少m次 ‘o{5,}‘匹配至少有5个o的行 x{m,n} 重复字符x，至少m次，不多于n次 ‘o{5,10}‘匹配5–10个o的行 \w 匹配文字和数字字符，也就是[A-Za-z0-9 ‘G\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p \W \w的反置形式，匹配一个或多个非单词字符 点号句号等 \b 单词锁定符 ‘\bgrep\b’只匹配grep 用于egrep和 grep -E的元字符扩展集+——匹配一个或多个先前的字符。如：’[a-z]+able’，匹配一个或多个小写字母后跟able的串，如loveable,enable,disable等。 ?——匹配零个或多个先前的字符。如：’gr?p’匹配gr后跟一个或没有字符，然后是p的行。 a|b|c——匹配a或b或c。如：grep|sed匹配grep或sed ()——分组符号，如：love(able|rs)ov+匹配loveable或lovers，匹配一个或多个ov。 x{m},x{m,},x{m,n}——作用同x{m},x{m,},x{m,n} POSIX字符类为了在不同国家的字符编码中保持一至，POSIX(The Portable Operating System Interface)增加了特殊的字符类，如[:alnum:]是A-Za-z0-9的另一个写法。要把它们放到[]号内才能成为正则表达式，如[A- Za-z0-9]或[[:alnum:]]。在linux下的grep除fgrep外，都支持POSIX的字符类。 POSIX字符组 说明 ASCII语言环境 [:alnum:] 文字数字字符 [a-zA-Z0-9] [:alpha:] 文字字符 [a-zA-Z] [:digit:] 数字字符 [0-9] [:graph:] 非空字符（非空格、控制字符） [\x21-\x7E] [:lower:] 小写字符 [a-z] [:cntrl:] 控制字符 [\x00-\x1F\x7F] [:print:] 非空字符（包括空格） [\x20-\x7E] [:punct:] 标点符号 [][!”#$%&amp;’()*+,./:;&lt;=&gt;?@\^_`{&#124;}~-] [:space:] 所有空白字符（新行,空格,制表符） [ \t\r\n\v\f] [:upper:] 大写字符 [A-Z] [:xdigit:] 十六进制数字（0-9，a-f，A-F） [A-Fa-f0-9] [:word:] 字母字符 [A-Za-z0-9_] [:ascii:] ASCII字符 [\x00-\x7F] [:blank:] 空格字符和制表符 [ \t] grep选项 选项 含义 选项 含义 匹配模式选择: - 输入控制: - -E, –extended-regexp 扩展正则表达式egrep -m, –max-count=NUM 匹配的最大数 -F, –fixed-strings 一个换行符分隔的字符串的集合fgrep -b, –byte-offset 打印匹配行前面打印该行所在的块号码 -G, –basic-regexp 基本正则 -n, –line-number 显示的加上匹配所在的行号 -P, –perl-regexp 调用的perl正则 –line-buffered 刷新输出每一行 -e, –regexp=PATTERN 后面根正则模式，默认无 -H, –with-filename 当搜索多个文件时，显示匹配文件名前缀 -f, –file=FILE 从文件中获得匹配模式 -h, –no-filename 当搜索多个文件时，不显示匹配文件名前缀 -i, –ignore-case 不区分大小写 –label=LABEL print LABEL as filename for standard input -w, –word-regexp 匹配整个单词 -o, –only-matching show only the part of a line matching PATTERN -x, –line-regexp 匹配整行 -q, –quiet, –silent 不显示任何东西 -z, –null-data a data line ends in 0 byte, not newline –binary-files=TYPE assume that binary files are TYPE 杂项: - -a, –text 匹配二进制的东西 -s, –no-messages 不显示错误信息 -I 不匹配二进制的东西 -v, –invert-match 显示不匹配的行 -d, –directories=ACTION 目录操作，读取，递归，跳过 -V, –version 显示版本号 -D, –devices=ACTION 设置对设备，FIFO,管道的操作，读取，跳过 –help 显示帮助信息 -R, -r, –recursive 递归调用 –mmap use memory-mapped input if possible –include=PATTERN files that match PATTERN will be examined 文件控制: - –exclude=PATTERN files that match PATTERN will be skipped -B, –before-context=NUM 打印匹配本身以及前面的几个行由NUM控制 –exclude-from=FILE files that match PATTERN in FILE will be skipped -A, –after-context=NUM 打印匹配本身以及随后的几个行由NUM控制 -L, –files-without-match 匹配多个文件时，显示不匹配的文件名 -C, –context=NUM 打印匹配本身以及随后，前面的几个行由NUM控制 -l, –files-with-matches 匹配多个文件时，显示匹配的文件名 -NUM 根-C的用法一样的 -c, –count 显示匹配了多少次 –colo[u]r[=WHEN] use markers to distinguish the matching string -Z, –null print 0 byte after FILE name -U, –binary do not strip CR characters at EOL (MSDOS) - - -u, –unix-byte-offsets report offsets as if CRs were not there (MSDOS) - - 注：部分选项对应的取值–binary-files=TYPE：TYPE is ‘binary’, ‘text’, or ‘without-match’-d, –directories=ACTION：ACTION is ‘read’, ‘recurse’, or ‘skip’-D, –devices=ACTION：ACTION is ‘read’ or ‘skip’–colour[=WHEN]：WHEN may be always&#39;,never’ or `auto’ 应用举例 匹配包含root的行 12[root@jsyd-1 shell]# grep root gtestroot:x:0:0:root:/root:/bin/bash 匹配以root或者zhang开始的行，使用反斜杠或者-E 1234567[root@jsyd-1 shell]# cat gtest | grep &apos;^\(root\|zhang\)&apos;root:x:0:0:root:/root:/bin/bashzhangy:x:1000:100:,,,:/home/zhangy:/bin/bash[root@jsyd-1 shell]# cat gtest | grep -E &apos;^(root|zhang)&apos;zhangsan:x:1000:100:,,,:/home/zhangjx:/bin/bashzhangsan:x:33:33::/srv/http:/bin/false/zhangsan 匹配以zhang开头且只包含字母的行 12[root@jsyd-1 shell]# echo zhangjiaxing | grep &apos;zhang[a-z]*$&apos;zhangsan -n参数显示匹配的行数 1234[root@jsyd-1 shell]# cat gtest | grep -n zhang7:zhangy:x:1000:100:,,,:/home/zhangy:/bin/bash 13:ba:x:1002:1002::/home/zhangy:/bin/bash 15:@zhangsan:*:1004:1004::/home/test:/bin/bash -v参数反选，不包含bin的行 12[root@jsyd-1 shell]# cat gtest | grep -nv &apos;bin&apos;16:policykit:x:102:1005:Po -c输出匹配的行数，不以bin开头的行数 12[root@jsyd-1 shell]# cat gtest | grep -cv &apos;^bin&apos;15 -i忽略大小写匹配 123[root@jsyd-1 shell]# cat gtest | grep system[root@jsyd-1 shell]# cat gtest | grep -i systemdbus:x:81:81:System message bus:/:/bin/false -w必须匹配整个单词 1234[root@jsyd-1 shell]# cat gtest | grep -w zhang[root@jsyd-1 shell]# cat gtest | grep -w zhangyzhangy:x:1000:100:,,,:/home/zhangy:/bin/bash ba:x:1002:1002::/home/zhangy:/bin/bash -x必须正行都和关键字相同 123456[root@jsyd-1 shell]# cat gtest | grep aaaabin:x:1:1:bin:/bin:/bin/false,aaa,bbbb,cccc,aaaaaaaaaa[root@jsyd-1 shell]# cat gtest | grep -x aaaaaaaa -m设定匹配的最大次数，只输出前两个匹配到的行 123[root@jsyd-1 shell]# cat gtest | grep -m 2 zhangzhangy:x:1000:100:,,,:/home/zhangy:/bin/bashba:x:1002:1002::/home/zhangy:/bin/bash -b显示匹配到时所在的字节数 1234[root@jsyd-1 shell]# cat gtest | grep -b zhang253:zhangy:x:1000:100:,,,:/home/zhangy:/bin/bash504:ba:x:1002:1002::/home/zhangy:/bin/bash586:@zhangying:*:1004:1004::/home/test:/bin/bash -H多文件匹配输出文件名，-h不输出 1234567[root@jsyd-1 shell]# grep -H zhang gtest*gtest:@zhangjiaxing:*:1004:1004::/home/test:/bin/bashgtest:zhangjx:x:1000:100:,,,:/home/zhangjx:/bin/bashgtest:zhangjx:x:33:33::/srv/http:/bin/false/zhangjxgtest1:zhangy:x:1000:100:,,,:/home/zhangy:/bin/bashgtest1:ba:x:1002:1002::/home/zhangy:/bin/bashgtest1:@zhangying:*:1004:1004::/home/test:/bin/bash -l只输出匹配到的文件名，-L相反输出没有匹配到的文件名，可通过$?查看匹配的结果 12345[root@jsyd-1 shell]# grep -l zhang gtest*gtestgtest1[root@jsyd-1 shell]# grep -L zhang gtest* -o输出匹配到的部分，而不是整行 -q不输出任何内容，可通过echo $?查看匹配的结果 -R对目录进行递归匹配，shell为文件目录 1grep zhang -R shell/ 输出匹配前后的行 123cat gtest | gerp -A 3 zhang 输出匹配后三行cat gtest1 | grep -B 3 zhang 输出匹配前三行cat gtest1 | grep -3 zhang == cat gtest1 | grep -C 3 zhang 出匹配前后个三行 显示颜色 1cat gtest1 | grep zhang --colour=always 从文件g1中获取匹配的关键字 1grep -f g1 gtest1 匹配zhang后接一个小写字符，POSIX字符使用需要两个[] 1234[root@jsyd-1 shell]# grep zhang[[:lower:]] gtest@zhangjiaxing:*:1004:1004::/home/test:/bin/bashzhangjx:x:1000:100:,,,:/home/zhangjx:/bin/bashzhangjx:x:33:33::/srv/http:/bin/false/zhangjx 匹配zhang后接3个小写字符 12[root@jsyd-1 shell]# egrep zhang[[:lower:]]&#123;3&#125; gtest@zhangjiaxing:*:1004:1004::/home/test:/bin/bash grep中模式之间的or、and、not操作 grep或(or)操作四种方式 1234$grep &apos;pattern1\|pattern2&apos; filename 使用\|$grep -E &apos;pattern1|pattern2&apos; filename 使用grep -E$egrep -E &apos;pattern1|pattern2&apos; filename 使用egrep：同上$grep -e pattern1 -e pattern2 filename 通过制定多个-e grep与(and)操作 12$grep -E &apos;pattern1.*pattern2|pattern2.*pattern1&apos; filename 使用-E选项和模式字符, 查找同时包含两个模式的行$grep &apos;pattern1&apos; filename | grep &apos;pattern2&apos; 用管道实现 grep非(not)操作 1$grep -v zhang filename 使用-v选项实现 grep与egrep、fgrep、pgrep的区别grep家族由命令grep、egrep、fgrep组成。grep命令是在文件中逐行全局查找指定的正则表达式，并且打印所有包含该表达式的行。egrep and fgrep均是grep的变体。egrep命令是扩展的grep，它能支持更多的正则表达式元字符。fgrep命令是固定的grep(fixed grep),有时也称作快速grep，它按字面解释所有字符，也就是说，正则表达式元字符不会被特殊处理，它们只匹配自己。自由软件基金会提供了grep的免费版本，称作GNU grep。Linux系统上使用的就是这种版本的grep。 grep命令:由来：grep = global regular expression print 它表示”全局查找正正则表达式(RE)并且打印结果行”。 egrep (扩展的grep):使用egrep的主要好处是它在grep提供的正则表达式元字符集的基础上添加了更多的元字符。但是egrep 不支持使用( ) 和{ }。如果使用的Linux系统，请参考GNU的grep -E命令. 元字符 功能 实例 匹配对象 + 匹配一个或多个前导字符 [a-z]+ove 匹配一个或多个小写字符后跟ove ？ 匹配0个或1个前导字符 lo?ve 匹配l后跟0个或者1个o以及字符ve a&#124;b 匹配a或b love&#124;hate 匹配love或者hate两个之一 () 字符组 love(able&#124;ly)(ov)+ 匹配loveable或lovely匹配ov的一次或多次 fgrep（固定的grep或快速的grep）fgrep命令的运行方式grep类似，但它不对任何正则表达式元字符做特殊处理。所有字符都只代表它自己的本身作为字符的意思。美元符就是美元符，全部如此。 pgrep（将模式解释为perl PE）参考grep -P 参考：http://blog.51yip.com/linux/1008.htmlhttp://man.chinaunix.net/newsoft/grep/open.htmhttp://blog.csdn.net/gaoyingju/article/details/7737651]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crtmpserver编译部署]]></title>
    <url>%2F2016%2F07%2F13%2Frtmpd-deploy%2F</url>
    <content type="text"><![CDATA[动态编译并安装到指定目录动态编译1234cd crtmpserver/builders/cmakecmake . -DCMAKE_BUILD_TYPE=Release -DCRTMPSERVER_INSTALL_PREFIX=&lt;install-dir&gt; (例如 /opt/crtmpserver)makemake install 在 /opt/crtmpserver中可以看到整理好的文件,crtmpserver在sbin文件夹中,crtmpserver.lua在etc文件夹中 复制配置表1cp ./crtmpserver/crtmpserver.lua /opt/crtmpserver/etc 复制动态库查看当前连接的动态库 1ldd sbin/crtmpserver 进入安装目录并复制额外的库 1234cd /opt/crtmpservercp /lib64/libssl.so.0.9.8e ./libcp /lib64/libcrypto.so.0.9.8e ./libcp /lib64/libdl-2.5.so ./lib 打包12cd /opttar -czvf crtmpserver.tar.gz ./crtmpserver 安装到新的机器上解压 123cd /opttar -zxvf crtmpserver.tar.gzcd crtmpserver 复制动态库到系统目录 1cp ./lib/crtmpserver/lib*.so /lib64 复制运行程序到库目录 1cp ./sbin/crtmpserver lib/crtmpserver/ 启动程序 12cd lib/crtmpserver./crtmpserver --use-implicit-console-appender ../../etc/crtmpserver.lua 静态编译DEBUG版在CMakeList.txt开始位置加入下面一句即可 1SET(ENV&#123;COMPILE_STATIC&#125; &quot;1&quot;) 编译的时调整参数为Debug 1cmake . -DCMAKE_BUILD_TYPE=Debug -DCRTMPSERVER_INSTALL_PREFIX=&lt;install-dir&gt; (例如 /opt/crtmpserver) 参考：http://blog.csdn.net/fireroll/article/details/20546299]]></content>
      <tags>
        <tag>crtmpserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[top命令详解]]></title>
    <url>%2F2016%2F07%2F13%2Flinux-top%2F</url>
    <content type="text"><![CDATA[top命令算是最直观、好用的查看服务器负载的命令了。它实时动态刷新显示服务器状态信息，且可以通过交互式命令自定义显示内容，非常强大。 在终端中输入top，回车后会显示如下内容： 1234567891011top - 17:25:46 up 44 days, 4:32, 8 users, load average: 0.14, 0.09, 0.03Tasks: 142 total, 1 running, 141 sleeping, 0 stopped, 0 zombieCpu(s): 0.3%us, 0.4%sy, 0.0%ni, 98.8%id, 0.0%wa, 0.0%hi, 0.0%si, 0.4%stMem: 6982184k total, 6632640k used, 349544k free, 276936k buffersSwap: 4194296k total, 285776k used, 3908520k free, 5641168k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 27 root 20 0 15036 1220 900 R 0.3 0.0 0:00.04 top 1 root 20 0 19232 544 352 S 0.0 0.0 0:00.71 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root RT 0 0 0 0 S 0.0 0.0 0:10.39 migration/0 系统信息统计前五行是系统整体状态的统计信息展示区域。下面分别介绍每一行中的内容： 第一行显示服务器概况 如下所示，第一行列出了服务器运行了多长时间，当前有多少个用户登录，服务器的负荷情况等，使用uptime命令能获得同样的结果 123top - 21:48:39 up 8:57, 2 users, load average: 0.36, 0.24, 0.14 / / / \当前时间 运行时长 当前登录用户数 平均负载（1分钟、5分钟、15分钟） 平均负载的值越小代表系统压力越小，越大则代表系统压力越大。通常，我们会以最后一个数值，也就是15分钟内的平均负载作为参考来评估系统的负载情况。 对于只有单核cpu的系统，1.0是该系统所能承受负荷的边界值，大于1.0则有处理需要等待。一个单核cpu的系统，平均负载的合适值是0.7以下。如果负载长期徘徊在1.0，则需要考虑马上处理了。超过1.0的负载，可能会带来非常严重的后果。当然，多核cpu的系统是在前述值的基础上乘以cpu内核的个数。如对于多核cpu的系统，有N个核则所能承受的边界值为N.0。详见：理解Linux系统负荷和理解load average 第二行是进程信息： 123Tasks: 322 total, 2 running, 320 sleeping, 0 stopped, 0 zombie / / / / / 进程总数 正运行进程数 睡眠进程数 停止进程数 僵尸进程数 第三行是CPU信息： 123456789%Cpu(s):5.0 us 用户空间CPU占比1.7 sy 内核空间CPU占比0.0 ni 用户进程空间改过优先级的进程CPU占比93.0 id 空闲CPU占比0.0 wa 待输入输出CPU占比0.3 hi 硬中断（Hardware IRQ）CPU占比0.0 si 软中断（Software Interrupts）CPU占比0.0 st - 百分比计算方式，比如一秒内有100个cpu时间片，这个cpu时间片就是cpu工作的最小单位。那么这100个cpu时间片在不同的区域和目的进行操作使用，就代表这个区域所占用的cpu时间比。也就是这里得出的cpu时间百分比。 将文件从磁盘的src位置拷贝到磁盘的dst位置。文件会从src先读取进入到内核空间，然后再读取到用户空间，然后拷贝数据到用户空间的buf上，再通过用户空间，内核空间，数据才到磁盘的dst上。所以从上面这个程序来看，cpu消耗在kernel space的时候就是sy（系统态使用的cpu百分比），cpu消耗在user space的时候就是us（用户态使用的cpu百分比）。 如果程序都没什么问题，那么是没有hi和si的，但是实际上有个硬中断和软中断的概念。比如硬中断，cpu在执行程序的时候，突然外设硬件（比如硬盘出现问题了）机器需要立刻通知cpu进行现场保存工作。这个时候会cpu会出现上下文切换。就是cpu会有一部分时间会被硬中断占用了，这个时间就是hi。相类似，si是软中断的cpu占用时间，软中断是由软件的指令方式触发的。 ni是nice的意思，nice是什么呢，每个linux进程都有个优先级，优先级高的进程有优先执行的权利，这个叫做pri。进程除了优先级外，还有个优先级的修正值。即比如你原先的优先级是20，然后修正值为-2，那么你最后的进程优先级为18。这个修正值就叫做进程的nice值。那么nice是一个进程的优先级修正值，为什么会占用cpu时间呢？ni是指用做nice加权的进程使用的用户态cpu时间比，我的理解就是一个进程的所谓修正值就意味着多分配一些cpu时间给这个进程的用户态，这个中间所多分配的cpu时间就是我们这里的ni。 wa指的是CPU等待磁盘写入完成的时间，就是说前提是要进行IO操作，在进行IO操作的时候，CPU等待时间。比如上面那个程序，最后一步，从系统空间到dst硬盘空间的时候，如果程序是阻塞的，那么这个时候cpu就要等待数据写入磁盘才能完成写操作了。所以这个时候cpu等待的时间就是wa。所以如果一台机器看到wa特别高，那么一般说明是磁盘IO出现问题，可以使用iostat等命令继续进行详细分析。 st的名字很生动，偷取。。。是专门对虚拟机来说的，一台物理是可以虚拟化出几台虚拟机的。在其中一台虚拟机上用top查看发现st不为0，就说明本来有这么多个cpu时间是安排给我这个虚拟机的，但是由于某种虚拟技术，把这个cpu时间分配给了其他的虚拟机了。这就叫做偷取。 剩下的id就是除了上面那么多cpu处理上下文以外的cpu时间片。当然在这些时间片上，cpu是空闲的。 top的所有这些cpu时间应该是相加为100%的。 第四行是内存信息： 123KiB Mem: 1010504 total, 937416 used, 73088 free, 23708 buffers / / / / 物理内存总量 使用中总量 空闲总量 缓存的内存量 内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 第五行是swap交换分区信息： 123KiB Swap: 1046524 total, 280708 used, 765816 free, 365556 cached Mem / / / / 交换区总量 使用中总量 空闲总量 缓存的内存量 程序可用内存数：Memfree + (buffers + cached) 程序已用内存数：Memused – (buffers + cached) 如果你的 swap used 数值大于 0 ，基本可以判断已经遇到内存瓶颈了，要么优化你的代码，要么加内存。 buffers是用来存储，目录里面有什么内容，权限等等。而cached直接用来记忆我们打开的文件，如果你想知道他是不是真的生效，你可以试一下，先后执行两次cat 一个比较大的日志文件，比如nginx的access.log，你就可以明显的感觉到第二次的开打的速度快很多。 因为Linux将你暂时不使用的内存作为文件和数据缓存，以提高系统性能，当你需要这些内存时，系统会自动释放（不像windows那样，即使你有很多空闲内存,他也要访问一下磁盘中的pagefiles） 进程（任务）状态监控第七行及以下显示了各进程（任务）的状态监控。各列所代表的含义如下： 123456789101112PID 进程idUSER 进程所有者PR 进程优先级NI nice值。负值表示高优先级，正值表示低优先级VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RESRES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATASHR 共享内存大小，单位kbS 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程%CPU 上次更新到现在的CPU时间占用百分比%MEM 进程使用的物理内存百分比TIME+ 进程使用的CPU时间总计，单位1/100秒COMMAND 进程名称（命令名/命令行） 交互命令1234567891011121314151617h或者? 显示帮助画面，给出一些简短的命令总结说明。k 终止一个进程。系统将提示用户输入需要终止的进程PID，以及需要发送给该进程什么样的信号。一般的终止进程可以使用15信号；如果不能正常结束那就使用信号9强制结束该进程。默认值是信号15。在安全模式中此命令被屏蔽。i 忽略闲置和僵死进程。这是一个开关式命令。q 退出程序。r 重新安排一个进程的优先级别。系统提示用户输入需要改变的进程PID以及需要设置的进程优先级值。输入一个正值将使优先级降低，反之则可以使该进程拥有更高的优先权。默认值是10。S 切换到累计模式。s 改变两次刷新之间的延迟时间。系统将提示用户输入新的时间，单位为s。如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s。需要注意的是如果设置太小的时间，很可能会引起不断刷新，从而根本来不及看清显示的情况，而且系统负载也会大大增加。f或者F 从当前显示中添加或者删除项目。o或者O 改变显示项目的顺序。l 切换显示平均负载和启动时间信息。m 切换显示内存信息。t 切换显示进程和CPU状态信息。c 切换显示命令名称和完整命令行。M 根据驻留内存大小进行排序。P 根据CPU使用百分比大小进行排序。T 根据时间/累计时间进行排序。W 将当前设置写入~/.toprc文件中。这是写top配置文件的推荐方法。 top常用命令1234567891011输入大写P，则结果按CPU占用降序排序。输入大写M，结果按内存占用降序排序。按数字 1 则可以显示所有CPU核心的负载情况。top -d 5 每隔 5 秒刷新一次，默认 1 秒top -p 4360,4358 监控指定进程top -U johndoe ‘U’为 真实/有效/保存/文件系统用户名。top -u 500 ‘u’为有效用户标识top -bn 1 显示所有进程信息，top -n 1 只显示一屏信息，供管道调用top -M show memory summary in megabytes not kilobytestop -p 25097 -n 1 -b -b 避免输出控制字符，管道调用出现乱码top翻页：top -bn1 | less 参考：http://tabalt.net/blog/linux-top/http://www.51xdn.net/czxt/Linux/20150123/11197.html]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux硬盘使用不一致问题]]></title>
    <url>%2F2016%2F07%2F13%2Fdf-du%2F</url>
    <content type="text"><![CDATA[在运维Linux服务器时，会碰到需要查看硬盘空间的情况，这时候，通常会使用df -lh命令来检查每个挂载了文件系统的硬盘的总量和已使用量，或者，可以使用du -sh [directory]命令来统计某个目录下所有文件的空间占用。 在使用df、du命令时，常常会遇到统计的硬盘使用情况不一致的问题。比如du统计根目录下文件总共大小为2G，而df判断挂载在根目录的硬盘已用空间达到了3G，20G甚至更多。发生这种情况，有以下三种原因： 预留空间为了预防紧急情况，linux ext文件系统会预留部分硬盘空间，具体预留的数值可以通过tune2fs -l [dev_name] | grep “Reserved block count”查看到（dev_name是设备名），这里预留的空间会被df计算到已用空间中，从而导致df和du统计不一致。如果需要调整预留空间大小，我们可以使用tune2fs -m [size] [dev_name]来进行调整。 幻影文件(phantom file)du是统计被文件系统记录到的每个文件的大小，然后进行累加得到的大小，这是通过文件系统获取到的。而df主要是从超级块（superblock）中读入硬盘使用信息，df获取到的是磁盘块被使用的情况。当一个文件被删除时，如果有别的进程正在使用它（占有句柄）， 这个文件将不会被du统计到，但是这个文件被占用的磁盘空间却依然会被df统计到。这些文件，以及正在使用这些文件的进程可以通过lsof | grep deleted查到。当进程停止或者被kill时，这些空间将被释放。 未统计到的文件如果上面两种情况都排除了，但是数据还是不一致，那是怎么回事？这里隐藏着一种情况：当我们将一个目录挂在到一个新的设备（硬盘）上之前，如果这个目录里面已经有数据，那么这一部分数据不会被du感知，在文件系统中也看不到这些数据，但是这些数据又是确实占用了磁盘空间，是能够被df所统计到的。这时候通过du/df统计原设备的空间使用情况，就会发现df统计到的比du要多。遇到这样的情况时，使用fuser -km [directory]杀死占用该目录的所有进程（小心操作！），然后使用umount [directory]将该目录挂载的设备卸载，这时，目录里面原来已有的数据就会出现，我们将其删除之后，再重新挂载设备（mount -t [type] [dev] [directory]）即可。]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[video嵌入测试]]></title>
    <url>%2F2016%2F07%2F12%2Fvideo-test%2F</url>
    <content type="text"><![CDATA[最后的莫西干人]]></content>
      <tags>
        <tag>VIDEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync简单介绍和使用]]></title>
    <url>%2F2016%2F07%2F11%2Frsync-use%2F</url>
    <content type="text"><![CDATA[介绍Rsync（remote synchronize）是一个远程数据同步工具。 Rsync使用“Rsync算法”来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。 Rsync可以通过rsh或ssh使用，也能以daemon模式去运行，在以daemon方式运行时Rsync server会打开一个873端口，等待客户端去连接。 主要特点： 可以镜像保存整个目录树和文件系统。 可以很容易做到保持原来文件的权限、时间、软硬链接等等。 无须特殊权限即可安装。 优化的流程，文件传输效率高。 可以使用rcp、ssh等方式来传输文件，当然也可以通过直接的socket连接。 支持匿名传输。 核心算法rsync 的核心算法 配置文件1234567891011121314151617181920212223242526272829303132333435363738# Minimal configuration file for rsync daemon# See rsync(1) and rsyncd.conf(5) man pages for help# This line is required by the /etc/init.d/rsyncd scriptpid file = /var/run/rsyncd.pid 进程写到 /var/run/rsyncd.pid 文件中port = 873address = 192.168.202.152uid = nobodygid = nobody 指定哪个用户和用户组来执行，默认是nobody，可能遇到权限问题，可以在定义要同步的目录时指定用户来解决权限的问题use chroot = no read only = yes 只读选择，不让客户端上传文件到服务器上，还有一个write only选项#limit access to private LANshosts allow=182.18.58.73 192.168.1.0/255.255.255.0 10.0.1.0/255.255.255.0hosts deny=*max connections = 5 客户端最多连接数motd file = /etc/rsyncd/rsyncd.motd 定义服务器信息的，要自己写 rsyncd.motd 文件内容，当用户登录时会看到这个信息#This will give you a separate log filelog file = /var/log/rsync.log rsync 服务器的日志#This will log every file transferred - up to 85,000+ per user, per synctransfer logging = yes 传输文件的日志log format = %t %a %m %f %bsyslog facility = local3timeout = 300[dbback]path = /opt/mysql#auth users = rootlist=yesignore errorssecrets file = /etc/rsyncd/rsyncd.secretscomment = root dataexclude = beinan/ samba/ 模块定义主要是定义服务器哪个目录要被同步，我们可以根据自己的需要，来指定多个模块。每个模块要指定认证用户，密码文件。 12345678[dbback]path = /opt/mysql 指定文件目录所在位置，这是必须指定的 auth users = root 认证用户是root ，是必须在服务器上存在的用户list=yes list 意思是把rsync 服务器上提供同步数据的目录在服务器上模块是否显示列出来。默认是yes 。如果你不想列出来，就no ；如果是no是比较安全的ignore errors 忽略IO错误secrets file = /etc/rsyncd/rsyncd.secrets 密码存在哪个文件,comment = root data 注释可以自己定义exclude = beinan/ samba/ 需要排除的子目录 rsyncd.secrets的内容格式为：用户名:密码，文件属性设为root拥有，且权限要设为600，出于安全目的，文件的属性必需是只有属主可读。 启动rsync服务器1/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf 启动前，先service iptables stop将防火墙关掉 rsync参数及实例rsync中的参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263-v, --verbose 详细模式输出-q, --quiet 精简输出模式-c, --checksum 打开校验开关，强制对文件传输进行校验-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD-r, --recursive 对子目录以递归模式处理-R, --relative 使用相对路径信息-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件)-l, --links 保留软链结-L, --copy-links 想对待常规文件一样处理软链结--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结--safe-links 忽略指向SRC路径目录树以外的链结-k, --copy-dirlinks 复制目录中符号链接-K, --keep-dirlinks 保持目录下符号链接-H, --hard-links 保留硬链结-p, --perms 保持文件权限-o, --owner 保持文件属主信息-g, --group 保持文件属组信息-D, --devices 保持设备文件信息-t, --times 保持文件时间信息-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间-n, --dry-run现实哪些文件将被传输-W, --whole-file 拷贝文件，不进行增量检测-x, --one-file-system 不要跨越文件系统边界-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节-e, --rsh=COMMAND 指定使用rsh、ssh方式进行数据同步--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件--delete 删除那些DST中SRC没有的文件，delete是指如果服务器端删除了这一文件，那么客户端也相应把文件删除，保持真正的一致--delete-excluded 同样删除接收端那些被该选项指定排除的文件--delete-after 传输结束以后再删除--ignore-errors 及时出现IO错误也进行删除--max-delete=NUM 最多删除NUM个文件--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输--force 强制删除目录，即使不为空--numeric-ids 不将数字的用户和组ID匹配为用户名和组名--timeout=TIME IP超时时间，单位为秒-I, --ignore-times 不跳过那些有同样的时间和长度的文件--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0-T --temp-dir=DIR 在DIR中创建临时文件--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份-P 等同于 --partial--progress 显示备份过程-z, --compress 对备份的文件在传输时进行压缩处理--exclude=PATTERN 指定排除不需要传输的文件模式--include=PATTERN 指定不排除而需要传输的文件模式--exclude-from=FILE 排除FILE中指定模式的文件--include-from=FILE 不排除FILE指定模式匹配的文件--version 打印版本信息--address 绑定到特定的地址--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件--port=PORT 指定其他的rsync服务端口--blocking-io 对远程shell使用阻塞IO-stats 给出某些文件的传输状态--progress 在传输时现实传输过程--log-format=formAT 指定日志文件格式--password-file=FILE 从FILE中得到密码；--password-file=/password/path/file来指定密码文件，这样就可以在脚本中使用而无需交互式地输入验证密码了，这里需要注意的是这份密码文件权限属性要设得只有属主可读。--bwlimit=KBPS 限制I/O带宽，KBytes per second-h, --help 显示帮助信息 查看服务端可用的模块列表以及注释信息 1rsync root@192.168.202.152:: 查看服务端dbback模块中的目录及文件列表 1rsync root@192.168.202.152::dbback 推送符号链接目录 1rsync -avk --bwlimit=2000 /root/meizipic rsync@192.168.202.153:/data/tmp_images 拉取含符号链接目录 1rsync -rptgoDKLvP root@192.168.202.153:/opt/data/download ./ 以服务的方法同步 1rsync -rptgoDKLvP root@192.168.202.153::dbback /opt/backcenter/]]></content>
      <tags>
        <tag>RSYNC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MYSQL部分问题及解决方法]]></title>
    <url>%2F2016%2F07%2F11%2Fmysql-tips%2F</url>
    <content type="text"><![CDATA[MYSQL出现”the table is full”的问题出现此问题的原因一般是内存表的大小超过了规定的范围，也可能是因为磁盘满导致。 针对内存表超过规定范围的解决方法一种是修改tmp_table_size参数，另外一种是修改max_heap_table_size参数。 修改Mysql的配置文件/etc/my.cnf，在[mysqld]下添加/修改： 12max_heap_table_size = 4096Mkey_buffer_size = 1000M 或者 1tmp_table_size = 256M 系统默认是16M，修改完后重启mysql MYSQL链接慢或链接不上当客户端连接数据库服务器时，服务器会进行主机名解析，并且当DNS很慢时，建立连接会很慢。因此建议在启动服务器时关闭skip_name_resolve选项。 skip-name-resolve：禁止MySQL对外部连接进行DNS解析，禁止MySQL对外部连接进行DNS解析。跳过域名解析步骤，增加远程登录的速度，原理如下： 所谓反向解析是这样的： mysql接收到连接请求后，获得的是客户端的ip，为了更好的匹配mysql.user里的权限记录（某些是用hostname定义的）。 如果mysql服务器设置了dns服务器，并且客户端ip在dns上并没有相应的hostname，那么这个过程很慢，导致连接等待。 关闭选项后唯一的局限是之后GRANT语句中只能使用IP地址了，因此在添加这项设置到一个已有系统中必须格外小心。 Too many connections如果你经常看到’Too many connections’错误，是因为max_connections的值太低了。这非常常见因为应用程序没有正确的关闭数据库连接，你需要比默认的151连接数更大的值。max_connection值被设高了(例如1000或更高)之后一个主要缺陷是当服务器运行1000个或更高的活动事务时会变的没有响应。在应用程序里使用连接池或者在MySQL里使用进程池有助于解决这一问题。]]></content>
      <tags>
        <tag>MYSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[var目录爆满问题]]></title>
    <url>%2F2016%2F07%2F11%2Flinux-clientmqueue%2F</url>
    <content type="text"><![CDATA[原因系统中有用户开启了cron，而cron中执行的程序有输出内容，输出内容会以邮件形式发给cron的用户，出现/var/spool/clientmqueue/非常大的情况通常因为没有合适的MTA发送邮件，就都积累在这里了。 解决方法在crontab里面的命令后面加上 1&gt; /dev/null 2&gt;&amp;1 将错误和输出同时抛弃掉。 清除/var/spool/clientmqueue/当/var/spool/clientmqueue/目录下文件比较多时，直接rm -f 会报错argument too long 12cd /var/spool/clientmqueuels | xargs rm -f 或者find命令去删除]]></content>
      <tags>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim注释]]></title>
    <url>%2F2016%2F07%2F11%2Fvim-commenter%2F</url>
    <content type="text"><![CDATA[简单介绍支持多种语言的补全，还支持单行注释，批量注释，等各种命令映射。 使用方法以下count代表数字，就是向下注释几行的意思，leader代表组合键的切入点，就是从这个键开始,vim捕获组合键，我这里的leader 是\逗号 1234567891011121314[count][leader]cc 注释当前行，加上数字就向下注释多少行 //fdsa[count][leader]cn 强制嵌套注[count][leader]c[空格] 注释与取消注释之间切换[count][leader]cm 前后注释 /*lfkdsa*/|NERDComInvertComment| 分别注释选中的行[count][leader]cs 性感注释 这里弄不效果 哦，可尝试一下[count][leader]cy 跟cc一样,为/* */注释方法[leader]c$ 从当前行开始注释到最后一行[leader]cA 在行的末尾添加注释[leader]ca 切换注释标记集 比较 // 变成 /* */[count][[leader]cm 为光标以下 n 行添加块注释[count][leader]cl Same cc, 并且左对齐[count][leader]cb Same cc, 并且两端对齐.[count][leader]cu 取消注释 使用cc快捷键进行注释选中的行，cu进行反注释。其中是键盘映射，默认情况下是反斜杆“”，则上述快捷键分别为：cc和cu。你可以使用命令自定义，例如命令:let mapleader=”,”将定义为”,”键。]]></content>
  </entry>
  <entry>
    <title><![CDATA[VIM标记]]></title>
    <url>%2F2016%2F07%2F11%2Fvim-mark%2F</url>
    <content type="text"><![CDATA[创建标记将光标移到某一行，使用ma命令进行标记。其中，m是标记命令，a是所做标记的名称。可以使用小写字母a-z或大写字母A-Z中的任意一个做为标记名称。小写字母的标记，仅用于当前缓冲区；而大写字母的标记，则可以跨越不同的缓冲区。 跳转标记创建标记后，可以使用’a命令，移动到指定标记行的首个非空字符。这里’是单引号。也可以使用a命令，移到所做标记时的光标位置。 使用mA命令标记后，可以通过&#39;A或A在文件间跳转。 列示标记利用:marks命令，可以列出所有标记。这其中也包括一些系统内置的特殊标记（Special marks）： 1234567. 最近编辑的位置0-9 最近使用的文件∧ 最近插入的位置&apos; 上一次跳转前的位置&quot; 上一次退出文件时的位置[ 上一次修改的开始处] 上一次修改的结尾处 删除标记如果删除了做过标记的文本行，那么所做的标记也就不存了。我们不仅可以利用标记来快速移动，而且还可以使用标记来删除文本，例如在某一行用ma做了标记，然后就可以使用d’a来删掉这一行。当然，我们也可以使用y’a命令就可以来复制这一行了。使用:delmarks a b c命令，可以删除某个或多个标记；而:delmarks! 命令，则会删除所有标记。利用:help mark-motions命令，可以查看关于标记的更多帮助信息。 123456m 创建标记&apos; 移动到标记的文本行首` 移动到标记的光标位置:marks 列示所有标记:delmarks 删除指定标记:delmarks! 删除所有标记]]></content>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Curl常用参数]]></title>
    <url>%2F2016%2F07%2F08%2Fcurl%2F</url>
    <content type="text"><![CDATA[Linux curl(Command Line URL viewer)是一个利用URL规则在命令行下工作的文件传输工具。它支持文件的上传和下载，所以是综合传输工具，但按传统，习惯称url为下载工具。 curl命令参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113-a/--append 上传文件时，附加到目标文件-A/--user-agent &lt;string&gt; 设置用户代理发送给服务器- anyauth 可以使用“任何”身份验证方法-b/--cookie &lt;name=string/file&gt; cookie字符串或文件读取位置- basic 使用HTTP基本验证-B/--use-ascii 使用ASCII /文本传输-c/--cookie-jar &lt;file&gt; 操作结束后把cookie写入到这个文件中-C/--continue-at &lt;offset&gt; 断点续转-d/--data &lt;data&gt; HTTP POST方式传送数据--data-ascii &lt;data&gt; 以ascii的方式post数据--data-binary &lt;data&gt; 以二进制的方式post数据--negotiate 使用HTTP身份验证--digest 使用数字身份验证--disable-eprt 禁止使用EPRT或LPRT--disable-epsv 禁止使用EPSV-D/--dump-header &lt;file&gt; 把header信息写入到该文件中--egd-file &lt;file&gt; 为随机数据(SSL)设置EGD socket路径--tcp-nodelay 使用TCP_NODELAY选项-e/--referer 来源网址-E/--cert &lt;cert[:passwd]&gt; 客户端证书文件和密码 (SSL)--cert-type &lt;type&gt; 证书文件类型 (DER/PEM/ENG) (SSL)--key &lt;key&gt; 私钥文件名 (SSL)--key-type &lt;type&gt; 私钥文件类型 (DER/PEM/ENG) (SSL)--pass &lt;pass&gt; 私钥密码 (SSL)--engine &lt;eng&gt; 加密引擎使用 (SSL). "--engine list" for list--cacert &lt;file&gt; CA证书 (SSL)--capath &lt;directory&gt; CA目录 (made using c_rehash) to verify peer against (SSL)--ciphers &lt;list&gt; SSL密码--compressed 要求返回是压缩的形势 (using deflate or gzip)--connect-timeout &lt;seconds&gt; 设置最大请求时间--create-dirs 建立本地目录的目录层次结构--crlf 上传是把LF转变成CRLF-f/--fail 连接失败时不显示http错误--ftp-create-dirs 如果远程目录不存在，创建远程目录--ftp-method [multicwd/nocwd/singlecwd] 控制CWD的使用--ftp-pasv 使用 PASV/EPSV 代替端口--ftp-skip-pasv-ip 使用PASV的时候,忽略该IP地址--ftp-ssl 尝试用 SSL/TLS 来进行ftp数据传输--ftp-ssl-reqd 要求用 SSL/TLS 来进行ftp数据传输-F/--form &lt;name=content&gt; 模拟http表单提交数据-form-string &lt;name=string&gt; 模拟http表单提交数据-g/--globoff 禁用网址序列和范围使用&#123;&#125;和[]-G/--get 以get的方式来发送数据-h/--help 帮助-H/--header &lt;line&gt;自定义头信息传递给服务器--ignore-content-length 忽略的HTTP头信息的长度-i/--include 输出时包括protocol头信息-I/--head 只显示文档信息从文件中读取-j/--junk-session-cookies忽略会话Cookie- 界面&lt;interface&gt;指定网络接口/地址使用- krb4 &lt;级别&gt;启用与指定的安全级别krb4-j/--junk-session-cookies 读取文件进忽略session cookie--interface &lt;interface&gt; 使用指定网络接口/地址--krb4 &lt;level&gt; 使用指定安全级别的krb4-k/--insecure 允许不使用证书到SSL站点-K/--config 指定的配置文件读取-l/--list-only 列出ftp目录下的文件名称--limit-rate &lt;rate&gt; 设置传输速度--local-port&lt;NUM&gt; 强制使用本地端口号-m/--max-time &lt;seconds&gt; 设置最大传输时间--max-redirs &lt;num&gt; 设置最大读取的目录数--max-filesize &lt;bytes&gt; 设置最大下载的文件总量-M/--manual 显示全手动-n/--netrc 从netrc文件中读取用户名和密码--netrc-optional 使用 .netrc 或者 URL来覆盖-n--ntlm 使用 HTTP NTLM 身份验证-N/--no-buffer 禁用缓冲输出-o/--output 把输出写到该文件中-O/--remote-name 把输出写到该文件中，保留远程文件的文件名-p/--proxytunnel 使用HTTP代理--proxy-anyauth 选择任一代理身份验证方法--proxy-basic 在代理上使用基本身份验证--proxy-digest 在代理上使用数字身份验证--proxy-ntlm 在代理上使用ntlm身份验证-P/--ftp-port &lt;address&gt; 使用端口地址，而不是使用PASV-Q/--quote &lt;cmd&gt;文件传输前，发送命令到服务器-r/--range &lt;range&gt;检索来自HTTP/1.1或FTP服务器字节范围--range-file 读取（SSL）的随机文件-R/--remote-time 在本地生成文件时，保留远程文件时间--retry &lt;num&gt; 传输出现问题时，重试的次数--retry-delay &lt;seconds&gt; 传输出现问题时，设置重试间隔时间--retry-max-time &lt;seconds&gt; 传输出现问题时，设置最大重试时间-s/--silent静音模式。不输出任何东西-S/--show-error 显示错误--socks4 &lt;host[:port]&gt; 用socks4代理给定主机和端口--socks5 &lt;host[:port]&gt; 用socks5代理给定主机和端口--stderr &lt;file&gt;-t/--telnet-option &lt;OPT=val&gt; Telnet选项设置--trace &lt;file&gt; 对指定文件进行debug--trace-ascii &lt;file&gt; Like --跟踪但没有hex输出--trace-time 跟踪/详细输出时，添加时间戳-T/--upload-file &lt;file&gt; 上传文件--url &lt;URL&gt; Spet URL to work with-u/--user &lt;user[:password]&gt;设置服务器的用户和密码-U/--proxy-user &lt;user[:password]&gt;设置代理用户名和密码-v/--verbose-V/--version 显示版本信息-w/--write-out [format]什么输出完成后-x/--proxy &lt;host[:port]&gt;在给定的端口上使用HTTP代理-X/--request &lt;command&gt;指定什么命令-y/--speed-time 放弃限速所要的时间。默认为30-Y/--speed-limit 停止传输速度的限制，速度时间'秒-z/--time-cond 传送时间设置-0/--http1.0 使用HTTP 1.0-1/--tlsv1 使用TLSv1（SSL）-2/--sslv2 使用SSLv2的（SSL）-3/--sslv3 使用的SSLv3（SSL）--3p-quote like -Q for the source URL for 3rd party transfer--3p-url 使用url，进行第三方传送--3p-user 使用用户名和密码，进行第三方传送-4/--ipv4 使用IP4-6/--ipv6 使用IP6-#/--progress-bar 用进度条显示当前的传送状态 常用curl实例 抓取网页 1curl -o home.html http://www.baofengcloud.com 用-O（大写的），后面的url要具体到某个文件，不然抓不下来。我们还可以用正则来抓取东西 1curl -O http://vod.baofengcloud.com/player/cloud.swf 自动跳转 使用-L参数，curl就会跳转到新的网址 1curl -o home.html http://www.baofengcloud.com 断点续传，-C(大写的) 1curl -C -O http://www.baofengcloud.com 显示抓取错误，下面这个例子，很清楚的表明了 1curl -f http://www.baofengcloud.com 伪造来源地址，有的网站会判断，请求来源地址 1curl -e http://localhost http://vod.baofengcloud.com/player/cloud.swf 代理 1curl -x 24.10.28.84:32779 -o player.swf http://vod.baofengcloud.com/player/cloud.swf 分段下载 1curl -r 0-100 -o player.swf http://vod.baofengcloud.com/player/cloud.swf 不会显示下载进度信息 1curl -s -o player.swf http://vod.baofengcloud.com/player/cloud.swf 显示下载进度条 1curl -# -o player.swf http://vod.baofengcloud.com/player/cloud.swf 通过ftp下载文件 1curl -u 用户名:密码 -O http:// 或者 1curl -O ftp://用户名:密码@ip:port/ 通过ftp上传 1curl -T test.sql ftp://用户名:密码@ip:port/ 显示头信息-i参数可以显示http response的头信息 显示通信过程-v参数可以显示一次http通信的整个过程，包括端口连接和http request头信息。 curl默认的HTTP动词是GET，使用-X参数可以支持其他动词 1curl -X POST www.example.com 发送表单信息发送表单信息有GET和POST两种方法。GET方法相对简单，只要把数据附在网址后面就行。 1 curl example.com/form.cgi?data=xxx POST方法必须把数据和网址分开，curl就要用到–data参数。 1 curl --data "data=xxx" example.com/form.cgi 如果你的数据没有经过表单编码，还可以让curl为你编码，参数是–data-urlencode。 1 curl --data-urlencode "date=April 1" example.com/form.cgi 参考：http://www.ruanyifeng.com/blog/2011/09/curl.html]]></content>
      <tags>
        <tag>CURL</tag>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VIM插件安装使用]]></title>
    <url>%2F2016%2F07%2F08%2Fvim-install-and-use%2F</url>
    <content type="text"><![CDATA[VIM是什么。代码开发的人都应该知道，特别是在linux下搞开发的人。这是一个强大的文本编辑工具，可以与任何windows下的IDE相媲美，甚至功能更加强大。而且所有的功能都能自己订制，只要你需要。 为什么要使用VIM首先，VIM是一个强大的编辑工具，他能明显的提高你的代码编写效率，初期上手可能比较困难，但是工欲善其事、必先利其器，花一些时间来安装、配置和学习VIＭ是完全值得的。 其次，用VIM进行代码开发，能够保证代码开发的质量。无论是代码格式上还是对代码整体功能上讲，用VIM开发绝对是有百利而无一害。 再次，作为一个搞Linux服务器开发的人，有了VIM才更显的专业。VIM是程序猿第一装逼利器。 另外，还有很多优点，比如在安装配置的整个过程中，你也能学习一下国外的优秀软件是这么设计的，同时也能证明自己的动手能力和解决问题的能力。 基本概念： Vim存在多个配置文件vimrc，比如/etc/vimrc，此文件影响整个系统的Vim。还有~/.vimrc，此文件只影响本用户的Vim。而且~/.vimrc文件中的配置会覆盖/etc/vimrc中的配置。这里我们只修改~/.vimrc文件，此文件需要再用户目录下手动创建。 Vim的插件（plugin）安装在Vim的runtimepath目录下，你可以在Vim命令行下运行”set rtp“命令查看。这里我们选择安装在~/.vim目录，没有就创建一个。 Vim的插件安装方式分为两种一种是编译安装，一种直接解压即可。Ctags和Cscope（如果没有）需要编译安装，其余直接解压即可。 插件下载地址均为Vim官网，如无法打开请翻墙。 解压安装的插件在解压完成后需要进入~/.vim/doc目录，在Vim下运行”helptags .”命令。此步骤是将doc下的帮助文档加入到Vim的帮助主题中，这样我们就可以通过在Vim中查看帮助。 相关插件及功能简介 Taglist : 用来提供单个源代码文件的函数列表之类的功能; NERDTree : 提供展示文件/目录列表的功能; Cscope : 比ctags更加强大的功能，举个例子，ctags只能分析出这个函数在哪里被定义，而cscope除了这一点之外，还能分析出这个函数再哪里被调用。 Ctags : 实现了c、c++、java、c#等语言的智能分析，并生成tags文件，后面所有的包括函数列表显示，变量定义跳转，自动补全等，都要依赖于他。有了tags文件后，只需要在变量上按下 CTRL + ]键，就可以自动跳到变量定义的位置 NERD_commenter : 提供快速注释/反注释代码块的功能 Omnicppcomplete : 提供C++代码的自动补全功能 SuperTab : 使Tab快捷键具有更快捷的上下文提示功能 Winmanager : 提供多文件同时编辑功能 MiniBufExplorer : 将这NERDTree界面和Taglist界面整合起来，使Vim更像VS VIM7.3 : 7.0及以下版本autocomplpop无法使用 AutoComplPop : 让自动完成的选项在你输入时就自动出现，并且随着你输入的内容不断过滤选项。 a.vim : .cpp和.h文件快速切换,直接可以:A，打开.cpp和.h对应的文件，:AV，打开.cpp和.h对应的文件，并且分屏. SnipMate : 代码块的自动补全. c.vim : 文件添加注释和说明。 bufexplorer ： 打开历史文件列表以达到快速切换文件的目的。命令模式下输入\be或:BufExplorer进入Buffer列表。 mark.vim : 提供高亮功能，安装方法vim mark.vba.gz :so % vim安装与卸载vim安装有两个方法：yum安装和编译安装 方法一：yum安装vim 1$ yum install vim* -y 主要包括vim-common.i386、vim-enhanced.i386、vim-minimal.i386这三个包。此方法为默认安装的版本，centos5.9为7.0的VIM，建议采用第二种方法手动下载安装。 方法二：下载、解包12$ wget ftp://ftp.vim.org/pub/vim/unix/vim-7.3.tar.bz2$ tar jxvf vim-7.3.tar.bz2 编译安装1234$ cd vim73/src$ ./configure --enable-multibyte \--with-features=huge \--disable-selinux$ make$ sudo make install 测试1$ vim --version 卸载1$ yum remove vim vim-enhanced vim-common vim-minimal 安装VIM中文帮助文档：1234$ wget http://sourceforge.net/projects/vimcdoc/files/vimcdoc/1.8.0/vimcdoc-1.8.0.tar.gz$ tar zxvf vimcdoc-1.8.0.tar.gz$ cd vimcdoc-1.8.0/$ ./vimcdoc.sh -I 插件安装安装使用CtagCtags工具是用来遍历源代码文件生成tags文件，这些tags文件能被编辑器或其它工具用来快速查找定位源代码中的符号（tag/symbol），如变量名，函数名等。比如，tags文件就是Taglist和OmniCppComplete工作的基础。 解压缩生成源代码目录，然后进入源代码根目录执行./configure，然后执行make,编译成功后执行make install。 到此，Ctags已安装成功。使用Ctags的也很简单。 进入我们的项目代码根目录，执行以下命令：1$ ctags -R --c++-kinds=+p --fields=+iaS --extra=+q . 有两组快捷键是最常用的。12$ Ctrl-] 跳转到光标所在符号的定义。$ Ctrl-t 回到上次跳转前的位置。 安装使用CscopeCscope提供交互式查询语言符号功能，如查询哪些地方使用某个变量或调用某个函数。Cscope已经是Vim的标准特性，默认都有支持，官方网址为http://cscope.sourceforge.net/。 在Vim下运行version查看Vim支持哪些特性，前面有前缀符号+的为支持。如果支持Cscope，则直接进入2），否则下载Cscope源代码包编译安装。步骤同Ctags安装。 确定Vim已支持Cscope后，将文件cscope_maps.vim下载到~/.vim/plugin目录。 到这里，我们就可以开始使用Cscope了。使用Cscope需要生成cscope数据库文件。进入项目代码根目录运行命令：1$ cscope -Rbq -f xxx.out 命令运行后会生成xxx.out文件，即cscope数据库文件。更多用法参考man cscope文档。进入项目代码根目录，在Vim下运行命令，或者添加到文件.vimrc：1$ cs add xxx.out 此命令将cscope数据库载入Vim。如出现重复装载索引问题，请按此操作即可。 安装使用OmniCppCompleteOmniCppComplete主要提供输入时实时提供类或结构体的属性或方法的提示和补全。跟Talist一样，OmniCppComplete也是一个Vim插件，同样依赖与Ctags工具生成的tags文件。安装步骤跟Taglist类似。与VS的VA功能类似，但是只会不全类或者结构体等。不具有普通变量的补全提示功能。 安装使用SuperTabSuperTab使Tab快捷键具有更快捷的上下文提示功能。跟OmniCppComplete一样，SuperTab也是一个Vim插件。 这个安装包跟先前的几个Vim插件不同，它是一个vba文件，即Vimball格式的安装包，这种格式安装包提供傻瓜式的安装插件的方法。 用Vim打开.vba安装包文件。 在Vim命令行下运行命令“UseVimball ~/.vim”。此命令将安装包解压缩到~/.vim目录。Vimball安装方式的便利之处在于你可以在任何目录打开.vba包安装，而不用切换到安装目的地目录。而且不用运行helptags命令安装帮助文档。 其余插件解压安装即可。]]></content>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NERDTree快捷键]]></title>
    <url>%2F2016%2F07%2F08%2Fvim-nerdtree%2F</url>
    <content type="text"><![CDATA[文件操作12345678910&quot; ============================&quot; File node mappings~double-click, &lt;CR&gt;, o: open in prev window 在已有窗口中打开文件、目录或书签，并跳到该窗口go: preview 在已有窗口 中打开文件、目录或书签，但不跳到该窗口t: open in new tab 在新Tab中打开选中文件/书签，并跳到新TabT: open in new tab silently 在新Tab中打开选中文件/书签，但不跳到新Tabmiddle-click, i: open split split一个新窗口打开选中文件，并跳到该窗口gi: preview split split一个新窗口打开选中文件，但不跳到该窗口s: open vsplit vsp一个新窗口打开选中文件，并跳到该窗口gs: preview vsplit vsp一个新 窗口打开选中文件，但不跳到该窗口 目录操作1234567&quot; ----------------------------&quot; Directory node mappings~double-click, o: open &amp; close node 开关闭节点O: recursively open node 递归打开选中 结点下的所有目录x: close parent of node 合拢选中结点的父目录X: close all child nodes of current node recursively 递归合拢选中结点下的所有目录middle-click, e: explore selected dir 浏览当前目录下 书签操作123456&quot; ----------------------------&quot; Bookmark table mappings~double-click,o: open bookmarkt: open in new tabT: open in new tab silentlyD: delete bookmark 节点跳转12345678&quot; ----------------------------&quot; Tree navigation mappings~P: go to root 跳到根结点p: go to parent 跳到父结点K: go to first child 跳到当前目录下同级的第一个结点J: go to last child 跳到当前目录下同级的最后一个结点&lt;C-j&gt;: go to next sibling 跳到当前目录下同级的前一个结点&lt;C-k&gt;: go to prev sibling 跳到当前目录下同级的后一个结点 文件系统操作123456789&quot; ----------------------------&quot; Filesystem mappings~C: change tree root to the selected dir 将选中目录或选中文件的父目录设为根结点u: move tree root up a dir 将当前根结点的父目录设为根目录，并变成合拢原根结点U: move tree root up a dir but leave old root open 将当前根结点的父目录设为根目录，但保持展开原根结点r: refresh cursor dir 递归刷新选中目录R: refresh current root 递归刷新根结点m: Show menu 显示文件系统菜单cd:change the CWD to the selected dir 将CWD设为选中目录 切换显示123456&quot; ----------------------------&quot; Tree filtering mappings~I: hidden files (off)f: file filters (on)F: files (on)B: bookmarks (off) 其他操作12345&quot; ----------------------------&quot; Other mappings~q: Close the NERDTree window 关闭NerdTree窗口A: Zoom (maximize-minimize) the NERDTree window?: toggle help 书签命令12345678&quot; ----------------------------&quot; Bookmark commands:Bookmark &lt;name&gt; 将选中结点添加到书签列表中，并命名为name（书签名不可包含空格）；如与现有书签重名，则覆盖现有书签:BookmarkToRoot &lt;name&gt; 以指定目录书签或文件书签的父目录作为根结点显示NerdTree:RevealBookmark &lt;name&gt; 如果指定书签已经存在于当前目录树下，打开它的上层结点并选中该书签 :OpenBookmark &lt;name&gt; 打开指定的文件。（参数必须是文件书签）如果该文件在当前的目录树下，则打开它的上层结点并选中该书签:ClearBookmarks [&lt;names&gt;] 清除指定书签；如未指定参数，则清除所有书签:ClearAllBookmarks 清除所有书签 切换标签页123456:tabnew [++opt选项] ［＋cmd］ 文件 建立对指定文件新的tab:tabc 关闭当前的 tab:tabo 关闭所有其他的 tab:tabs 查看所有打开的 tab:tabp 前一个 tab:tabn 后一个 tab 标准模式下： gt , gT 可以直接在tab之间切换。 vim相关设置12345678&quot;setting for NERD tree=================================================nmap &lt;silent&gt; &lt;leader&gt;tt :NERDTreeToggle&lt;cr&gt;let NERDTreeWinSize=30autocmd VimEnter * NERDTree 在vim 启动的时候默认开启 NERDTree（autocmd 可以缩写为 au）autocmd bufenter * if (winnr(&quot;$&quot;) == 1 &amp;&amp; exists(&quot;b:NERDTreeType&quot;) &amp;&amp; b:NERDTreeType == &quot;primary&quot;) | q | endif if the last window is NERDTree, then close Vimlet NERDTreeWinPos=&quot;right&quot; 将 NERDTree 的窗口设置在 vim 窗口的右侧（默认为左侧）let NERDChristmasTree=1 让树更好看,真心没发现啊let NERDTreeDirArrows=0]]></content>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim分屏操作]]></title>
    <url>%2F2016%2F07%2F08%2Fvim-split%2F</url>
    <content type="text"><![CDATA[打开分屏vim中打开分屏 12345678910111213:sp filename 水平分屏:split filename:vsp filename 垂直分屏:vsplit filename:sview filename 只读分屏打开文件:split 水平打开当前文件分屏ctrl+W s 水平打开当前文件分屏:vspilt 垂直打开当前文件分屏ctrl+W v 垂直打开当前文件分屏:30split 打开一个高度为30的窗口:30vsplit 打开一个宽带为30的窗口 shell中打开分屏12vim -On file1, file2 ... 垂直分屏vim -on file1, file2 ... 水平分屏 其中n为分几个屏 切换分屏1234Ctrl+w w 后一个Ctrl+w p 前一个ctrl+w h,j,k,l 上下左右操作ctrl+w 上下左右键头 关闭分屏12345ctrl+W c 关闭当前窗口ctrl+w q 关闭当前窗口，若只有一个分屏且退出vimctrl+w o 关闭其他窗口:only 仅保留当前分屏:hide 关闭当前分屏 调整分屏的大小123456789101112131415161718ctrl+w = 所有分屏都统一高度ctrl+w + 增加高度:res[ize] +N 使得当前窗口高度加N(默认值是1), 如果在 &apos;vertical&apos; 之后使用，则使得宽度加Nctrl+w - 减少高度:res[ize] -N 使得当前窗口高度减N(默认值是1), 如果在 &apos;vertical&apos; 之后使用，则使得宽度减N10 ctrl+w + 增加10行高度10 ctrl+w - 减少10行高度:res[ize] [N]CTRL-W _ 设置当前窗口的高度为 N (默认值为最大可能高度)。:vertical res[ize] [N]CTRL-W | 设置当前窗口的宽度为 N (默认值为最大可能宽度)。CTRL-W &lt; 使得当前窗口宽度减 N (默认值是 1)CTRL-W &gt; 使得当前窗口宽度加 N (默认值是 1) 移动分屏1ctrl+W H,J,K,L]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim插件cscope]]></title>
    <url>%2F2016%2F07%2F07%2Fvim-cscope%2F</url>
    <content type="text"><![CDATA[cscope生成数据库参数1cscope –Rbq 这个命令会生成三个文件：cscope.out, cscope.in.out, cscope.po.out,其中cscope.out是基本的符号索引，后两个文件是使用”-q”选项生成的，可以加快cscope的索引速度。 在缺省情况下，cscope在生成数据库后就会进入它自己的查询界面，我们一般不用这个界面，所以使用了“-b”选项。如果你已经进入了这个界面，按CTRL-D退出。 Cscope在生成数据库中，在你的项目目录中未找到的头文件，会自动到/usr/include目录中查找。如果你想阻止它这样做，使用“-k”选项。 Cscope缺省只解析C文件(.c和.h)、lex文件(.l)和yacc文件(.y)，虽然它也可以支持C++以及Java，但它在扫描目录时会跳过C++及Java后缀的文件。如果你希望cscope解析C++或Java文件，需要把这些文件的名字和路径保存在一个名为cscope.files的文件。当cscope发现在当前目录中存在cscope.files时，就会为cscope.files中列出的所有文件生成索引数据库。 使用“-R”参数，因为在cscope.files中已经包含了子目录中的文件。 Cscope只在第一次解析时扫描全部文件，以后再调用cscope，它只扫描那些改动过的文件，这大大提高了cscope生成索引的速度。 123456789-R: 在生成索引文件时，搜索子目录树中的代码-b: 只生成索引文件，不进入cscope的查询界面-q: 生成cscope.in.out和cscope.po.out文件，加快cscope的索引速度-k: 在生成索引文件时，不搜索/usr/include目录-i: 如果保存文件列表的文件名不是cscope.files时，需要加此选项告诉cscope到哪儿去找源文件列表。可以使用“-”，表示由标准输入获得文件列表。-I dir: 在-I选项指出的目录中查找头文件-u: 扫描所有文件，重新生成交叉索引文件-C: 在搜索时忽略大小写-P path: 在以相对路径表示的文件前加上的path，这样，你不用切换到你数据库文件所在的目录也可以使用它了。 使用参数12345678s: 查找C语言符号，即查找函数名、宏、枚举值等出现的地方g: 查找函数、宏、枚举等定义的位置，类似ctags所提供的功能d: 查找本函数调用的函数c: 查找调用本函数的函数t: 查找指定的字符串e: 查找egrep模式，相当于egrep功能，但查找速度快多了f: 查找并打开文件，类似vim的find功能i: 查找包含本文件的文件 vim添加快捷键12345678nmap &lt;C-\&gt;s :cs find s &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;g :cs find g &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;c :cs find c &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;t :cs find t &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;e :cs find e &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;f :cs find f &lt;C-R&gt;=expand(&quot;&lt;cfile&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;i :cs find i ^&lt;C-R&gt;=expand(&quot;&lt;cfile&gt;&quot;)&lt;CR&gt;$&lt;CR&gt;nmap &lt;C-\&gt;d :cs find d &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;]]></content>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim命令收集]]></title>
    <url>%2F2016%2F07%2F06%2Fvim-cmd%2F</url>
    <content type="text"><![CDATA[光标控制命令 1234567891011121314151617181920212223242526h或^h 向左移一个字符j或^j或^n 向下移一行k或^p 向上移一行l或空格 向右移一个字符G 移到文件的最后一行nG 移到文件的第n行w 移到下一个字的开头W 移到下一个字的开头，忽略标点符号b 移到前一个字的开头B 移到前一个字的开头，忽略标点符号L 移到当前屏幕的最后一行M 移到当前屏幕的中间一行H 移到当前屏幕的第一行e 移到下一个字的结尾E 移到下一个字的结尾，忽略标点符号( 移到句子的开头) 移到句子的结尾&#123; 移到段落的开头&#125; 移到下一个段落的开头0或| 移到当前行的第一列n| 移到当前行的第n列^ 移到当前行的第一个非空字符$ 移到当前行的最后一个字符+或return 移到下一行的第一个字符- 移到前一行的第一个非空字符gd vim中快速定位到当前光标所在变量或函数处: 在vi中添加文本12345678910a 在光标后插入文本A 在当前行尾插入文本i 在光标前插入文本I 在当前行前插入文本o 在当前行的下边插入新行O 在当前行的上边插入新行:r file 读入文件file内容，并插在当前行后:nr file 读入文件file内容，并插在第n行后escape 回到命令模式^v char 插入时忽略char的指定意义，这是为了插入特殊字符 在vi中删除文本123456789101112131415161718x 删除光标处的字符，可以在x前加上需要删除的字符数目nx 从当前光标处往后删除n个字符X 删除光标前的字符，可以在X前加上需要删除的字符数目nX 从当前光标处往前删除n个字符dw 删至下一个字的开头ndw 从当前光标处往后删除n个字dG 删除行，直到文件结束dd 删除整行ndd 从当前行开始往后删除db 删除光标前面的字ndb 从当前行开始往前删除n字:n,md 从第m行开始往前删除n行d或d$ 从光标处删除到行尾dcursor_command 删除至光标命令处，如dG将从当产胆行删除至文件的末尾^h或backspace 插入时，删除前面的字符^w 插入时，删除前面的字cw 找到字符，然后cw，就删除了，然后修改为想要的字符串, 然后n，进入下一个字符，然后&quot;.&quot;就可以重复上面的操作。cnw 是删除n个字符，并进入插入模式。 修改vi文本12345678rchar 用char替换当前字符R text escape 用text替换当前字符直到换下Esc键stext escape 用text代替当前字符S或cctext escape 用text代替整行cwtext escape 将当前字改为textCtext escape 将当前行余下的改为textcG escape 修改至文件的末尾ccursor_cmd text escape 从当前位置处到光标命令位置处都改为text 在vi中查找与替换12345678910111213141516171819# 光标下反向搜索关键词 (search the word under cursor backward)* 光标下正向搜索关键词 (search the word under cursor forward)/text 在文件中向前查找text?text 在文件中向后查找textn 在同一方向重复查找N 在相反方向重复查找ftext 在当前行向前查找textFtext 在当前行向后查找textttext 在当前行向前查找text，并将光标定位在text的第一个字符Ttext 在当前行向后查找text，并将光标定位在text的第一个字符:set ic 查找时忽略大小写:set noic 查找时对大小写敏感:s/oldtext/newtext 用newtext替换oldtext:m,ns/oldtext/newtext 在m行通过n，用newtext替换oldtext&amp; 重复最后的:s命令:g/text1/s/text2/text3 查找包含text1的行，用text3替换text2:g/text/command 在所有包含text的行运行command所表示的命令:v/text/command 在所有不包含text的行运行command所表示的命令% 查看与当前符号匹配的另外一半符号// &quot;&quot; 或者&apos;&apos;或者()或者[ ]或者&#123;&#125;等匹配出现的符号 在vi中复制文本1234567891011yy 将当前行的内容放入临时缓冲区nyy 将n行的内容放入临时缓冲区p 将临时缓冲区中的文本放入光标后P 将临时缓冲区中的文本放入光标前dsfsd &quot;(a-z)nyy 复制n行放入名字为圆括号内的可命名缓冲区，省略n表示当前行&quot;(a-z)ndd 删除n行放入名字为圆括号内的可命名缓冲区，省略n表示当前行&quot;(a-z)p 将名字为圆括号的可命名缓冲区的内容放入当前行后&quot;(a-z)P 将名字为圆括号的可命名缓冲区的内容放入当前行前D 剪切从光标位置到行尾到剪贴板。Y 拷贝当前行。C 和 D 类似，最后进入插入模式。 在vi中撤消与重复12345678910111213141516u 撤消最后一次修改U 撤消当前行的所有修改. 重复最后一次修改, 以相反的方向重复前面的f、F、t或T查找命令; 重复前面的f、F、t或T查找命令&quot;np 取回最后第n次的删除(缓冲区中存有一定次数的删除内容，一般为9)n 重复前面的/或?查找命令N 以相反方向重复前面的/或?命令ctrl+r 撤销:undo 2 undo 到结构的2层 (undo to tree 2):undolist 显示所有的undo列表 (show undo list):later 45s 显示 45 秒之后的文本状态:earlier 10s undo到10秒前的编辑 (undo to 10 seconds ago):earlier 10h undo到10小时前的编辑 (back to 10 hours ago):earlier 1m undo到1分钟前 (back to 1 minutes ago) vim的undo是树结构的，你可以回到这个结构中的任何地方 保存文本和退出vi12345:w 保存文件但不退出vi:w file 将修改保存在file中但不退出vi:wq或ZZ或:x 保存文件并退出vi:q! 不保存文件，退出vi:e! 放弃所有修改，从上次保存文件开始再编辑 vi中的选项12345678:set all 打印所有选项:set nooption 关闭option选项:set nu 每行前打印行号:set showmode 显示是输入模式还是替换模式:set noic 查找时忽略大小写:set list 显示制表符(^I)和行尾符号:set ts=8 为文本输入设置tab stops:set window=n 设置文本窗口显示n行 vi的状态1234:.= 打印当前行的行号:= 打印文件中的行数^g 显示文件名、当前的行号、文件的总行数和文件位置的百分比:l 使用字母&quot;l&quot;来显示许多的特殊字符，如制表符和换行符 在文本中定位段落和放置标记12345&#123; 在第一列插入&#123;来定义一个段落[[ 回到段落的开头处]] 向前移到下一个段落的开头处m(a-z) 用一个字母来标记当前位置，如用mz表示标记z&apos;(a-z) 将光标移动到指定的标记，如用&apos;z表示移动到z 在vi中连接行12J 将下一行连接到当前行的末尾nJ 连接后面n行 光标放置与屏幕调整123456789101112131415161718192021H 将光标移动到屏幕的顶行nH 将光标移动到屏幕顶行下的第n行M 将光标移动到屏幕的中间L 将光标移动到屏幕的底行nL 将光标移动到屏幕底行上的第n行ctrl+e 将屏幕上滚一行ctrl+y 将屏幕下滚一行ctrl+u 将屏幕上滚半页ctrl+d 将屏幕下滚半页ctrl+b 将屏幕上滚一页ctrl+f 将屏幕下滚一页ctrl+l 重绘屏幕z 将当前行置为屏幕的顶行nz 将当前行下的第n行置为屏幕的顶行z. 将当前行置为屏幕的中央nz. 将当前行上的第n行置为屏幕的中央z- 将当前行置为屏幕的底行nz- 将当前行上的第n行置为屏幕的底行zz 让光标所杂的行居屏幕中央zt 让光标所杂的行居屏幕最上一行 t=topzb 让光标所杂的行居屏幕最下一行 b=bottom vi中的shell转义命令12345678:!command 执行shell的command命令，如:!ls:!! 执行前一个shell命令:r!command 读取command命令的输入并插入，如:r!ls会先执行ls，然后读入内容:3r!date -u 将外部命令date -u的结果输入在vim的第三行中:w!command 将当前已编辑文件作为command命令的标准输入并执行command命令，如:w!grep all:cd directory 将当前工作目录更改为directory所表示的目录:sh 将启动一个子shell，使用^d(ctrl+d)返回vi:so file 在shell程序file中读入和执行命令 vi中的宏与缩写避免使用控制键和符号，不要使用字符K、V、g、q、v、*、=和功能键 123456:map key command_seq 定义一个键来运行command_seq，如:map e ea，无论什么时候都可以e移到一个字的末尾来追加文本:map 在状态行显示所有已定义的宏:umap key 删除该键的宏:ab string1 string2 定义一个缩写，使得当插入string1时，用string2替换string1。当要插入文本时，键入string1然后按Esc键，系统就插入了string2:ab 显示所有缩写:una string 取消string的缩写 在vi中缩进文本123456ctrl+i或tab 插入文本时，插入移动的宽度，移动宽度是事先定义好的:set ai 打开自动缩进:set sw=n 将移动宽度设置为n个字符n&lt;&lt; 使n行都向左移动一个宽度n&gt;&gt; 使n行都向右移动一个宽度，例如3&gt;&gt;就将接下来的三行每行都向右移动一个移动宽度gg+=+G 先gg跳转到文件开始位置，再输入=，再输入G，vim将自动进行C和C++源码的对齐操作 格式转换(format)dos/windows跟unix/linux对于文件的结束是不一样的。vim可以直接设定/更改格式用指令:set fileformats=unix,dos 可以改变文件的格式 (change format) 123:set ff=unix 设定文件成unix格式 (set file in unix format):set ff=dos 设定文件成dos格式 (set file in dos format):set ff? 检查当前文件格式 (check the format of current file)]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim配置文件]]></title>
    <url>%2F2016%2F07%2F06%2Fvim-vimrc%2F</url>
    <content type="text"><![CDATA[vimrc完整配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270&quot;Use Vim settings, rather than Vi settings (much better!).&quot;This must be first, because it changes other options as a side effect.&quot;Without no where use Vi settings, but when load vim settting it will be wrong.&quot; Or set nocp.set nocompatibleset helplang=cn &quot; set help doc languageset incsearch &quot; search when input not completeset history=1000 &quot; history文件中需要记录的行数if has(&quot;vms&quot;) set nobackup &quot; do not keep a backup file, use versions insteadelse set backup &quot; keep a backup fileendif&quot; ===================================================================&quot;set paste &quot; must not add, or auto complete will not use&quot; ===================================================================syntax enablesyntax on &quot; 让语法高亮显示filetype on &quot; 侦测文件类型filetype plugin on &quot; 允许插件filetype indent on &quot; 为特定文件类型载入相关缩进文件filetype plugin indent on &quot; 启动自动补全set iskeyword+=_,$,@,%,#,- &quot; 带有如下符号的单词不要被换行分割&quot; ===================================================================&quot; set shortmess=atI &quot; 启动的时候不显示那个援助索马里儿童的提示set autoread &quot; Set to auto read when a file is changed from the outsidennoremap &lt;F2&gt; :set nonumber!&lt;CR&gt;:set foldcolumn=0&lt;CR&gt; &quot; 为方便复制，用&lt;F2&gt;开启/关闭行号显示nnoremap &lt;F3&gt; :set nolist!&lt;CR&gt; &quot; 为方便复制，用&lt;F3&gt;开启/关闭set listnnoremap &lt;F4&gt; :set nohls!&lt;CR&gt; &quot; 为方便复制，用&lt;F4&gt;取消高亮map &lt;leader&gt;e :e! ~/.vimrc&lt;cr&gt; &quot; Fast editing of the .vimrcset ffs=unix,dos,mac &quot; Use Unix as the standard file typeautocmd! bufwritepost .vimrc source ~/.vimrc &quot; When vimrc is edited, reload itset scrolloff=7 &quot; 光标到上下各7行的时候不在移动&quot; normal===============================================================&quot; set nowrap &quot; 不要换行$&quot; set cursorline &quot; 突出显示当前行set number &quot; 显示行号set autoindent &quot; 自动对齐set cindent &quot; 使用C风格的缩进方案&quot; set smartindent &quot; 比autoindent稍智能的自动缩进set softtabstop=4 &quot; 按退格键时可以一次删掉 4 个空格set tabstop=4 &quot; tab为4个空格set shiftwidth=4 &quot; 当前行之间交错时使用4个空格set showmatch &quot; 插入括号时，短暂地跳转到匹配的对应括号set matchtime=2 &quot; 短暂跳转到匹配括号的时间set background=dark &quot; 让深色的字体高亮显示（例如：注释等）set listchars=tab:&gt;-,trail:-,extends:&gt;,precedes:&lt; &quot;,eol:$ &quot; 将制表符显示为&apos;&gt;---&apos;,将行尾空格显示为&apos;-&apos;set nobackupset noerrorbells &quot; 关闭错误信息响铃set novisualbell &quot; 关闭使用可视响铃代替呼叫set vb t_vb= &quot; 当vim进行编辑时，如果命令错误，会发出警报，该设置去掉警报set showcmd &quot; 在状态栏显示正在输入的命令set ruler &quot; 在编辑过程中，在右下角显示光标位置的状态行set magic &quot; 设置魔术,正则匹配set ignorecase smartcase &quot; 搜索时忽略大小写，但在有一个或以上大写字母时仍保持对大小写敏感&quot;set nowrapscan &quot; 禁止在搜索到文件两端时重新搜索set incsearch &quot; 输入搜索内容时就显示搜索结果set hlsearch &quot; 搜索时高亮显示被找到的文本set cmdheight=4 &quot; 设置命令行的高度set list&quot;colo evening &quot; setting colors from vimset cmdheight=1 &quot; 设定命令行的行数为 1set laststatus=2 &quot; 显示状态栏 (默认值为 1, 无法显示状态栏)set formatoptions=rq &quot; 注释格式化选项&quot; setting fron fold===================================================set foldenable &quot; 开始折叠set foldmethod=syntax &quot; 设置语法折叠set foldlevel=100 &quot; 设置折叠层数为,设置此选项为零关闭所有的折叠。更高的数字关闭更少的折叠。nnoremap &lt;space&gt; @=((foldclosed(line(&apos;.&apos;)) &lt; 0) ? &apos;zc&apos; : &apos;zo&apos;)&lt;CR&gt; &quot; 用空格键来开关折叠set foldcolumn=0 &quot; 在左侧显示折叠的层次&quot;set foldclose=all &quot; 设置为自动关闭折叠&quot;colorscheme colorzone &quot; 设定配色方案&quot;colorscheme molokai &quot; 设定配色方案set foldexpr=2 &quot; 设置代码块折叠后显示的行数&quot; ======================================================================&quot; Only do this part when compiled with support for autocommands.if has(&quot;autocmd&quot;) &quot; Enable file type detection. &quot; Use the default filetype settings, so that mail gets &apos;tw&apos; set to 72, &quot; &apos;cindent&apos; is on in C files, etc. &quot; Also load indent files, to automatically do language-dependent indenting. filetype plugin indent on &quot; Put these in an autocmd group, so that we can delete them easily. augroup vimrcEx au! &quot; For all text files set &apos;textwidth&apos; to 78 characters. autocmd FileType text setlocal textwidth=78 &quot; When editing a file, always jump to the last known cursor position. &quot; Don&apos;t do it when the position is invalid or when inside an event handler &quot; (happens when dropping a file on gvim). &quot; Also don&apos;t do it when the mark is in the first line, that is the default &quot; position when opening a file. autocmd BufReadPost * \ if line(&quot;&apos;\&quot;&quot;) &gt; 1 &amp;&amp; line(&quot;&apos;\&quot;&quot;) &lt;= line(&quot;$&quot;) | \ exe &quot;normal! g`\&quot;&quot; | \ endif augroup ENDelse set autoindent &quot; always set autoindenting onendif &quot; has(&quot;autocmd&quot;)&quot; setting for building c/c++ program===============================&quot; C的编译和运行map &lt;F5&gt; :call CompileRunGcc()&lt;CR&gt;func! CompileRunGcc() exec &quot;w&quot; exec &quot;!gcc % -o %&lt;&quot; exec &quot;! ./%&lt;&quot;endfunc&quot; C++的编译和运行map &lt;F6&gt; :call CompileRunGpp()&lt;CR&gt;func! CompileRunGpp() exec &quot;w&quot; exec &quot;!g++ % -o %&lt;&quot; exec &quot;! ./%&lt;&quot;endfunc&quot; setting for backspace==============================================set backspace=indent,eol,start &quot; allow backspacing over everything in insert modeset whichwrap+=&lt;,&gt;,h,l&quot;setting for taglist=================================================let Tlist_Ctags_Cmd=&apos;ctags&apos;let Tlist_Show_One_File=1 &quot; 不同时显示多个文件的tag，只显示当前文件的let Tlist_Exit_OnlyWindow=1 &quot; 如果taglist窗口是最后一个窗口，则退出vimlet Tlist_Use_Right_Window=1 &quot; 在右侧窗口中显示taglist窗口let Tlist_Auto_Update=1 &quot; update tags automaticallylet Tlist_Auto_Highlight_Tag=1let Tlist_Highlight_Tag_On_BufEnter=1 &quot; highlight on current taglet Tlist_Show_Menu=0 &quot; show tags menulet Tlist_Auto_Highlight_Tag=1map &lt;C-]&gt; g&lt;C-]&gt;&quot;map &lt;C-]&gt; :tag&lt;cr&gt;&quot;let Tlist_Auto_Open = 1 &quot; open tablist automatically&quot;let Tlist_Close_On_Select=1 &quot; close tablist on select&quot;let Tlist_WinHeight=50let Tlist_WinWidth=30map &lt;silent&gt;&lt;leader&gt;tl :TlistToggle&lt;cr&gt;&quot;setting for filebuffer switch=========================================nmap &lt;C-N&gt; :bnext&lt;CR&gt;nmap &lt;C-P&gt; :bprevious&lt;CR&gt;&quot;setting for windows===================================================nmap &lt;C-h&gt; &lt;C-w&gt;hnmap &lt;C-j&gt; &lt;C-w&gt;jnmap &lt;C-k&gt; &lt;C-w&gt;knmap &lt;C-l&gt; &lt;C-w&gt;l&quot;set mark==============================================================nmap &lt;silent&gt; &lt;leader&gt;hl &lt;Plug&gt;MarkSetvmap &lt;silent&gt; &lt;leader&gt;hl &lt;Plug&gt;MarkSetnmap &lt;silent&gt; &lt;leader&gt;hh &lt;Plug&gt;MarkClearvmap &lt;silent&gt; &lt;leader&gt;hh &lt;Plug&gt;MarkClearnmap &lt;silent&gt; &lt;leader&gt;hr &lt;Plug&gt;MarkRegexvmap &lt;silent&gt; &lt;leader&gt;hr &lt;Plug&gt;MarkRegex&quot;setting for minibufexpl===============================================let g:miniBufExplorerMoreThanOne=1 &quot; 即使只有一个文件，也显示minibuf窗口let g:miniBufExplMapWindowNavVim=1let g:miniBufExplMapWindowNavArrows=1let g:miniBufExplMapCTabSwitchBufs=1let g:miniBufExplModSelTarget=1&quot;setting for NERD tree=================================================nmap &lt;silent&gt; &lt;leader&gt;tt :NERDTreeToggle&lt;cr&gt;let NERDTreeWinSize=30&quot;autocmd VimEnter * NERDTree &quot; 在 vim 启动的时候默认开启 NERDTree（autocmd 可以缩写为 au）autocmd bufenter * if (winnr(&quot;$&quot;) == 1 &amp;&amp; exists(&quot;b:NERDTreeType&quot;) &amp;&amp; b:NERDTreeType == &quot;primary&quot;) | q | endif &quot; if the last window is NERDTree, then close Vim&quot;let NERDTreeWinPos=&quot;right&quot; &quot; 将 NERDTree 的窗口设置在 vim 窗口的右侧（默认为左侧）let NERDChristmasTree=1 &quot; 让树更好看,真心没发现啊let NERDTreeDirArrows=0&quot;setting for winmanager==============================================let g:winManagerWindowLayout = &quot;FileExplorer,BufExplorer|TagList&quot;let g:winManagerWidth=30let g:defaultExplorer=0nmap &lt;silent&gt; &lt;leader&gt;wm :WMToggle&lt;cr&gt;&quot;setting for omnicppcomplete========================================let OmniCpp_GlobalScopeSearch = 1 &quot; 0 or 1let OmniCpp_NamespaceSearch = 1 &quot; 0 , 1 or 2let OmniCpp_DisplayMode = 1let OmniCpp_ShowScopeInAbbr = 0let OmniCpp_ShowPrototypeInAbbr = 1let OmniCpp_ShowAccess = 1let OmniCpp_MayCompleteDot = 1let OmniCpp_MayCompleteArrow = 1let OmniCpp_MayCompleteScope = 1&quot;let g:SuperTabRetainCompletionType=2&quot;let g:SuperTabDefaultCompletionType=&quot;&lt;C-X&gt;&lt;C-O&gt;&quot;set completeopt=longest,menu&quot;setting for cscope+ctags===========================================if filereadable(&quot;cscope.out&quot;) cs add cscope.outendifcs add ~/p2p_server/branches/taishan/server/framecommon/src/cscope.out ~/p2p_server/branches/taishan/server/framecommon/src/cs add ~/p2p_server/branches/taishan/server/srvframe/src/cscope.out ~/p2p_server/branches/taishan/server/srvframe/src/set tags+=~/.vim/cpp_src/tagsset tags+=/usr/include/tagsset tags+=~/p2p_server/branches/taishan/server/framecommon/src/tagsset tags+=~/p2p_server/branches/taishan/server/srvframe/src/tagsnmap &lt;C-\&gt;s :cs find s &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;g :cs find g &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;c :cs find c &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;t :cs find t &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;e :cs find e &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;f :cs find f &lt;C-R&gt;=expand(&quot;&lt;cfile&gt;&quot;)&lt;CR&gt;&lt;CR&gt;nmap &lt;C-\&gt;i :cs find i ^&lt;C-R&gt;=expand(&quot;&lt;cfile&gt;&quot;)&lt;CR&gt;$&lt;CR&gt;nmap &lt;C-\&gt;d :cs find d &lt;C-R&gt;=expand(&quot;&lt;cword&gt;&quot;)&lt;CR&gt;&lt;CR&gt;&quot;setting for multi-encoding ========================================if has(&quot;multi_byte&quot;) &quot;set bomb set fileencodings=ucs-bom,utf-8,cp936,big5,euc-jp,euc-kr,latin1 &quot; CJK environment detection and corresponding setting if v:lang =~ &quot;^zh_CN&quot; &quot; Use cp936 to support GBK, euc-cn == gb2312 set encoding=cp936 set termencoding=cp936 set fileencoding=cp936 elseif v:lang =~ &quot;^zh_TW&quot; &quot; cp950, big5 or euc-tw &quot; Are they equal to each other? set encoding=big5 set termencoding=big5 set fileencoding=big5 elseif v:lang =~ &quot;^ko&quot; &quot; Copied from someone&apos;s dotfile, untested set encoding=euc-kr set termencoding=euc-kr set fileencoding=euc-kr elseif v:lang =~ &quot;^ja_JP&quot; &quot; Copied from someone&apos;s dotfile, untested set encoding=euc-jp set termencoding=euc-jp set fileencoding=euc-jp endif &quot; Detect UTF-8 locale, and replace CJK setting if needed if v:lang =~ &quot;utf8$&quot; || v:lang =~ &quot;UTF-8$&quot; set encoding=utf-8 set termencoding=utf-8 set fileencoding=utf-8 endifelse echoerr &quot;Sorry, this version of (g)vim was not compiled with multi_byte&quot;endif]]></content>
      <tags>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WOWZA流媒体直播配置]]></title>
    <url>%2F2015%2F09%2F22%2Fwowza%2F</url>
    <content type="text"><![CDATA[本文目的 安装和使用wowza流媒体服务器 使用ffmpeg向wowza推直播ts流并播放 wowza流媒体服务器简介WowzaStreaming Engine 4 (也就是著名的WowzaMedia Server®)是一个高性能、可扩展的流媒体服务器软件，支持直播、VOD、在线视频聊天、远程录制功能， 它也支持多种播放器技术，包括： Adobe® HTTP Dynamic Streaming (HDS). AdobeFlash® 播放器 Apple® HTTP Live Streaming (HLS). iPhone®,iPad®, iPod touch®, Safari® 浏览器,QuickTime® 播放器 Microsoft® Smooth Streaming. MicrosoftSilverlight® MPEG-DASH streaming. DASH clients Real Time Streaming Protocol (RTSP/RTP).QuickTime 播放器,VLC 媒体播放器,以及许多移动终端 MPEG-2 Transport Streams (MPEG-TS). 机顶盒和IPTV解决方案 wowza下载安装 下载 官网下载页面：可以选择你自己需要的平台版本进行下载 百度云地址：linux版本，本文配置已linux为例 获取许可密钥 在官网注册一个账户，Wowza就会给你发一个使用密钥，我这里获得的密钥有效期为半年。 安装 12sudo chmod +x WowzaStreamingEngine-4.0.3.rpm.bin sudo ./WowzaStreamingEngine-4.1.2.tar.bin 其中安装过程根据控制台打印的提示，问你是否同意license条款：yes，提示输入Wowza Streaming Engine Manager管理员用户名和密码。这个用户名和密码是用来登陆管理页面时需要，如丢失只能在安装目录的conf目录下相关文件中找回，接下来会要求输入license key，输入后开始安装，默认安装在/usr/local下。 使用 wowza 4 引入了友好的可视化服务管理，用户不需要去面对各种 conf 也能通过网页对 wowza 服务进行管理、配置了，甚至还可以轻松地对 wowza 的服务的各个应用的状态进行实时监控。刚装完时，无法通过http://192.168.200.81:8088/enginemanager登陆到管理页面上去，这个时候需要重启一下服务： 12/etc/init.d/WowzaStreamingEngine start/etc/init.d/WowzaStreamingEngineManager start 通过http://192.168.200.81::8088/enginemanager登陆管理页面 wowza直播源设置步骤 登陆Wowza管理页面 applications下默认有live和vod两个应用，也可以自己根据需要在重新添加一个应用，这里我们选择live 选着左侧Stream Files，然后再Add Stream File,即可创建流 添加流名称和地址，名称任意，流地址为服务器地址和想使用的端口组合 进入Server-&gt;Stream Files点击流名称的箭头连接按钮 这里可以注意一下，左边有个Startup Streams,这里是当前启动的流，创建的流需要启动，否则无法使用。启动流点击流名称的加号按钮即可，启动后会看到服务器已经监听响应端口。 添加Application Name和MediaCasterType，这里推的是rtsp的ts流 再次回到Applictions-&gt;Stream Files进入创建的stream，右上角Test Players即可观看到推流结果 ffmpeg推流使用ffmpeg推rtsp的ts流到服务器，地址即为之前创建的流的ip和端口，命令如下： 1ffmpeg -re -i "panda.mp4" -acodec copy -vcodec copy -vbsf h264_mp4toannexb -f mpegts udp://192.168.200.81:5004?pkt_size=1316]]></content>
      <tags>
        <tag>WOWZA</tag>
      </tags>
  </entry>
</search>
